{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3879fff0",
   "metadata": {},
   "source": [
    "Import necessari:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a7224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Built-in ===\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# === Third-party generici ===\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.feature import hog\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# === scikit-learn ===\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    "    pairwise_distances\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import MDS\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286363c",
   "metadata": {},
   "source": [
    "Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c5311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    return data['features'], data['labels'], data.get('filenames', [f\"img_{i}\" for i in range(len(data['features']))])\n",
    "\n",
    "\n",
    "feat_matrix_part1, lbls_part1, flname_part1 = load_features(\"hog_features_part1.npz\")\n",
    "feat_matrix_part2, lbls_part2, flname_part2 = load_features(\"hog_features_part2.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_brain(img):\n",
    "    \"\"\"Ritaglia l'area informativa (cervello) da un'immagine.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    coords = cv2.findNonZero(thresh)\n",
    "    if coords is not None:\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        return img[y:y+h, x:x+w]\n",
    "    return img  # fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e16956",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Histograms of oriented gradients, HOG: Mappa l'immagine in scala di grigi, ridimensiona l'immagine a 300 x 10, partiziona l'immagine in una griglia 10x10, calcola l'istogramma del gradiente con  magnitude-weighted a 9 bin (con segno) (ogni bin corrispondente a 40 gradi) per ogni cella della griglia e combina questi istogrammi in un descrittore di feature di dimensione 10x10x9 = 900.    \n",
    "<br>Puoi usare {-1, 0, 1} e {-1, 0, 1}^T maschere per ottenere dI/dx e dI/dy per ogni posizione di pixel nella cella della griglia.\n",
    "\n",
    "## Task 2\n",
    "Implementa un programma che **estrae e memorizza i descrittori di feature** per tutte le immagini nel set di dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99d462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Funzione: Estrazione HOG ===\n",
    "def extract_hog_features(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERRORE] Immagine non trovata o non valida: {img_path}\")\n",
    "        return None\n",
    "    \n",
    "    img = crop_to_brain(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. Ridimensiona a 300x10 (come richiesto nella consegna)\n",
    "    resized = cv2.resize(gray, (300, 10))\n",
    "    \n",
    "    # 3. Definisci le maschere per i gradienti come specificato\n",
    "    mask_x = np.array([[-1, 0, 1]])         # per dI/dx\n",
    "    mask_y = np.array([[-1], [0], [1]])     # per dI/dy (trasposta di dx)\n",
    "    # mask_y = mask_x.T\n",
    "\n",
    "    # 4. Calcola i gradienti usando convoluzione\n",
    "    gradient_x = convolve(resized.astype(np.float32), mask_x, mode='constant', cval=0)\n",
    "    gradient_y = convolve(resized.astype(np.float32), mask_y, mode='constant', cval=0)\n",
    "\n",
    "    # 5. Calcola magnitude(intensità) e angolo(orientazione)\n",
    "    magnitude = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "    angle = np.arctan2(gradient_y, gradient_x)\n",
    "\n",
    "    # 6. Converti orientazione in gradi e normalizza a [0, 360)\n",
    "    angle_degree = np.degrees(angle)\n",
    "    angle_degree = (angle_degree + 360) % 360\n",
    "\n",
    "    # 7. Partiziona in griglia 10x10\n",
    "    # L'immagine è 300x10, quindi ogni cella è 30x1\n",
    "    cell_height = resized.shape[0] // 10  # 30\n",
    "    cell_width = resized.shape[1] // 10   # 1\n",
    "    \n",
    "    # 8. Inizializza il descrittore HOG\n",
    "    hog_features = np.zeros((10, 10, 9))\n",
    "\n",
    "    # 9. Per ogni cella della griglia\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Estrai la cella\n",
    "            start_row = i * cell_height\n",
    "            end_row = start_row + cell_height\n",
    "            start_col = j * cell_width\n",
    "            end_col = start_col + cell_width\n",
    "\n",
    "            cell_magnitude = magnitude[start_row:end_row, start_col:end_col]\n",
    "            cell_orientation = angle_degree[start_row:end_row, start_col:end_col]\n",
    "\n",
    "            # 10. Calcola l'istogramma a 9 bin (ogni bin = 40 gradi)\n",
    "            histogram = np.zeros(9)\n",
    "\n",
    "            for y in range(cell_magnitude.shape[0]):\n",
    "                for x in range(cell_magnitude.shape[1]):\n",
    "                    angle = cell_orientation[y, x]\n",
    "                    mag = cell_magnitude[y, x]\n",
    "\n",
    "                    # Determina il bin (0-8 per 9 bin)\n",
    "                    bin_idx = int(angle // 40) % 9\n",
    "\n",
    "                    # Aggiungi magnitude-weighted\n",
    "                    histogram[bin_idx] += mag\n",
    "\n",
    "            # Salva l'istogramma nella posizione corretta\n",
    "            hog_features[i, j, :] = histogram\n",
    "\n",
    "    # 11. Flatten per ottenere il descrittore finale di dimensione 900\n",
    "    hog_descriptor = hog_features.flatten()\n",
    "\n",
    "    return hog_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0beff54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrae le feature da immagini organizzate in sottocartelle e le salva in un file .npz\n",
    "def process_and_save_features(base_folder, subfolders, output_file):\n",
    "\n",
    "    all_features, all_filenames, all_labels = [], [], []\n",
    "\n",
    "    for label in subfolders:\n",
    "        folder_path = os.path.join(base_folder, label)\n",
    "        print(f\"[INFO] Elaboro cartella: {label}\")\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                features = extract_hog_features(img_path)\n",
    "                if features is not None:\n",
    "                    all_features.append(features)\n",
    "                    all_filenames.append(filename)\n",
    "                    all_labels.append(label)\n",
    "    \n",
    "    np.savez(output_file,\n",
    "             features=np.array(all_features),\n",
    "             filenames=np.array(all_filenames),\n",
    "             labels=np.array(all_labels))\n",
    "    print(f\"[SALVATO] Features salvate in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0503565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri\n",
    "subfolders = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]\n",
    "\n",
    "# Estrazione e salvataggio\n",
    "process_and_save_features(\"Part1\", subfolders, \"hog_features_part1.npz\")\n",
    "process_and_save_features(\"Part2\", subfolders, \"hog_features_part2.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544b1f2",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implementa un programma che, **dato il **nome di un file immagine** e un valore \"k\"**, **restituisce e visualizza le k immagini più simili** in base a ciascun modello visivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf884d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Correlation Similarity ===\n",
    "def correlation_similarity(a, B):\n",
    "    a_mean = a - np.mean(a)\n",
    "    B_mean = B - np.mean(B, axis=1, keepdims=True)\n",
    "    num = np.dot(B_mean, a_mean)\n",
    "    denom = np.linalg.norm(B_mean, axis=1) * np.linalg.norm(a_mean)\n",
    "    return num / (denom + 1e-10)\n",
    "\n",
    "# === K-NN Similarity ===\n",
    "def find_k_similar_hog(base_folder, img_path, k, distance_type=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a una query HOG usando diverse metriche.\n",
    "    \"\"\"\n",
    "    # Estrai feature query\n",
    "    query_feature = extract_hog_features(img_path)\n",
    "    if query_feature is None:\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    if distance_type == \"euclidean\":\n",
    "        distances = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "        scores = distances\n",
    "        ascending = True  # più basso è meglio\n",
    "\n",
    "    elif distance_type == \"correlation\":\n",
    "        sims = correlation_similarity(query_feature.flatten(), feat_matrix_part1)\n",
    "        scores = sims\n",
    "        ascending = False  # più alto è meglio\n",
    "\n",
    "    elif distance_type == \"cosine\":\n",
    "        sims = cosine_similarity(feat_matrix_part1, query_feature).flatten()\n",
    "        scores = sims\n",
    "        ascending = False  # più alto è meglio\n",
    "    \n",
    "    else:\n",
    "        print(f\"[ERRORE] Tipo di distanza non supportato: {distance_type}\")\n",
    "        return\n",
    "\n",
    "    # Escludi la query stessa se presente\n",
    "    query_filename = os.path.basename(img_path)\n",
    "    query_label = os.path.basename(os.path.dirname(img_path))\n",
    "    for i in range(len(flname_part1)):\n",
    "        if flname_part1[i] == query_filename and lbls_part1[i] == query_label:\n",
    "            if ascending:\n",
    "                scores[i] = np.inf\n",
    "            else:\n",
    "                scores[i] = -np.inf\n",
    "            break\n",
    "    \n",
    "    # Ordina e seleziona\n",
    "    top_k_idx = np.argsort(scores)\n",
    "    if not ascending:\n",
    "        top_k_idx = top_k_idx[::-1]\n",
    "    top_k_idx = top_k_idx[:k]\n",
    "\n",
    "    # Stampa risultati\n",
    "    print(f\"\\nTop {k} immagini simili a: {img_path} (HOG - {distance_type})\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        val = scores[idx]\n",
    "        if ascending:\n",
    "            print(f\"{rank + 1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Distanza: {val:.4f}\")\n",
    "        else:\n",
    "            print(f\"{rank + 1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Similarità: {val:.4f}\")\n",
    "\n",
    "    # Visualizza immagini\n",
    "    fig, axs = plt.subplots(1, k + 1, figsize=(15, 5))\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, idx in enumerate(top_k_idx):\n",
    "        img_match_path = os.path.join(base_folder, lbls_part1[idx], flname_part1[idx])\n",
    "        img_match = cv2.imread(img_match_path)\n",
    "        axs[i + 1].imshow(cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB))\n",
    "        if ascending:\n",
    "            axs[i + 1].set_title(f\"Rank {i + 1}\\nD={scores[idx]:.2f}\")\n",
    "        else:\n",
    "            axs[i + 1].set_title(f\"Rank {i + 1}\\nS={scores[idx]:.2f}\")\n",
    "        axs[i + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735def42",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Part1\"\n",
    "img_path = \"Part1/brain_menin/brain_menin_0013.jpg\"\n",
    "\n",
    "find_k_similar_hog(base_folder, img_path, k=5, distance_type=\"euclidean\")\n",
    "find_k_similar_hog(base_folder, img_path, k=5, distance_type=\"correlation\")\n",
    "find_k_similar_hog(base_folder, img_path, k=5, distance_type=\"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4ee3d",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "Implementa un programma che, dati:\n",
    "- (a) un'immagine di query della **Parte 2**,\n",
    "- (b) uno **spazio di feature** selezionato dall'utente,\n",
    "- (c) un numero intero positivo **k**,\n",
    "\n",
    "identifichi ed elenchi le **k etichette di corrispondenza più probabili**, insieme ai loro punteggi, calcolati nello **spazio di feature selezionato**.\n",
    "\n",
    "---\n",
    "\n",
    "## Requisiti\n",
    "\n",
    "1. Accettare come input:\n",
    "   - Un’immagine di query (da **Part2**),\n",
    "   - Una scelta dell’utente sul tipo di **feature space**,\n",
    "   - Un valore intero **k** (con **k ≤ 2**).\n",
    "\n",
    "2. Calcolare le **feature** dell’immagine di query secondo lo **spazio di feature selezionato**.\n",
    "\n",
    "3. Calcolare la distanza tra la query e tutte le immagini del dataset, utilizzando una metrica come:\n",
    "   - **Distanza Euclidea**,\n",
    "   - **Distanza di Mahalanobis**.\n",
    "\n",
    "4. Raggruppare le distanze per **etichetta** e selezionare le **k classi più simili**, ad esempio calcolando:\n",
    "   - la **distanza media**,\n",
    "   - oppure la **somma inversa delle distanze**.\n",
    "\n",
    "5. Stampare o restituire una **classifica delle k etichette più probabili**, con il rispettivo punteggio.\n",
    "\n",
    "---\n",
    "\n",
    "## Definizioni\n",
    "\n",
    "**Feature space:** spazio vettoriale in cui ogni immagine è rappresentata come un **vettore di caratteristiche** (feature vector).\n",
    "\n",
    "**Selected feature space:** indica **il tipo di caratteristiche** estratte per rappresentare le immagini (es. color moments, HOG, deep features, ecc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b9a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ranking_strategies(query_img_path, k=2):\n",
    "    \"\"\"\n",
    "    Confronta le etichette predette da due strategie:\n",
    "    - distanza media per classe\n",
    "    - distanza al rappresentante (prototipo) della classe\n",
    "    Visualizza solo le top-k etichette con un grafico comparativo.\n",
    "    \"\"\"\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "\n",
    "    query_feature = extract_hog_features(query_img_path)\n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    # ===== Strategia 1: distanza media per classe =====\n",
    "    distances_all = cosine_similarity(feat_matrix_part1, query_feature).flatten()\n",
    "    df_all = pd.DataFrame({\n",
    "        'label': lbls_part1,\n",
    "        'distance': distances_all\n",
    "    })\n",
    "    mean_dists = df_all.groupby('label')['distance'].mean().sort_values()\n",
    "\n",
    "    # ===== Strategia 2: distanza dal prototipo (centroide) =====\n",
    "    df_features = pd.DataFrame(feat_matrix_part1)\n",
    "    df_features['label'] = lbls_part1\n",
    "    class_prototypes = df_features.groupby('label').mean().drop(columns=['label'], errors='ignore')\n",
    "    proto_vectors = class_prototypes.values\n",
    "    proto_labels = class_prototypes.index\n",
    "    proto_dists = cosine_similarity(proto_vectors, query_feature).flatten()\n",
    "    proto_dists_series = pd.Series(proto_dists, index=proto_labels).sort_values()\n",
    "\n",
    "    # ===== Prendi le top-k etichette comuni =====\n",
    "    top_k_mean = mean_dists.head(k)\n",
    "    top_k_proto = proto_dists_series.head(k)\n",
    "\n",
    "    union_labels = sorted(set(top_k_mean.index).union(set(top_k_proto.index)))\n",
    "\n",
    "    # ===== Plot solo per le top-k etichette =====\n",
    "    x = np.arange(len(union_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.bar(x - width/2, [mean_dists[label] for label in union_labels], width, label='Distanza Media')\n",
    "    ax.bar(x + width/2, [proto_dists_series[label] for label in union_labels], width, label='Distanza Prototipo')\n",
    "\n",
    "    ax.set_ylabel('Distanza')\n",
    "    ax.set_title(f\"Top-{k} Strategie - Query: {os.path.basename(query_img_path)}\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(union_labels)\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ===== Stampa i top-k risultati =====\n",
    "    print(\"\\n Top-k etichette per ciascuna strategia:\\n\")\n",
    "    print(\"Strategia: Distanza Media\")\n",
    "    print(top_k_mean)\n",
    "\n",
    "    print(\"\\n Strategia: Prototipo di Classe\")\n",
    "    print(top_k_proto)\n",
    "\n",
    "\n",
    "     # ===== Confronto top-1 =====\n",
    "    top1_mean_label = top_k_mean.index[0]\n",
    "    top1_mean_value = top_k_mean.iloc[0]\n",
    "    top1_proto_label = top_k_proto.index[0]\n",
    "    top1_proto_value = top_k_proto.iloc[0]\n",
    "\n",
    "    print(\"\\n==== Analisi della Strategia Migliore ====\")\n",
    "    if top1_mean_label == top1_proto_label:\n",
    "        print(f\"[OK] Entrambe le strategie concordano sulla classe '{top1_mean_label}'.\")\n",
    "        print(f\" → Distanza media: {top1_mean_value:.4f}, distanza prototipo: {top1_proto_value:.4f}\")\n",
    "    else:\n",
    "        print(f\"[DIFFERENZA] Le strategie danno risultati diversi:\")\n",
    "        print(f\" - Distanza Media: '{top1_mean_label}' con distanza {top1_mean_value:.4f}\")\n",
    "        print(f\" - Prototipo: '{top1_proto_label}' con distanza {top1_proto_value:.4f}\")\n",
    "        if top1_mean_value < top1_proto_value:\n",
    "            print(f\" → [SCELTA SUGGERITA] Preferibile 'Distanza Media' ({top1_mean_label})\")\n",
    "        else:\n",
    "            print(f\" → [SCELTA SUGGERITA] Preferibile 'Prototipo' ({top1_proto_label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_glioma/brain_glioma_1142.jpg\"\n",
    "compare_label_ranking_strategies(query_img, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f5b1b",
   "metadata": {},
   "source": [
    "# Task 5: Estrazione delle Semantiche Latenti\n",
    "\n",
    "Implementa un programma che, dati:\n",
    "- (a) uno dei modelli di feature disponibili,\n",
    "- (b) un valore **k** specificato dall'utente,\n",
    "- (c) una delle tre tecniche di riduzione della dimensionalità (**SVD**, **LDA**, **k-means**) scelta dall'utente,\n",
    "\n",
    "estragga le **prime k semantiche latenti** dallo spazio delle feature selezionato.\n",
    "\n",
    "---\n",
    "\n",
    "## Funzionalità richieste\n",
    "\n",
    "- **Memorizzare** le semantiche latenti estratte in un file di output, **adeguatamente nominato**.\n",
    "- **Elencare** per ogni componente latente le **coppie (imageID, peso)** ordinate in ordine decrescente di peso.\n",
    "\n",
    "---\n",
    "\n",
    "## Definizione del feature model\n",
    "\n",
    "Il **feature model** è un file contenente la rappresentazione delle feature di tutte le immagini del dataset, salvato come matrice (`feature_matrix`).  \n",
    "La matrice ha dimensioni **n × d**, dove:\n",
    "- **n** = numero di immagini,\n",
    "- **d** = numero di feature.\n",
    "\n",
    "Esempio di feature model: `hog_features_part1.npz`.\n",
    "\n",
    "---\n",
    "\n",
    "## Procedura\n",
    "\n",
    "### Input\n",
    "- Un **feature model** (es. `hog_features_part1.npz`).\n",
    "- Un valore **k** che indica il numero di componenti latenti da estrarre.\n",
    "- Una tecnica di riduzione dimensionale (**SVD**, **LDA**, o **k-means**).\n",
    "\n",
    "### Operazioni\n",
    "1. **Riduzione dimensionale** del feature space selezionato tramite la tecnica scelta.\n",
    "2. **Estrazione delle top-k componenti latenti**.\n",
    "3. Per ogni componente:\n",
    "   - Calcolare i **pesi** associati a ogni immagine.\n",
    "   - Ordinare le coppie **(imageID, peso)** in ordine decrescente di peso.\n",
    "\n",
    "4. **Salvare i risultati** in un file di testo con nome descrittivo, ad esempio:  \n",
    "   `latent_semantics_svd_hog_k3.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "## Significato di **k**\n",
    "\n",
    "| Tecnica   | Significato di k                                           |\n",
    "|-----------|------------------------------------------------------------|\n",
    "| **SVD**   | Numero di componenti principali → riduzione mantenendo la variazione globale. |\n",
    "| **LDA**   | Numero di direzioni discriminanti → riduzione focalizzata sulla separazione tra le classi. |\n",
    "| **k-means** | Numero di cluster → suddivisione delle immagini in gruppi simili (senza usare le etichette). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space_2d(X_transformed, labels, technique, k):\n",
    "    \"\"\"Visualizza la proiezione 2D delle immagini nello spazio latente (solo per SVD/LDA).\"\"\"\n",
    "    if X_transformed.shape[1] < 2:\n",
    "        print(\"[INFO] Meno di 2 componenti: impossibile visualizzare in 2D.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=labels, palette=\"Set2\", s=80)\n",
    "    plt.title(f\"{technique.upper()} - Proiezione sulle prime 2 componenti latenti (k={k})\")\n",
    "    plt.xlabel(\"Componente 1\")\n",
    "    plt.ylabel(\"Componente 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_kmeans_clusters_2d(feature_matrix, labels, n_clusters):\n",
    "    \"\"\"Visualizza le immagini raggruppate da KMeans su uno spazio 2D ridotto con SVD.\"\"\"\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd.fit_transform(feature_matrix)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_labels, palette='tab10', s=80, style=labels)\n",
    "    plt.title(f\"KMeans Clustering (k={n_clusters}) con proiezione SVD 2D\")\n",
    "    plt.xlabel(\"Componente Latente 1 (da SVD)\")\n",
    "    plt.ylabel(\"Componente Latente 2 (da SVD)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def task5_latent_semantics_hog(feature_model_path, technique, k):\n",
    "    \"\"\"\n",
    "    Estrae i top-k concetti latenti da feature HOG tramite SVD, LDA o KMeans.\n",
    "    Visualizza lo spazio latente ed esporta un file .txt con i pesi delle immagini.\n",
    "    \"\"\"\n",
    "    technique = technique.lower()\n",
    "\n",
    "    if technique == \"svd\":\n",
    "        model = TruncatedSVD(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1)\n",
    "        components = model.components_\n",
    "        method = \"svd\"\n",
    "\n",
    "    elif technique == \"lda\":\n",
    "        unique_labels = np.unique(lbls_part1)\n",
    "        max_k = min(k, len(unique_labels) - 1)\n",
    "        if k > max_k:\n",
    "            print(f\"[ATTENZIONE] LDA supporta al massimo {max_k} componenti con {len(unique_labels)} classi.\")\n",
    "            k = max_k\n",
    "        model = LDA(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1, lbls_part1)\n",
    "        components = model.scalings_.T[:k]\n",
    "        method = \"lda\"\n",
    "\n",
    "    elif technique == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        model.fit(feat_matrix_part1)\n",
    "        components = model.cluster_centers_\n",
    "        X_transformed = model.transform(feat_matrix_part1)\n",
    "        method = \"kmeans\"\n",
    "\n",
    "    else:\n",
    "        print(\"[ERRORE] Tecnica non supportata. Usa: 'svd', 'lda', 'kmeans'\")\n",
    "        return\n",
    "\n",
    "    # Visualizzazione\n",
    "    if technique in [\"svd\", \"lda\"]:\n",
    "        plot_latent_space_2d(X_transformed, lbls_part1, technique, k)\n",
    "    elif technique == \"kmeans\":\n",
    "        plot_kmeans_clusters_2d(feat_matrix_part1, lbls_part1, k)\n",
    "\n",
    "# Creazione output\n",
    "    output_dir = os.path.join(\"task5_output\", \"latent_hog_color_moments\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = os.path.join(output_dir, f\"latent_semantics_{method}_{base_name}_k{k}.txt\")\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for i in range(k):\n",
    "            f.write(f\"\\n--- Latent Semantic {i+1} ---\\n\")\n",
    "            if technique in [\"svd\", \"lda\"]:\n",
    "                weights = feat_matrix_part1 @ components[i].T\n",
    "                sorted_idx = np.argsort(-np.abs(weights))\n",
    "            else:  # KMeans: distanza dal centroide, più piccola = più vicino\n",
    "                weights = -X_transformed[:, i]\n",
    "                sorted_idx = np.argsort(weights)\n",
    "\n",
    "            for idx in sorted_idx:\n",
    "                image_id = flname_part1[idx]\n",
    "                f.write(f\"{image_id} | Peso: {weights[idx]:.4f} | Classe: {lbls_part1[idx]}\\n\")\n",
    "\n",
    "    print(f\"[SALVATO] Latent semantics salvati in: {out_file}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c6277",
   "metadata": {},
   "source": [
    "Visualizzazione "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c9976",
   "metadata": {},
   "source": [
    "Esecuzione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "task5_latent_semantics_hog(\"hog_features_part1.npz\", \"svd\", 5)\n",
    "task5_latent_semantics_hog(\"hog_features_part1.npz\", \"lda\", 2)\n",
    "task5_latent_semantics_hog(\"hog_features_part1.npz\", \"kmeans\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd8555",
   "metadata": {},
   "source": [
    "# Task 6: Calcolo della Dimensionalità Intrinseca\n",
    "\n",
    "## Obiettivo generale\n",
    "\n",
    "Determinare la **dimensionalità intrinseca** degli spazi di feature, ossia il numero minimo di componenti latenti necessarie per spiegare una percentuale significativa della varianza dei dati.\n",
    "\n",
    "Questa analisi permette di:\n",
    "- Trasformare i dati in uno **spazio ortonormale**, mantenendo la maggior parte dell'informazione discriminante.\n",
    "- **Ridurre la dimensionalità** dei dati senza perdere informazioni rilevanti.\n",
    "\n",
    "---\n",
    "\n",
    "## Descrizione dei sotto-task\n",
    "\n",
    "### a) Dimensionalità intrinseca globale\n",
    "Implementa un programma che:\n",
    "- Calcola la **dimensionalità intrinseca complessiva** delle immagini della **Parte 1**.\n",
    "- Riporta il numero minimo di componenti che spiegano una soglia predefinita di varianza (es. 95%).\n",
    "\n",
    "---\n",
    "\n",
    "### b) Dimensionalità intrinseca per classe\n",
    "Implementa un programma che:\n",
    "- Calcola la **dimensionalità intrinseca separatamente per ciascuna etichetta unica** delle immagini della **Parte 1**.\n",
    "- Riporta il numero minimo di componenti che spiegano la soglia di varianza scelta (es. 95%) per ciascuna classe.\n",
    "\n",
    "---\n",
    "\n",
    "## Concetti chiave\n",
    "\n",
    "###  Trasformazione latente ortonormale\n",
    "- Utilizza tecniche di riduzione dimensionale (ad esempio **PCA**) per trasformare i dati in uno spazio le cui componenti sono ortonormali e ordinate per varianza spiegata.\n",
    "\n",
    "###  Concentrazione dell'informazione\n",
    "- Le prime componenti spiegano la maggior parte della varianza.\n",
    "- Le componenti successive contengono informazioni progressivamente meno significative.\n",
    "\n",
    "###  Dimensionalità intrinseca\n",
    "- Definisce il **numero minimo di componenti** necessarie per spiegare almeno una certa percentuale della varianza (**threshold**, es. 95%).\n",
    "- Se il numero di componenti latenti estratte (**m**) è uguale al numero di feature originali, la trasformazione conserva tutta l'informazione.\n",
    "- Se la dimensionalità intrinseca è minore, si introduce un'**approssimazione controllata** nelle distanze e negli angoli dello spazio trasformato.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c19c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### explained = pca.explained_variance_ratio_\n",
    "es. explained = [0.40, 0.30, 0.20, 0.07, 0.03]\n",
    "\n",
    "Un **array di float** dove ciascun **valore rappresenta la percentuale di varianza spiegata da ciascuna componente principale della PCA**.\n",
    "È ordinato dalla componente più importante a quella meno importante.\n",
    "\n",
    "Significa che **la prima componente spiega il 40% della varianza**, la seconda il 30%, ecc.\n",
    "\n",
    "### cumulative = np.cumsum(explained)\n",
    "es:cumulative = [0.40, 0.70, 0.90, 0.97, 1.00]\n",
    "\n",
    "Calcola la somma cumulativa dell’array explained, cioè la **varianza totale spiegata fino a ciascuna componente**.\n",
    "\n",
    "Significa:\n",
    "- Le prime 2 componenti spiegano il 70%\n",
    "- Le prime 3 il 90%\n",
    "- Le prime 4 il 97%, ecc.\n",
    "\n",
    "### intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "Questa riga:\n",
    "\t- Trova il primo indice in cui la varianza cumulativa raggiunge o supera una soglia (threshold), ad esempio 0.95 (95%).\n",
    "\t- np.argmax(...) restituisce il primo True nella condizione cumulative >= threshold.\n",
    "\t- Si aggiunge +1 perché gli indici Python partono da 0, ma il numero di componenti parte da 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_intrinsic_dimensionality(feature_matrix, threshold, plot=True):\n",
    "    max_components = min(feature_matrix.shape)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(feature_matrix)\n",
    "\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "    intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "    #print(f\"[INFO] Spiegazione varianza per ogni componente PCA:\\n{explained}\")\n",
    "    #print(f\"[INFO] Varianza cumulativa:\\n{cumulative}\")\n",
    "    #print(f\"[INFO] Soglia impostata: {threshold}\")\n",
    "    #print(f\"[INFO] Dimensione intrinseca stimata: {intrinsic_dim}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cumulative, marker='o', label=\"Varianza cumulativa\")\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Soglia {threshold*100:.0f}%\")\n",
    "        plt.axvline(x=intrinsic_dim, color='g', linestyle='--', label=f\"k suggerito: {intrinsic_dim}\")\n",
    "        plt.xlabel(\"Numero componenti\")\n",
    "        plt.ylabel(\"Varianza cumulativa\")\n",
    "        plt.title(\"Scelta ottimale di k (PCA/SVD)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"[INFO] k ottimale suggerito (soglia {threshold*100:.0f}%): {intrinsic_dim}\")\n",
    "    return intrinsic_dim, cumulative\n",
    "\n",
    "def suggest_k(feature_matrix, threshold_list=[0.90, 0.95, 0.99]):\n",
    "    print(f\"[INFO] Feature matrix shape: {feature_matrix.shape}\")\n",
    "    k_values = {}\n",
    "    for t in threshold_list:\n",
    "        k, _ = estimate_intrinsic_dimensionality(feature_matrix, threshold=t, plot=False)\n",
    "        k_values[t] = k\n",
    "        print(f\"Soglia {int(t*100)}% : k = {k}\")\n",
    "    return k_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality_per_label(feature_matrix, labels, threshold):\n",
    "    label_dim_map = {}\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"[INFO] Etichette uniche trovate: {len(unique_labels)}\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        label_features = feature_matrix[indices]\n",
    "\n",
    "        if len(indices) < 2:\n",
    "            print(f\"[AVVISO] Label '{label}' ha meno di 2 campioni — ignorata.\")\n",
    "            continue\n",
    "\n",
    "        k, _ = estimate_intrinsic_dimensionality(label_features, threshold=threshold, plot=False)\n",
    "        label_dim_map[label] = k\n",
    "        print(f\" Label '{label}' : k = {k}\")\n",
    "\n",
    "    return label_dim_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b98c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola k per varie soglie\n",
    "print(\"\\n Stima automatica di k in base alla varianza spiegata:\\n\")\n",
    "k_suggeriti = suggest_k(feat_matrix_part1)\n",
    "# Plot dettagliato per la soglia 95%\n",
    "estimate_intrinsic_dimensionality(feat_matrix_part1, threshold=0.95, plot=True)\n",
    "\n",
    "\n",
    "print(\"\\n Task 6b – Dimensionalità per etichetta:\\n\")\n",
    "label_dimensionalities = estimate_dimensionality_per_label(feat_matrix_part1, lbls_part1, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6111e",
   "metadata": {},
   "source": [
    "# Task 7\n",
    "\n",
    "Implementare un programma che:\n",
    "\n",
    "1. Per ogni etichetta unica **l**, calcoli le corrispondenti **k componenti semantiche latenti** (a scelta) associate alle immagini della **Parte 1**.\n",
    "2. Per le immagini della **Parte 2**, preveda le etichette più probabili utilizzando le **distanze/similarità calcolate nello spazio semantico latente specifico per ciascuna etichetta**.\n",
    "\n",
    "Il sistema deve inoltre calcolare e riportare:\n",
    "- **Precisione (Precision)** per ciascuna classe,\n",
    "- **Richiamo (Recall)** per ciascuna classe,\n",
    "- **Punteggio F1 (F1-score)** per ciascuna classe,\n",
    "- **Accuratezza complessiva (Accuracy)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Estrazione della semantica latente per ogni classe\n",
    "\n",
    "**Funzione:** `compute_latent_semantics_per_class(X, y, k)`\n",
    "\n",
    "### Input\n",
    "- `X`: Matrice delle feature delle immagini della Parte 1.\n",
    "- `y`: Vettore delle etichette corrispondenti.\n",
    "- `k`: Numero di componenti latenti da estrarre (ad esempio tramite SVD).\n",
    "\n",
    "### Output\n",
    "- `class_models`: Dizionario che, per ogni classe, contiene:\n",
    "  - uno **scaler** per la normalizzazione delle feature,\n",
    "  - il modello **SVD**,\n",
    "  - i vettori latenti della classe.\n",
    "- `class_means`: Vettori medi (**centroidi latenti**) per ciascuna classe.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Predizione delle etichette\n",
    "\n",
    "**Funzione:** `predict_label(X_test, class_models, class_means)`\n",
    "\n",
    "### Input\n",
    "- `X_test`: Matrice delle feature delle immagini da classificare (Parte 2).\n",
    "- `class_models`: Modelli latenti per ciascuna classe.\n",
    "- `class_means`: Centroidi latenti di ciascuna classe.\n",
    "\n",
    "### Output\n",
    "- `y_pred`: Lista delle etichette previste per ciascuna immagine.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Valutazione delle prestazioni\n",
    "\n",
    "**Funzioni:** `evaluate(y_true, y_pred)` oppure `evaluate_predictions(true_labels, predicted_labels)`\n",
    "\n",
    "### Metriche calcolate\n",
    "- **Precisione** per classe.\n",
    "- **Recall** per classe.\n",
    "- **F1-score** per classe.\n",
    "- **Accuratezza complessiva.**\n",
    "\n",
    "---\n",
    "\n",
    "## Concetto chiave\n",
    "\n",
    "Ogni classe ha un **proprio spazio semantico latente**, in cui le immagini della classe sono rappresentate in modo compatto.  \n",
    "Quando un'immagine deve essere classificata:\n",
    "- viene proiettata in ogni spazio latente di classe,\n",
    "- viene calcolata la distanza rispetto al **centroide latente** di ciascuna classe,\n",
    "- l'immagine viene assegnata alla classe il cui centroide è il più vicino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_semantics_per_class(X, y, k=10):\n",
    "    class_models = {}\n",
    "    class_means = {}\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        X_class = X[y == label]  # Prende solo le istanze della classe corrente\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_class)  # Normalizza i dati della classe\n",
    "\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        latent = svd.fit_transform(X_scaled)  # Riduzione dimensionale con SVD\n",
    "\n",
    "        # Salva modello SVD e scaler per la classe\n",
    "        class_models[label] = {\n",
    "            'svd': svd,\n",
    "            'scaler': scaler,\n",
    "            'latent_vectors': latent\n",
    "        }\n",
    "        # Calcola la media dei vettori latenti della classe\n",
    "        class_means[label] = np.mean(latent, axis=0)\n",
    "    return class_models, class_means\n",
    "\n",
    "def predict_label(X_test, class_models, class_means):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        best_label = None\n",
    "        min_dist = float('inf')\n",
    "        for label, model in class_models.items():\n",
    "            x_scaled = model['scaler'].transform(x.reshape(1, -1))  # Normalizza x\n",
    "            x_latent = model['svd'].transform(x_scaled)  # Trasforma in spazio latente\n",
    "            dist = np.linalg.norm(x_latent - class_means[label])  # Distanza dal centroide\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_label = label\n",
    "        y_pred.append(best_label)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    labels = np.unique(y_true)\n",
    "    print(\"Per-class metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\n",
    "            f\"Class {label}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f}\")\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    print(\"[VALUTAZIONE] Report di classificazione:\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8611765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento sui dati di Part1\n",
    "class_models, class_means = compute_latent_semantics_per_class(\n",
    "    feat_matrix_part1, lbls_part1, k=10)\n",
    "\n",
    "# Predizione su Part2\n",
    "predicted_labels = predict_label(feat_matrix_part2, class_models, class_means)\n",
    "\n",
    "# Valutazione\n",
    "evaluate(lbls_part2, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9819c57",
   "metadata": {},
   "source": [
    "# Task 8\n",
    "\n",
    "Implementa un programma che, per ciascuna etichetta univoca **l**, calcoli i **c cluster più significativi** associati alle immagini della **Parte 1**, utilizzando l'algoritmo **DBSCAN**.\n",
    "\n",
    "I cluster risultanti devono essere visualizzati in due modalità:\n",
    "- Come **nuvole di punti** colorate in modo diverso, proiettate in uno spazio a **2 dimensioni** tramite **MDS (Multidimensional Scaling)**.\n",
    "- Come **gruppi di miniature di immagini**, dove ogni gruppo rappresenta un cluster distinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864ca453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applichiamo tecniche di dimensionality reduction\n",
    "def reduce_features(features, method, n_components, random_state=42):\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=n_components)\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Metodo di riduzione '{method}' non supportato.\")\n",
    "\n",
    "    return reducer.fit_transform(features)\n",
    "\n",
    "#Applichiamo una tecnica di riduzione mediante reduce_feature alle features di partenza ottenendo la lista di Feature Latenti\n",
    "#Applichiamo su insieme di feature latenti StandardScaler per cercare di ottenere migliori cluster tramite DBSCAN\n",
    "\n",
    "def apply_dbscan_with_pca(features, eps, min_samples, n_components, method):\n",
    "    print(f\"Applicazione di {method} -> Riduzione a {n_components} componenti\")\n",
    "    reduced_features = reduce_features(features, method, n_components=n_components)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    reduced_scaled = scaler.fit_transform(reduced_features)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(reduced_scaled)\n",
    "    return labels\n",
    "\n",
    "#Calcola i 'c' cluster di maggior cardinalità\n",
    "def top_c_clusters(cluster_labels, c):\n",
    "    label_counts = Counter(cluster_labels)\n",
    "    label_counts.pop(-1, None) # rimozione cluster catalogato come rumore (-1)\n",
    "    if not label_counts:\n",
    "        print(\"[WARN] DBSCAN non ha trovato alcun cluster valido (solo rumore).\")\n",
    "        return []\n",
    "    \n",
    "    # Estraiamo i 'c' cluster più frequenti\n",
    "    most_common = label_counts.most_common(c)\n",
    "    top = [int(lbl) for lbl, _ in most_common]\n",
    "    \n",
    "    if len(top) < c:\n",
    "        print(f\"[WARN] DBSCAN ha trovato solo {len(top)} cluster (meno di {c}).\")\n",
    "    return top\n",
    "\n",
    "\n",
    "#Applichiamo al risultato di DBSCAN l'algoritmo di MDS\n",
    "def plot_mds_clusters(features, cluster_labels, top_clusters, metric):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    #Generazione di una nuova matrice basata sul parametro metric (es. 'cosine')\n",
    "    D = pairwise_distances(features_scaled, metric=metric)\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    Y = mds.fit_transform(D)\n",
    "\n",
    "    cmap= matplotlib.colormaps['tab10']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(Y)):\n",
    "        lbl = cluster_labels[i]\n",
    "        if lbl in top_clusters:\n",
    "            color_idx = top_clusters.index(lbl)\n",
    "            plt.scatter(Y[i,0], Y[i,1], color=cmap(color_idx), s=30, edgecolor='k', linewidth=0.2)\n",
    "        else:\n",
    "            # punti rumore o cluster “non top”\n",
    "            plt.scatter(Y[i,0], Y[i,1], color='lightgray', s=8)\n",
    "    \n",
    "    plt.title(f\"MDS 2D - Top {len(top_clusters)} cluster\")\n",
    "    plt.xlabel(\"MDS 1\")\n",
    "    plt.ylabel(\"MDS 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Genera blocco di immagini trovate per cluster\n",
    "def show_cluster_thumbnails(images, cluster_labels, top_clusters, thumb_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    images: lista (o array) di percorsi file (lunghezza N), \n",
    "            ossia a images[i] corrisponde features[i].\n",
    "    cluster_labels: array (N,) di cluster per ogni immagine.\n",
    "    top_clusters: lista dei c cluster (int) che vogliamo visualizzare.\n",
    "    thumb_size: dimensione (w,h) di ogni miniatura.\n",
    "    Per ogni cluster ∈ top_clusters stampa a video (o fa plt.show) \n",
    "    una griglia di miniature (fino a ~16-25 alla volta).\n",
    "    \"\"\"\n",
    "    for cluster_id in top_clusters:\n",
    "        # Indici di tutte le immagini che appartengono a questo cluster\n",
    "        idxs = [i for i, cl in enumerate(cluster_labels) if cl == cluster_id]\n",
    "        print(f\"[INFO] Cluster {cluster_id}: {len(idxs)} immagini trovate\")\n",
    "\n",
    "        # Se vogliamo limitare a N miniatura per cluster (tipo 16):\n",
    "        max_display = min(len(idxs), 16)\n",
    "        n = int(np.ceil(np.sqrt(max_display)))  # facciamo una griglia n×n\n",
    "        plt.figure(figsize=(n, n))\n",
    "\n",
    "        for j, i_img in enumerate(idxs[:max_display]):\n",
    "            img = Image.open(images[i_img]).convert('RGB')\n",
    "            img_thumb = img.resize(thumb_size, Image.LANCZOS)\n",
    "            \n",
    "            ax = plt.subplot(n, n, j+1)\n",
    "            plt.imshow(img_thumb)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Cluster {cluster_id} – {len(idxs)} immagini (mostrate: {max_display})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8938a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "def db_scan_detection(eps, min_samples, n_components, c, method):\n",
    "\n",
    "    # Costruisce l’elenco dei full path per tutte le immagini\n",
    "    base_folder = \"Part1\"  # o path assoluto \"/Users/.../Parte1\"\n",
    "    images_full = [os.path.join(base_folder, lbl, fname) for fname, lbl in zip(flname_part1, lbls_part1)]\n",
    "\n",
    "    # Scorre ogni label di Parte1 ed applico DBSCAN+PCA\n",
    "    unique_labels = np.unique(lbls_part1)  # es. [\"Glioma\",\"Meningioma\",\"Pituitary\"]\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"[INFO] Elaboro label: {lbl}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        #Estrae le righe di feat_matrix_part1 / flname_part1 corrispondenti\n",
    "        mask_lbl = (lbls_part1 == lbl)\n",
    "        features_label = feat_matrix_part1[mask_lbl]   # shape = (n_i, d)\n",
    "        images_label = np.array(images_full)[mask_lbl]\n",
    "\n",
    "        #Chiama la tua funzione PCA + DBSCAN\n",
    "        cluster_labels = apply_dbscan_with_pca(\n",
    "            features_label,\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            n_components=n_components,\n",
    "            method = method\n",
    "        )\n",
    "        print(f\"[INFO] Cluster-labels trovati: {np.unique(cluster_labels)}\")\n",
    "\n",
    "        # Trova i c cluster più grandi\n",
    "        top_clusters = top_c_clusters(cluster_labels, c)\n",
    "        print(f\"[INFO] Top {c} cluster (per dimensione): {top_clusters}\")\n",
    "\n",
    "        # MDS‐2D + scatter plot del clustering\n",
    "        print(f\"[INFO] Disegno MDS 2D per i cluster di '{lbl}' …\")\n",
    "\n",
    "        plot_mds_clusters(\n",
    "            features_label,\n",
    "            cluster_labels,\n",
    "            top_clusters,\n",
    "            metric='cosine'\n",
    "        )\n",
    "\n",
    "        # Creo le miniature di ogni cluster “significativo”\n",
    "        print(f\"[INFO] Genero miniature per ciascun cluster di '{lbl}' …\")\n",
    "        show_cluster_thumbnails(\n",
    "            images_label,      # array di stringhe di percorsi\n",
    "            cluster_labels,    # array di int di lunghezza n_i\n",
    "            top_clusters,      # la lista dei c indici di cluster\n",
    "            thumb_size=(64, 64)\n",
    "        )\n",
    "\n",
    "    print(\"\\n[FINITO] Task 8 completato per tutte le label di Parte1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5579abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 5.0            # valore DBSCAN di esempio\n",
    "min_samples = 3      # valore DBSCAN di esempio\n",
    "n_components = 50    # quante dimensioni tenere con PCA PRIMA di DBSCAN\n",
    "c = 3                # quanti cluster “significativi” voglio prendere per ciascuna label\n",
    "method = 'umap'      # umap or pca\n",
    "db_scan_detection(eps, min_samples, n_components, c, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9be37",
   "metadata": {},
   "source": [
    "# Task 9: Classificazione m-NN e Decision Tree\n",
    "\n",
    "Implementa un programma che, date le immagini della **Parte 1**:\n",
    "\n",
    "- Crei un classificatore **m-NN** (con **m** specificato dall'utente).\n",
    "- Crei un classificatore basato su **albero decisionale**.\n",
    "\n",
    "Per questo task puoi utilizzare **lo spazio delle feature a tua scelta**.\n",
    "\n",
    "Il programma deve poi:\n",
    "- Applicare il classificatore selezionato dall'utente per prevedere le etichette più probabili delle immagini della **Parte 2**.\n",
    "- Calcolare e visualizzare:\n",
    "  - **Precisione (Precision)** per etichetta,\n",
    "  - **Richiamo (Recall)** per etichetta,\n",
    "  - **Punteggio F1 (F1-score)** per etichetta,\n",
    "  - **Accuratezza complessiva (Accuracy)** del classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta il valore di m per l'm-NN\n",
    "m = 5  # Modifica questo valore in base alle tue necessità\n",
    "\n",
    "# Addestramento m-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=m)\n",
    "knn_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Addestramento Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Predizioni su Part2\n",
    "pred_knn = knn_model.predict(feat_matrix_part2)\n",
    "pred_dt = dt_model.predict(feat_matrix_part2)\n",
    "\n",
    "# Valutazione m-NN\n",
    "print(\"Risultati m-NN:\")\n",
    "print(classification_report(lbls_part2, pred_knn))\n",
    "print(\"Accuratezza complessiva m-NN:\", accuracy_score(lbls_part2, pred_knn))\n",
    "\n",
    "# Valutazione Decision Tree\n",
    "print(\"Risultati Decision Tree:\")\n",
    "print(classification_report(lbls_part2, pred_dt))\n",
    "print(\"Accuratezza complessiva Decision Tree:\", accuracy_score(lbls_part2, pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d147a1",
   "metadata": {},
   "source": [
    "# Task 10: Locality Sensitive Hashing e Ricerca di Immagini Simili\n",
    "\n",
    "## 10a: Creazione dell'indice LSH\n",
    "\n",
    "Implementa uno strumento di **Locality Sensitive Hashing (LSH)** (per la distanza euclidea), che prende come input:\n",
    "- **L**: Numero di livelli,\n",
    "- **h**: Numero di hash per livello,\n",
    "- Un insieme di vettori (feature delle immagini).\n",
    "\n",
    "Il programma deve creare **una struttura di indice in memoria** contenente l'insieme di vettori dato.\n",
    "\n",
    "> Riferimento:  \n",
    "> \"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\",  \n",
    "> Alexandr Andoni e Piotr Indyk, *Communications of the ACM*, vol. 51, no. 1, 2008, pp. 117–122.\n",
    "\n",
    "---\n",
    "\n",
    "## 11b: Ricerca di immagini simili con LSH\n",
    "\n",
    "Implementa un algoritmo di ricerca di immagini simili utilizzando la struttura LSH creata in 11a, memorizzando le immagini della **Parte 1** e un modello visivo di tua scelta (**il modello visivo combinato deve avere almeno 256 dimensioni**).\n",
    "\n",
    "Per una data immagine di query e un numero intero **t**, il programma deve:\n",
    "\n",
    "- Visualizzare le **t immagini più simili**.\n",
    "- Mostrare:\n",
    "  - Il **numero di immagini uniche** considerate,\n",
    "  - Il **numero totale di immagini analizzate** durante il processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe LSH con quantizzazione\n",
    "\n",
    "class LSH_EuclideanQuantized:\n",
    "    \"\"\"\n",
    "    LSH per distanza Euclidea (p-stable) con bucket width r.\n",
    "    Ogni hash h_j(v) = floor((a_j · v + b_j) / r).\n",
    "\n",
    "    Parametri:\n",
    "      - num_layers   = L = numero di tavole hash\n",
    "      - num_hashes   = h = numero di functions concatenati in ciascuna tavola\n",
    "      - dim          = D = dimensione dei vettori di input\n",
    "      - r            = bucket width (parte intera di quantizzazione)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_hashes: int, dim: int, r: float):\n",
    "        self.L = num_layers\n",
    "        self.h = num_hashes\n",
    "        self.d = dim\n",
    "        self.r = r\n",
    "\n",
    "        # Prepara L tavole hash: ciascuna è un dict (chiave tuple di h interi -> lista di indici)\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.L)]\n",
    "\n",
    "        # Per ogni layer l=0..L-1, e per ogni j=0..h-1, genero:\n",
    "        #   - a_lj  vettore gaussiano di dimensione D\n",
    "        #   - b_lj  offset (uniforme in [0, r) )\n",
    "        self.a_vectors = [\n",
    "            [np.random.randn(self.d) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.b_offsets = [\n",
    "            [np.random.uniform(0, self.r) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "\n",
    "        # Memorizzerò i vettori originali di Part1 in questo array, shape=(N, D)\n",
    "        self.data_vectors = None\n",
    "\n",
    "    def _compute_hash_tuple(self, vec: np.ndarray, layer_idx: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Calcola l'hash (h interi) per il layer layer_idx su un vettore vec:\n",
    "          h_j = floor((a_vectors[layer_idx][j] · vec + b_offsets[layer_idx][j]) / r)\n",
    "        Ritorna una tupla di h interi.\n",
    "        \"\"\"\n",
    "        keys = []\n",
    "        a_vs = self.a_vectors[layer_idx]\n",
    "        b_os = self.b_offsets[layer_idx]\n",
    "        for j in range(self.h):\n",
    "            a_j = a_vs[j]         # vettore dimensione D\n",
    "            b_j = b_os[j]         # float in [0, r)\n",
    "            proj = float(np.dot(a_j, vec) + b_j)\n",
    "            h_val = int(np.floor(proj / self.r))\n",
    "            keys.append(h_val)\n",
    "        return tuple(keys)\n",
    "\n",
    "    def index(self, vectors: np.ndarray):\n",
    "        \"\"\"\n",
    "        Costruisci l'indice LSH su un insieme di vettori di Part1:\n",
    "          vectors: numpy array shape = (N, D)\n",
    "        Al termine di questa chiamata:\n",
    "          - self.data_vectors = vectors\n",
    "          - self.hash_tables[l][hash_tuple] conterrà la lista di indici i per cui\n",
    "            hash_tuple = _compute_hash_tuple(vectors[i], l).\n",
    "        \"\"\"\n",
    "        self.data_vectors = vectors\n",
    "        N, D = vectors.shape\n",
    "        assert D == self.d, f\"Errore: dimensione vettore ({D}) ≠ atteso ({self.d}).\"\n",
    "\n",
    "        # Inserisco ogni vettore in ciascuna tavola hash\n",
    "        for idx in range(N):\n",
    "            v = vectors[idx]\n",
    "            for l in range(self.L):\n",
    "                key = self._compute_hash_tuple(v, l)\n",
    "                self.hash_tables[l][key].append(idx)\n",
    "\n",
    "    def query(self, q_vec: np.ndarray, top_t: int = 5):\n",
    "        \"\"\"\n",
    "        Esegui una query LSH per cercare i top_t vettori più vicini a q_vec.\n",
    "        Restituisce:\n",
    "          - top_results: lista di tuple (indice, distanza) ord. per dist. crescente\n",
    "          - unique_count: numero di indici distinti considerati (cardinalità dei candidati)\n",
    "          - total_checked: somma della lunghezza di tutti i bucket esaminati\n",
    "        \"\"\"\n",
    "        assert q_vec.shape[0] == self.d, \"Errore: dimensione query ≠ D.\"\n",
    "        candidati = set()\n",
    "        total_checked = 0\n",
    "\n",
    "        # Per ciascun layer, ottengo la chiave polidimensionale e i suoi bucket\n",
    "        for l in range(self.L):\n",
    "            h_tuple = self._compute_hash_tuple(q_vec, l)\n",
    "            bucket = self.hash_tables[l].get(h_tuple, [])\n",
    "            total_checked += len(bucket)\n",
    "            candidati.update(bucket)\n",
    "\n",
    "        # Ora calcolo la distanza euclidea esatta tra q_vec e ciascun candidato\n",
    "        risultati = []\n",
    "        for idx in candidati:\n",
    "            v_i = self.data_vectors[idx]\n",
    "            dist = np.linalg.norm(v_i - q_vec)\n",
    "            risultati.append((idx, dist))\n",
    "\n",
    "        # Ordino e prendo i primi top_t\n",
    "        risultati.sort(key=lambda x: x[1])\n",
    "        top_results = risultati[:top_t]\n",
    "        return top_results, len(candidati), total_checked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costruzione LSH_EuclideanQuantized su Part1\n",
    "\n",
    "# 1) (Opzionale ma consigliato) centra e normalizza i vettori di Part1\n",
    "#    Questo passaggio riduce l'effetto di scale diverse e spesso migliora la qualità LSH\n",
    "mean_vec = np.mean(feat_matrix_part1, axis=0)\n",
    "feat_centered = feat_matrix_part1 - mean_vec\n",
    "feat_normed = normalize(feat_centered, norm='l2', axis=1)\n",
    "\n",
    "# 2) Parametri LSH\n",
    "D = feat_normed.shape[1]      # di solito 900\n",
    "L = 10                         # numero di tavole hash (scegli in base a esperimenti)\n",
    "h = 7                      # numero di funzioni concatenati in ciascuna tavola\n",
    "r = 1                       # parametro di larghezza (esempio: 0.5); puoi sperimentare\n",
    "\n",
    "# 3) Creo l'oggetto e indicizzo\n",
    "lsh_quant = LSH_EuclideanQuantized(num_layers=L, num_hashes=h, dim=D, r=r)\n",
    "lsh_quant.index(feat_normed)\n",
    "\n",
    "print(f\"[INFO] LSH quantizzato costruito: D={D}, L={L}, h={h}, r={r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per cercare top_k con LSH quantizzato\n",
    "\n",
    "def find_k_similar_lsh_quant(base_folder: str, img_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Cerca le k immagini di Part1 più simili a img_path (di Part2) usando LSH_EuclideanQuantized.\n",
    "    Stampa:\n",
    "      - i primi k risultati (file name, label, distanza)\n",
    "      - il numero di immagini uniche considerate\n",
    "      - il numero totale di controlli (somma delle lunghezze dei bucket)\n",
    "    E poi visualizza (query + k risultati) con matplotlib.\n",
    "    \"\"\"\n",
    "    # 1) Estrazione feature raw (900-dim) con la funzione esistente\n",
    "    raw_q = np.array(extract_hog_features(img_path), dtype=np.float32)\n",
    "\n",
    "    # 2) Center + normalize (stesso mean_vec e L2 norm usati su Part1)\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # 3) Chiamata a LSH\n",
    "    top_results, unique_count, total_checked = lsh_quant.query(q_normed, top_t=k)\n",
    "\n",
    "    # 4) Stampa output testuale\n",
    "    print(f\"\\n[LSH-Quant] Top {k} simili a: {img_path}\")\n",
    "    for rank, (idx, dist) in enumerate(top_results, start=1):\n",
    "        label = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        print(f\"  {rank}. {fname} | Classe: {label} | Distanza Euclidea: {dist:.2f}\")\n",
    "    print(f\"[LSH-Quant] Immagini uniche considerate: {unique_count}\")\n",
    "    print(f\"[LSH-Quant] Immagini totali controllate: {total_checked}\")\n",
    "\n",
    "    # 5) Visualizzazione (query + primi k risultati)\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(4*(k+1), 4))\n",
    "    img_q = cv2.imread(img_path)\n",
    "    img_q = cv2.cvtColor(img_q, cv2.COLOR_BGR2RGB)\n",
    "    axs[0].imshow(img_q)\n",
    "    axs[0].set_title(\"Query (LSH-Quant)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, (idx, dist) in enumerate(top_results, start=1):\n",
    "        lab = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        full_path = os.path.join(base_folder, lab, fname)\n",
    "        img_match = cv2.imread(full_path)\n",
    "        img_match = cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img_match)\n",
    "        axs[i].set_title(f\"Rank {i}\\nd={dist:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd729c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di utilizzo su un'immagine di Part2 ---\n",
    "query_path = \"Part2/brain_glioma/brain_glioma_1409.jpg\"\n",
    "\n",
    "k = 7                         # numero di immagini simili da visualizzare\n",
    "\n",
    "# Eseguo la ricerca LSH\n",
    "find_k_similar_lsh_quant(\"Part1\", query_path, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
