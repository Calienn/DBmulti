{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d663937",
   "metadata": {},
   "source": [
    "Metodo\n",
    "\n",
    "ResNet-AvgPool-1024: Ridimensiona l'immagine a 224x24; collega un \"hook\" all'output del livello \"avgpool\" dell'architettura pre-addestrata ResNet per ottenere un vettore di dimensione 2048, riduci il numero di dimensioni del vettore a 1024 calcolando la media di due voci consecutive nel vettore. \n",
    "\n",
    "** Spiegare il metodo nel dettaglio ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b047aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Built-in ===\n",
    "import os\n",
    "\n",
    "# === Third-party ===\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report,pairwise_distances\n",
    "\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c56c11",
   "metadata": {},
   "source": [
    "# Task 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbd33a",
   "metadata": {},
   "source": [
    "Implementa un programma che estrae e memorizza i descrittori di feature per tutte le immagini nel set di dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0747cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetAvgPool1024Extractor:\n",
    "    def __init__(self):\n",
    "        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.model.eval()\n",
    "        self.feature = None\n",
    "        self.model.avgpool.register_forward_hook(self._hook_avgpool)\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 24)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def _hook_avgpool(self, module, input, output):\n",
    "        self.feature = output.squeeze().detach().numpy()\n",
    "\n",
    "    def extract_feature(self, image_path):\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.transform(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input_tensor)\n",
    "\n",
    "        feat_2048 = self.feature\n",
    "        return 0.5 * (feat_2048[::2] + feat_2048[1::2])  # Ridotto a 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a74ba",
   "metadata": {},
   "source": [
    "Estrazione,Salvataggio e Caricamneto delle Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4708e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_features(base_folder, subfolders, output_file):\n",
    "    extractor = ResNetAvgPool1024Extractor()\n",
    "    all_features, all_filenames, all_labels = [], [], []\n",
    "\n",
    "    for label in subfolders:\n",
    "        folder_path = os.path.join(base_folder, label)\n",
    "        print(f\"[INFO] Elaboro cartella: {label}\")\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                features = extractor.extract_feature(img_path)\n",
    "                if features is not None:\n",
    "                    all_features.append(features)\n",
    "                    all_filenames.append(filename)\n",
    "                    all_labels.append(label)\n",
    "\n",
    "    np.savez(output_file,\n",
    "             features=np.array(all_features),\n",
    "             filenames=np.array(all_filenames),\n",
    "             labels=np.array(all_labels))\n",
    "    \n",
    "    print(f\"[SALVATO] Features salvate in {output_file}\")\n",
    "    print(f\"[FINE] Totale immagini processate: {len(all_features)}\")\n",
    "\n",
    "\n",
    "def load_features(file_path):\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    return data[\"features\"], data[\"filenames\"], data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac58be",
   "metadata": {},
   "source": [
    "Visualizzazione dei Risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad30579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(query_path, indices, labels, filenames, distances, metric_label):\n",
    "    k = len(indices)\n",
    "    fig, axs = plt.subplots(1, k + 1, figsize=(15, 5))\n",
    "\n",
    "    # Immagine query\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(query_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Immagini simili\n",
    "    for i, idx in enumerate(indices):\n",
    "        img_path = os.path.join(\"Part1\", labels[idx], filenames[idx])\n",
    "        match_img = cv2.imread(img_path)\n",
    "        axs[i + 1].imshow(cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB))\n",
    "        axs[i + 1].set_title(f\"Rank {i+1}\\n{metric_label}={distances[idx]:.2f}\")\n",
    "        axs[i + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5aeb94",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727d33e",
   "metadata": {},
   "source": [
    " Implementa un programma che, dato il nome di un file immagine e un valore \"k\", restituisce e visualizza le k immagini più simili in base a ciascun modello visivo - selezionerai l'appropriata misura di distanza/similarità per ciascun modello di feature.  Per ogni corrispondenza, elenca anche il corrispondente punteggio di distanza/similarità. \n",
    "\n",
    "  *Retrieval: Immagini più Simili (distanza coseno)*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a1e2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar(img_path, k, extractor, features, filenames, labels):\n",
    "    query_feature = extractor.extract_feature(img_path)\n",
    "    if query_feature is None:\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "    dist_euc = euclidean_distances(features, query_feature).flatten()\n",
    "    dist_cos = cosine_distances(features, query_feature).flatten()\n",
    "\n",
    "    top_k_idx_euc = np.argsort(dist_euc)[:k]\n",
    "    top_k_idx_cos = np.argsort(dist_cos)[:k]\n",
    "\n",
    "    print(f\"\\nTop {k} simili a {img_path}:\")\n",
    "    print(\"=== Euclide ===\")\n",
    "    for rank, idx in enumerate(top_k_idx_euc):\n",
    "        print(f\"{rank+1}. {filenames[idx]} | Classe: {labels[idx]} | Euclid: {dist_euc[idx]:.4f}\")\n",
    "\n",
    "    print(\"=== Coseno ===\")\n",
    "    for rank, idx in enumerate(top_k_idx_cos):\n",
    "        print(f\"{rank+1}. {filenames[idx]} | Classe: {labels[idx]} | Cosine: {dist_cos[idx]:.4f}\")\n",
    "\n",
    "    # Visualizza entrambi\n",
    "    plot_matches(img_path, top_k_idx_euc, labels, filenames, dist_euc, \"Euclid\")\n",
    "    plot_matches(img_path, top_k_idx_cos, labels, filenames, dist_cos, \"Cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d957217",
   "metadata": {},
   "source": [
    "Ricerca Immagini Simili - Mahalanobis -- Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "220b43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_mahalanobis(img_path, k, extractor, features, filenames, labels):\n",
    "    query_feature = extractor.extract_feature(img_path)\n",
    "    if query_feature is None:\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature)\n",
    "    cov = np.cov(features.T)\n",
    "    try:\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"[ERRORE] Covarianza non invertibile. Uso pseudoinversa.\")\n",
    "        cov_inv = np.linalg.pinv(cov)\n",
    "\n",
    "    distances = np.array([mahalanobis(query_feature, f, cov_inv) for f in features])\n",
    "    top_k_idx = np.argsort(distances)[:k]\n",
    "\n",
    "    print(f\"\\nTop {k} simili (Mahalanobis) a {img_path}:\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        print(f\"{rank+1}. {filenames[idx]} | Classe: {labels[idx]} | Distanza: {distances[idx]:.2f}\")\n",
    "\n",
    "    plot_matches(img_path, top_k_idx, labels, filenames, distances, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbba47",
   "metadata": {},
   "source": [
    "Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parametri ===\n",
    "subfolders = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]\n",
    "\n",
    "# === Estrai e salva features (solo una volta) ===\n",
    "process_and_save_features(\"Part1\", subfolders, \"resnet1024_pt1.npz\")\n",
    "process_and_save_features(\"Part2\", subfolders, \"resnet1024_pt2.npz\")\n",
    "\n",
    "# === Carica features ===\n",
    "features_pt1, filenames_pt1, labels_pt1 = load_features(\"resnet1024_pt1.npz\")\n",
    "features_pt2, filenames_pt2, labels_pt2 = load_features(\"resnet1024_pt2.npz\")\n",
    "\n",
    "# === Inizializza extractor ===\n",
    "extractor = ResNetAvgPool1024Extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5298a",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Query ===\n",
    "query_img = \"Part1/brain_glioma/brain_glioma_0005.jpg\"\n",
    "find_k_similar(query_img, k=5, extractor=extractor, features=features_pt1, filenames=filenames_pt1, labels=labels_pt1)\n",
    "find_k_similar_mahalanobis(query_img, k=5, extractor=extractor, features=features_pt1, filenames=filenames_pt1, labels=labels_pt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339aa08",
   "metadata": {},
   "source": [
    "# Task 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfbbf52",
   "metadata": {},
   "source": [
    " Implementa un programma che, dati (a) un file immagine di query della parte 2, (b) uno spazio di feature selezionato dall'utente e (c) un numero intero positivo k (k<=2), identifica ed elenca le k etichette di corrispondenza più probabili, insieme ai loro punteggi, nello spazio di feature selezionato.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fbc5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(query_feat, target_feats, metric=\"euclidean\"):\n",
    "    query_feat = query_feat.reshape(1, -1)\n",
    "    if metric == \"euclidean\":\n",
    "        return euclidean_distances(target_feats, query_feat).flatten()\n",
    "    elif metric == \"cosine\":\n",
    "        return cosine_similarity(query_feat, target_feats).flatten()\n",
    "    else:\n",
    "        raise ValueError(\"Metric must be 'euclidean' or 'cosine'\")\n",
    "\n",
    "def predict_top_k_labels_distance_mean_1024(query_img_path, k, extractor, features, labels, metric=\"euclidean\"):\n",
    "    query_feat = extractor.extract_feature(query_img_path)\n",
    "    if query_feat is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    scores = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        class_feats = features[labels == label]\n",
    "        dists_or_sims = compute_metric(query_feat, class_feats, metric)\n",
    "        score = dists_or_sims.mean()\n",
    "        scores.append(score)\n",
    "\n",
    "    if metric == \"euclidean\":\n",
    "        top_k = np.argsort(scores)[:k]\n",
    "    else:\n",
    "        top_k = np.argsort(scores)[::-1][:k]\n",
    "\n",
    "    print(f\"\\n[STRATEGIA: distanza media - metrica: {metric}]\")\n",
    "    for idx in top_k:\n",
    "        print(f\"Classe: {unique_labels[idx]} | Score medio: {scores[idx]:.4f}\")\n",
    "\n",
    "def predict_top_k_labels_prototype_1024(query_img_path, k, extractor, features, labels, metric=\"euclidean\"):\n",
    "    query_feat = extractor.extract_feature(query_img_path)\n",
    "    if query_feat is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_prototypes = []\n",
    "    for label in unique_labels:\n",
    "        class_feats = features[labels == label]\n",
    "        class_prototypes.append(class_feats.mean(axis=0))\n",
    "    class_prototypes = np.vstack(class_prototypes)\n",
    "\n",
    "    scores = compute_metric(query_feat, class_prototypes, metric)\n",
    "\n",
    "    if metric == \"euclidean\":\n",
    "        top_k = np.argsort(scores)[:k]\n",
    "    else:\n",
    "        top_k = np.argsort(scores)[::-1][:k]\n",
    "\n",
    "    print(f\"\\n[STRATEGIA: prototipo di classe - metrica: {metric}]\")\n",
    "    for idx in top_k:\n",
    "        print(f\"Classe: {unique_labels[idx]} | Score: {scores[idx]:.4f}\")\n",
    "\n",
    "def task4_predict_labels_1024(query_img_path, k, extractor, features, labels, metric=\"euclidean\"):\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "    print(f\"\\n======== PREDIZIONE PER: {os.path.basename(query_img_path)} ========\")\n",
    "    predict_top_k_labels_distance_mean_1024(query_img_path, k, extractor, features, labels, metric)\n",
    "    predict_top_k_labels_prototype_1024(query_img_path, k, extractor, features, labels, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091418b",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3167b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_menin/brain_menin_1202.jpg\"\n",
    "task4_predict_labels_1024(query_img, k=2, extractor=extractor, features=features_pt2, labels=labels_pt2, metric=\"euclidean\")\n",
    "task4_predict_labels_1024(query_img, k=2, extractor=extractor, features=features_pt2, labels=labels_pt2, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9e030",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c8c59",
   "metadata": {},
   "source": [
    "Implementa un programma che (a) dato uno dei modelli di feature, (b) un valore k specificato dall'utente, (c) una delle tre tecniche di riduzione della dimensionalità (SVD, LDA, k-means) scelte dall'utente, riporta le prime k semantiche latenti estratte nello spazio di feature selezionato.    \n",
    "\n",
    "-Memorizza le semantiche latenti in un file di output adeguatamente nominato.    \n",
    "\n",
    "-Elenca le coppie imageID-peso, ordinate in ordine decrescente di pesi.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2220da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task5_latent_semantics_resnet1024(feature_model_path, technique, k):\n",
    "    \"\"\"\n",
    "    Estrae i top-k concetti latenti da uno spazio di feature ResNet AvgPool1024\n",
    "    usando SVD, LDA o KMeans. Salva i risultati su file e visualizza lo spazio latente.\n",
    "    \"\"\"\n",
    "    # === Carica feature ===\n",
    "    data = np.load(feature_model_path, allow_pickle=True)\n",
    "    feature_matrix = data[\"features\"]\n",
    "    filenames = data[\"filenames\"]\n",
    "    labels = data[\"labels\"]\n",
    "\n",
    "    technique = technique.lower()\n",
    "\n",
    "    if technique == \"svd\":\n",
    "        model = TruncatedSVD(n_components=k, random_state=42)\n",
    "        X_transformed = model.fit_transform(feature_matrix)\n",
    "        components = model.components_\n",
    "        method = \"svd\"\n",
    "\n",
    "    elif technique == \"lda\":\n",
    "        unique_labels = np.unique(labels)\n",
    "        max_k = len(unique_labels) - 1\n",
    "        if k > max_k:\n",
    "            print(f\"[ATTENZIONE] LDA supporta al massimo {max_k} componenti con {len(unique_labels)} classi.\")\n",
    "            k = max_k\n",
    "        model = LDA(n_components=k)\n",
    "        X_transformed = model.fit_transform(feature_matrix, labels)\n",
    "        components = model.scalings_.T[:k]\n",
    "        method = \"lda\"\n",
    "\n",
    "    elif technique == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        model.fit(feature_matrix)\n",
    "        components = model.cluster_centers_\n",
    "        X_transformed = model.transform(feature_matrix)\n",
    "        method = \"kmeans\"\n",
    "    else:\n",
    "        print(\"[ERRORE] Tecnica non supportata. Usa: 'svd', 'lda', 'kmeans'\")\n",
    "        return\n",
    "\n",
    "    # === Visualizzazione 2D ===\n",
    "    if technique in [\"svd\", \"lda\"]:\n",
    "        plot_latent_space_2d(X_transformed, labels, technique, k)\n",
    "    elif technique == \"kmeans\":\n",
    "        plot_kmeans_clusters_2d(feature_matrix, labels, k)\n",
    "\n",
    "    # === Salvataggio output ===\n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = f\"latent_semantics_{method}_{base_name}_k{k}.txt\"\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for i in range(k):\n",
    "            f.write(f\"\\n--- Latent Semantic {i+1} ---\\n\")\n",
    "            if technique in [\"svd\", \"lda\"]:\n",
    "                weights = feature_matrix @ components[i].T\n",
    "            else:\n",
    "                weights = -X_transformed[:, i]  # distanza inversa per KMeans\n",
    "\n",
    "            sorted_idx = np.argsort(-np.abs(weights))\n",
    "            for idx in sorted_idx:\n",
    "                f.write(f\"{filenames[idx]} | Peso: {weights[idx]:.4f} | Classe: {labels[idx]}\\n\")\n",
    "\n",
    "    print(f\"[SALVATO] Latent semantics salvati in: {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92a13af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space_2d(X_transformed, labels, technique, k):\n",
    "    \"\"\"Visualizza la proiezione 2D dello spazio latente.\"\"\"\n",
    "    if X_transformed.shape[1] < 2:\n",
    "        print(\"[INFO] Meno di 2 componenti: impossibile visualizzare in 2D.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=labels, palette=\"Set2\", s=80)\n",
    "    plt.title(f\"{technique.upper()} - Proiezione sulle prime 2 componenti latenti (k={k})\")\n",
    "    plt.xlabel(\"Componente 1\")\n",
    "    plt.ylabel(\"Componente 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_kmeans_clusters_2d(feature_matrix, labels, n_clusters):\n",
    "    \"\"\"Visualizza i cluster KMeans in 2D usando SVD per proiezione.\"\"\"\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd.fit_transform(feature_matrix)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_labels, palette='tab10', s=80, style=labels)\n",
    "    plt.title(f\"KMeans Clustering (k={n_clusters}) con proiezione SVD 2D\")\n",
    "    plt.xlabel(\"Componente Latente 1 (da SVD)\")\n",
    "    plt.ylabel(\"Componente Latente 2 (da SVD)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6776db",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ESEMPIO USO ===\n",
    "task5_latent_semantics_resnet1024(\"resnet1024_pt1.npz\", technique=\"svd\", k=5)\n",
    "task5_latent_semantics_resnet1024(\"resnet1024_pt1.npz\", technique=\"lda\", k=2)\n",
    "task5_latent_semantics_resnet1024(\"resnet1024_pt1.npz\", technique=\"kmeans\", k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ba51e",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f60357",
   "metadata": {},
   "source": [
    "\n",
    ">Task 6a:\n",
    "\n",
    " Implementa un programma che calcola e stampa la \"inherent dimensionality\" associata alle immagini della parte 1.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fc88464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_intrinsic_dimensionality(feature_matrix, threshold, plot=True):\n",
    "    max_components = min(feature_matrix.shape)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(feature_matrix)\n",
    "\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "    intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cumulative, marker='o', label=\"Varianza cumulativa\")\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Soglia {threshold*100:.0f}%\")\n",
    "        plt.axvline(x=intrinsic_dim, color='g', linestyle='--', label=f\"k suggerito: {intrinsic_dim}\")\n",
    "        plt.xlabel(\"Numero componenti\")\n",
    "        plt.ylabel(\"Varianza cumulativa\")\n",
    "        plt.title(\"Scelta ottimale di k (PCA su ResNet1024)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"[INFO] k ottimale suggerito (soglia {threshold*100:.0f}%): {intrinsic_dim}\")\n",
    "    return intrinsic_dim, cumulative\n",
    "\n",
    "\n",
    "def suggest_k(feature_matrix, threshold_list=[0.90, 0.95, 0.99]):\n",
    "    print(f\"[INFO] Feature matrix shape: {feature_matrix.shape}\")\n",
    "    k_values = {}\n",
    "    for t in threshold_list:\n",
    "        k, _ = estimate_intrinsic_dimensionality(feature_matrix, threshold=t, plot=False)\n",
    "        k_values[t] = k\n",
    "        print(f\"Soglia {int(t*100)}% : k = {k}\")\n",
    "    return k_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec095d3a",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTask 6a – Dimensionalità intrinseca (globale):\\n\")\n",
    "k_suggeriti_resnet1024 = suggest_k(features_pt1)\n",
    "estimate_intrinsic_dimensionality(features_pt1, threshold=0.95, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff70c8",
   "metadata": {},
   "source": [
    "\n",
    "> Task 6b: \n",
    "\n",
    "Implementa un programma che calcola e stampa la \"dimensionalità intrinseca\" (numero di dim indipendenti minime necassari per rappresentare set) associata a ciascuna etichetta univoca delle immagini della parte 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58027c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality_per_label(feature_matrix, labels, threshold):\n",
    "    label_dim_map = {}\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"[INFO] Etichette uniche trovate: {len(unique_labels)}\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(np.array(labels) == label)[0]\n",
    "        label_features = feature_matrix[indices]\n",
    "\n",
    "        if len(indices) < 2:\n",
    "            print(f\"[AVVISO] Label '{label}' ha meno di 2 campioni — ignorata.\")\n",
    "            continue\n",
    "\n",
    "        k, _ = estimate_intrinsic_dimensionality(label_features, threshold=threshold, plot=False)\n",
    "        label_dim_map[label] = k\n",
    "        print(f\" • Label '{label}' : k = {k}\")\n",
    "    return label_dim_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93736c94",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTask 6b – Dimensionalità intrinseca per etichetta (ResNet1024):\\n\")\n",
    "label_dimensionalities_resnet1024 = estimate_dimensionality_per_label(\n",
    "    features_pt1, labels_pt1, threshold=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5933f4b",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b02f5",
   "metadata": {},
   "source": [
    "Implementa un programma che,per ciascuna etichetta univoca l, calcola le corrispondenti k semantiche latenti (a tua scelta) associate alle immagini della parte 1, e\n",
    "per le immagini della parte 2, prevede le etichette più probabili utilizzando distanze/similarità calcolate sotto le semantiche latenti specifiche dell'etichetta.    \n",
    "Il sistema dovrebbe anche fornire valori di precision, recall, and F1-score per etichetta, nonché un valore di accuratezza complessiva.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eed5a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_semantics_per_class_resnet(X, y, k=10):\n",
    "    class_models = {}\n",
    "    class_means = {}\n",
    "    labels = np.unique(y)\n",
    "\n",
    "    for label in labels:\n",
    "        X_class = X[np.array(y) == label]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_class)\n",
    "\n",
    "        svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "        latent = svd.fit_transform(X_scaled)\n",
    "\n",
    "        class_models[label] = {\n",
    "            'scaler': scaler,\n",
    "            'svd': svd,\n",
    "            'latent_vectors': latent\n",
    "        }\n",
    "        class_means[label] = np.mean(latent, axis=0)\n",
    "    return class_models, class_means\n",
    "\n",
    "def predict_labels_with_latent_semantics(X_test, class_models, class_means):\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        best_label = None\n",
    "        min_dist = float('inf')\n",
    "        for label, model in class_models.items():\n",
    "            x_scaled = model['scaler'].transform(x.reshape(1, -1))\n",
    "            x_latent = model['svd'].transform(x_scaled)\n",
    "            dist = np.linalg.norm(x_latent - class_means[label])\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_label = label\n",
    "        predictions.append(best_label)\n",
    "    return predictions\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    print(\"[VALUTAZIONE] Metriche per classe:\")\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    for label, p, r, f in zip(np.unique(y_true), precision, recall, f1):\n",
    "        print(f\"Classe {label}: Precisione={p:.2f}, Recall={r:.2f}, F1-Score={f:.2f}\")\n",
    "    print(f\"\\nAccuratezza globale: {accuracy:.2f}\\n\")\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e6fea",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ESECUZIONE ======\n",
    "\n",
    "# Step 1: addestramento sulle immagini Part1\n",
    "class_models_resnet1024, class_means_resnet1024 = compute_latent_semantics_per_class_resnet(\n",
    "    features_pt1, labels_pt1, k=10\n",
    ")\n",
    "\n",
    "# Step 2: predizione sulle immagini Part2\n",
    "predicted_labels_resnet1024 = predict_labels_with_latent_semantics(\n",
    "    features_pt2, class_models_resnet1024, class_means_resnet1024\n",
    ")\n",
    "\n",
    "# Step 3: valutazione delle predizioni\n",
    "evaluate_predictions(labels_pt2, predicted_labels_resnet1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b5b63",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95a141",
   "metadata": {},
   "source": [
    "Implementa un programma che, per ciascuna etichetta univoca l, calcola i corrispondenti c cluster più significativi associati alle immagini della parte 1 (utilizzando l'algoritmo DBScan);\n",
    "i cluster risultanti devono essere visualizzati sia\n",
    "come nuvole di punti colorate in modo diverso in uno spazio MDS a 2 dimensioni, sia\n",
    "come gruppi di miniature di immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0410de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applichiamo tecniche di dimensionality reduction\n",
    "def reduce_features(features, method, n_components, random_state=42):\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=n_components)\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Metodo di riduzione '{method}' non supportato.\")\n",
    "\n",
    "    return reducer.fit_transform(features)\n",
    "\n",
    "#Applichiamo una tecnica di riduzione mediante reduce_feature alle features di partenza ottenendo la lista di Feature Latenti\n",
    "#Applichiamo su insieme di feature latenti StandardScaler per cercare di ottenere migliori cluster tramite DBSCAN\n",
    "\n",
    "def apply_dbscan_with_pca(features, eps, min_samples, n_components, method):\n",
    "    print(f\"Applicazione di {method} -> Riduzione a {n_components} componenti\")\n",
    "    reduced_features = reduce_features(features, method, n_components=n_components)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    reduced_scaled = scaler.fit_transform(reduced_features)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(reduced_scaled)\n",
    "    return labels\n",
    "\n",
    "#Calcola i 'c' cluster di maggior cardinalità\n",
    "def top_c_clusters(cluster_labels, c):\n",
    "    label_counts = Counter(cluster_labels)\n",
    "    label_counts.pop(-1, None) # rimozione cluster catalogato come rumore (-1)\n",
    "    if not label_counts:\n",
    "        print(\"[WARN] DBSCAN non ha trovato alcun cluster valido (solo rumore).\")\n",
    "        return []\n",
    "    \n",
    "    # Estraiamo i 'c' cluster più frequenti\n",
    "    most_common = label_counts.most_common(c)\n",
    "    top = [int(lbl) for lbl, _ in most_common]\n",
    "    \n",
    "    if len(top) < c:\n",
    "        print(f\"[WARN] DBSCAN ha trovato solo {len(top)} cluster (meno di {c}).\")\n",
    "    return top\n",
    "\n",
    "\n",
    "#Applichiamo al risultato di DBSCAN l'algoritmo di MDS\n",
    "def plot_mds_clusters(features, cluster_labels, top_clusters, metric):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    #Generazione di una nuova matrice basata sul parametro metric (es. 'cosine')\n",
    "    D = pairwise_distances(features_scaled, metric=metric)\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    Y = mds.fit_transform(D)\n",
    "\n",
    "    cmap= matplotlib.colormaps['tab10']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(Y)):\n",
    "        lbl = cluster_labels[i]\n",
    "        if lbl in top_clusters:\n",
    "            color_idx = top_clusters.index(lbl)\n",
    "            plt.scatter(Y[i,0], Y[i,1], color=cmap(color_idx), s=30, edgecolor='k', linewidth=0.2)\n",
    "        else:\n",
    "            # punti rumore o cluster “non top”\n",
    "            plt.scatter(Y[i,0], Y[i,1], color='lightgray', s=8)\n",
    "    \n",
    "    plt.title(f\"MDS 2D - Top {len(top_clusters)} cluster\")\n",
    "    plt.xlabel(\"MDS 1\")\n",
    "    plt.ylabel(\"MDS 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Genera blocco di immagini trovate per cluster\n",
    "def show_cluster_thumbnails(images, cluster_labels, top_clusters, thumb_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    images: lista (o array) di percorsi file (lunghezza N), \n",
    "            ossia a images[i] corrisponde features[i].\n",
    "    cluster_labels: array (N,) di cluster per ogni immagine.\n",
    "    top_clusters: lista dei c cluster (int) che vogliamo visualizzare.\n",
    "    thumb_size: dimensione (w,h) di ogni miniatura.\n",
    "    Per ogni cluster ∈ top_clusters stampa a video (o fa plt.show) \n",
    "    una griglia di miniature (fino a ~16-25 alla volta).\n",
    "    \"\"\"\n",
    "    for cluster_id in top_clusters:\n",
    "        # Indici di tutte le immagini che appartengono a questo cluster\n",
    "        idxs = [i for i, cl in enumerate(cluster_labels) if cl == cluster_id]\n",
    "        print(f\"[INFO] Cluster {cluster_id}: {len(idxs)} immagini trovate\")\n",
    "\n",
    "        # Se vogliamo limitare a N miniatura per cluster (tipo 16):\n",
    "        max_display = min(len(idxs), 16)\n",
    "        n = int(np.ceil(np.sqrt(max_display)))  # facciamo una griglia n×n\n",
    "        plt.figure(figsize=(n, n))\n",
    "\n",
    "        for j, i_img in enumerate(idxs[:max_display]):\n",
    "            img = Image.open(images[i_img]).convert('RGB')\n",
    "            img_thumb = img.resize(thumb_size, Image.LANCZOS)\n",
    "            \n",
    "            ax = plt.subplot(n, n, j+1)\n",
    "            plt.imshow(img_thumb)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Cluster {cluster_id} – {len(idxs)} immagini (mostrate: {max_display})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6c24e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "def db_scan_detection(eps, min_samples, n_components, c, method):\n",
    "\n",
    "    # Costruisce l’elenco dei full path per tutte le immagini\n",
    "    base_folder = \"Part1\"  # o path assoluto \"/Users/.../Parte1\"\n",
    "    images_full = [os.path.join(base_folder, lbl, fname) for fname, lbl in zip(filenames_pt1, labels_pt1)]\n",
    "\n",
    "    # Scorre ogni label di Parte1 ed applico DBSCAN+PCA\n",
    "    unique_labels = np.unique(labels_pt1)  # es. [\"Glioma\",\"Meningioma\",\"Pituitary\"]\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"[INFO] Elaboro label: {lbl}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        #Estrae le righe di feat_matrix_part1 / flname_part1 corrispondenti\n",
    "        mask_lbl = (labels_pt1 == lbl)\n",
    "        features_label = features_pt1[mask_lbl]   # shape = (n_i, d)\n",
    "        images_label = np.array(images_full)[mask_lbl]\n",
    "\n",
    "        #Chiama la tua funzione PCA + DBSCAN\n",
    "        cluster_labels = apply_dbscan_with_pca(\n",
    "            features_label,\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            n_components=n_components,\n",
    "            method = method\n",
    "        )\n",
    "        print(f\"[INFO] Cluster-labels trovati: {np.unique(cluster_labels)}\")\n",
    "\n",
    "        # Trova i c cluster più grandi\n",
    "        top_clusters = top_c_clusters(cluster_labels, c)\n",
    "        print(f\"[INFO] Top {c} cluster (per dimensione): {top_clusters}\")\n",
    "\n",
    "        # MDS‐2D + scatter plot del clustering\n",
    "        print(f\"[INFO] Disegno MDS 2D per i cluster di '{lbl}' …\")\n",
    "\n",
    "        plot_mds_clusters(\n",
    "            features_label,\n",
    "            cluster_labels,\n",
    "            top_clusters,\n",
    "            metric='cosine'\n",
    "        )\n",
    "\n",
    "        # Creo le miniature di ogni cluster “significativo”\n",
    "        print(f\"[INFO] Genero miniature per ciascun cluster di '{lbl}' …\")\n",
    "        show_cluster_thumbnails(\n",
    "            images_label,      # array di stringhe di percorsi\n",
    "            cluster_labels,    # array di int di lunghezza n_i\n",
    "            top_clusters,      # la lista dei c indici di cluster\n",
    "            thumb_size=(64, 64)\n",
    "        )\n",
    "\n",
    "    print(\"\\n[FINITO] Task 8 completato per tutte le label di Parte1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 5.0            # valore DBSCAN di esempio\n",
    "min_samples = 3      # valore DBSCAN di esempio\n",
    "n_components = 50    # quante dimensioni tenere con PCA PRIMA di DBSCAN\n",
    "c = 3                # quanti cluster “significativi” voglio prendere per ciascuna label\n",
    "method = 'umap'       # umap or pca\n",
    "db_scan_detection(eps, min_samples, n_components, c, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b42d64",
   "metadata": {},
   "source": [
    "# Task 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e947ed",
   "metadata": {},
   "source": [
    "Implementa un programma che, dati le immagini della parte 1:\n",
    "\n",
    "*crea un classificatore m-NN (per una m specificata dall'utente),\n",
    "\n",
    "*crea un classificatore ad albero decisionale,\n",
    "Per questo task, puoi utilizzare lo spazio delle feature di tua scelta.    \n",
    "\n",
    "Per le immagini della parte 2, prevede le etichette più probabili utilizzando il classificatore selezionato dall'utente.\n",
    "Il sistema dovrebbe anche fornire valori di precisione, richiamo e punteggio F1 per etichetta, nonché un valore di accuratezza complessiva.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta il valore di m per l'm-NN\n",
    "m = 5  # Modifica questo valore in base alle tue necessità\n",
    "\n",
    "# Addestramento m-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=m)\n",
    "knn_model.fit(features_pt1, labels_pt1)\n",
    "\n",
    "# Addestramento Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(features_pt1, labels_pt1)\n",
    "\n",
    "# Predizioni su Part2\n",
    "pred_knn = knn_model.predict(features_pt2)\n",
    "pred_dt = dt_model.predict(features_pt2)\n",
    "\n",
    "# Valutazione m-NN\n",
    "print(\"Risultati m-NN:\")\n",
    "print(classification_report(labels_pt2, pred_knn))\n",
    "print(\"Accuratezza complessiva m-NN:\", accuracy_score(labels_pt2, pred_knn))\n",
    "\n",
    "# Valutazione Decision Tree\n",
    "print(\"Risultati Decision Tree:\")\n",
    "print(classification_report(labels_pt2, pred_dt))\n",
    "print(\"Accuratezza complessiva Decision Tree:\", accuracy_score(labels_pt2, pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa7a11",
   "metadata": {},
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb3920",
   "metadata": {},
   "source": [
    "> 10a:\n",
    "\n",
    " Implementa uno strumento di Locality Sensitive Hashing (LSH) (per la distanza euclidea) che prende come input (a) il numero di livelli, L, (b) il numero di hash per livello, h, e (c) un insieme di vettori come input e crea una struttura di indice in memoria contenente l'insieme di vettori dato. \n",
    "\n",
    "Vedi:\n",
    "\"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\" (di Alexandr Andoni e Piotr Indyk). Communications of the ACM, vol. 51, no. 1, 2008, pp. 117-122.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b7da9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH_EuclideanQuantized:\n",
    "    \"\"\"\n",
    "    LSH per distanza Euclidea con quantizzazione (p-stable).\n",
    "    num_layers = L, num_hashes = h, dim = D, r = bucket width.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_hashes: int, dim: int, r: float):\n",
    "        self.L = num_layers\n",
    "        self.h = num_hashes\n",
    "        self.d = dim\n",
    "        self.r = r\n",
    "\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.L)]\n",
    "        self.a_vectors = [\n",
    "            [np.random.randn(self.d) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.b_offsets = [\n",
    "            [np.random.uniform(0, self.r) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.data_vectors = None\n",
    "\n",
    "    def _compute_hash_tuple(self, vec: np.ndarray, layer_idx: int) -> tuple:\n",
    "        bits = []\n",
    "        for j in range(self.h):\n",
    "            a_j = self.a_vectors[layer_idx][j]\n",
    "            b_j = self.b_offsets[layer_idx][j]\n",
    "            proj = float(np.dot(a_j, vec) + b_j)\n",
    "            h_val = int(np.floor(proj / self.r))\n",
    "            bits.append(h_val)\n",
    "        return tuple(bits)\n",
    "\n",
    "    def index(self, vectors: np.ndarray):\n",
    "        self.data_vectors = vectors\n",
    "        N, D = vectors.shape\n",
    "        assert D == self.d, f\"Dimensione vettore ({D}) ≠ D di LSH ({self.d}).\"\n",
    "        for idx in range(N):\n",
    "            v = vectors[idx]\n",
    "            for l in range(self.L):\n",
    "                key = self._compute_hash_tuple(v, l)\n",
    "                self.hash_tables[l][key].append(idx)\n",
    "\n",
    "    def query(self, q_vec: np.ndarray, top_t: int = 5):\n",
    "        assert q_vec.shape[0] == self.d, \"Dimensione query ≠ D.\"\n",
    "        candidati = set()\n",
    "        total_checked = 0\n",
    "        for l in range(self.L):\n",
    "            h_tuple = self._compute_hash_tuple(q_vec, l)\n",
    "            bucket = self.hash_tables[l].get(h_tuple, [])\n",
    "            total_checked += len(bucket)\n",
    "            candidati.update(bucket)\n",
    "\n",
    "        risultati = []\n",
    "        for idx in candidati:\n",
    "            v_i = self.data_vectors[idx]\n",
    "            dist = np.linalg.norm(v_i - q_vec)\n",
    "            risultati.append((idx, dist))\n",
    "        risultati.sort(key=lambda x: x[1])\n",
    "        top_results = risultati[:top_t]\n",
    "        return top_results, len(candidati), total_checked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50427c94",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beeb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costruzione LSH_EuclideanQuantized su Part1\n",
    "\n",
    "# 1) (Opzionale ma consigliato) centra e normalizza i vettori di Part1\n",
    "#    Questo passaggio riduce l'effetto di scale diverse e spesso migliora la qualità LSH\n",
    "mean_vec = np.mean(features_pt1, axis=0)\n",
    "feat_centered = features_pt1 - mean_vec\n",
    "feat_normed = normalize(feat_centered, norm='l2', axis=1)\n",
    "\n",
    "# 2) Parametri LSH\n",
    "D = feat_normed.shape[1]      # di solito 900\n",
    "L = 10                         # numero di tavole hash (scegli in base a esperimenti)\n",
    "h = 5                      # numero di funzioni concatenati in ciascuna tavola\n",
    "r = 1                      # parametro di larghezza (esempio: 0.5); puoi sperimentare\n",
    "\n",
    "# 3) Creo l'oggetto e indicizzo\n",
    "lsh_quant = LSH_EuclideanQuantized(num_layers=L, num_hashes=h, dim=D, r=r)\n",
    "lsh_quant.index(feat_normed)\n",
    "\n",
    "print(f\"[INFO] LSH quantizzato costruito: D={D}, L={L}, h={h}, r={r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e4a10",
   "metadata": {},
   "source": [
    "> 10b:\n",
    "\n",
    " Implementa un algoritmo di ricerca di immagini simili utilizzando questa struttura di indice che memorizza le immagini della parte 1 e un modello visivo di tua scelta (il modello visivo combinato deve avere almeno 256 dimensioni): per una data immagine di query e un numero intero t, \n",
    " \n",
    "*visualizza le t immagini più simili,\n",
    "*fornisce il numero di immagini univoche e il numero complessivo di immagini considerate durante il processo.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b9cf9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_lsh_quant(base_folder: str, img_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a img_path (di Part2) usando lsh_quant costruito su Part1.\n",
    "    \"\"\"\n",
    "    # 1) Estrai feature raw (900-dim)\n",
    "    print(img_path)\n",
    "    raw_q = np.array(extractor.extract_feature(img_path), dtype=np.float32)\n",
    "\n",
    "    # 2) Center + normalize (stesso mean_vec usato su Part1)\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # 3) Chiamata LSH\n",
    "    top_results, unique_count, total_checked = lsh_quant.query(q_normed, top_t=k)\n",
    "\n",
    "    # 4) Stampo i risultati testuali\n",
    "    print(f\"\\n[LSH-Quant] Top {k} simili a: {img_path}\")\n",
    "    for rank, (idx, dist) in enumerate(top_results, start=1):\n",
    "        label = labels_pt1[idx]\n",
    "        fname = filenames_pt1[idx]\n",
    "        print(f\"  {rank}. {fname} | Classe: {label} | Distanza Euclidea: {dist:.2f}\")\n",
    "    print(f\"[LSH-Quant] Immagini uniche considerate: {unique_count}\")\n",
    "    print(f\"[LSH-Quant] Immagini totali controllate: {total_checked}\")\n",
    "\n",
    "    # 5) Visualizzazione (query + k risultati)\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(4*(k+1), 4))\n",
    "    img_q = cv2.imread(img_path)\n",
    "    img_q = cv2.cvtColor(img_q, cv2.COLOR_BGR2RGB)\n",
    "    axs[0].imshow(img_q)\n",
    "    axs[0].set_title(\"Query (LSH-Quant)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, (idx, dist) in enumerate(top_results, start=1):\n",
    "        lab = labels_pt1[idx]\n",
    "        fname = filenames_pt1[idx]\n",
    "        full_path = os.path.join(base_folder, lab, fname)\n",
    "        img_match = cv2.imread(full_path)\n",
    "        img_match = cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img_match)\n",
    "        axs[i].set_title(f\"Rank {i}\\nd={dist:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68f6d0",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo su un'immagine di Part2 ---\n",
    "query_path = \"Part2/brain_menin/brain_menin_1003.jpg\"\n",
    "\n",
    "k = 5                         # numero di immagini simili da visualizzare\n",
    "\n",
    "# Eseguo la ricerca LSH\n",
    "find_k_similar_lsh_quant(\"Part1\", query_path, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
