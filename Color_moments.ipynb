{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0b65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "get_ipython().system('{sys.executable} -m pip install --quiet numpy pandas matplotlib seaborn scikit-learn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b8fc2",
   "metadata": {},
   "source": [
    "# Color Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36d2ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from scipy.stats import skew\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, pairwise_distances\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import MDS\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eaae8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58732e0",
   "metadata": {},
   "source": [
    "### Descrizione della funzione `crop_to_brain()`\n",
    "\n",
    "Questa funzione prevede i seguenti passaggi principali:\n",
    "\n",
    "1. **Conversione in scala di grigi:** semplifica l’immagine riducendola a un solo canale di intensità luminosa.\n",
    "2. **Applicazione di una soglia binaria:** vengono considerati significativi solo i pixel con intensità superiore a **10** (per eliminare il nero dello sfondo).\n",
    "3. **Calcolo del bounding box:** viene individuato il rettangolo più piccolo che contiene tutti i pixel significativi.\n",
    "4. **Ritaglio dell’immagine:** l’immagine viene ritagliata lungo i bordi del bounding box, lasciando solo l’area cerebrale.\n",
    "\n",
    "---\n",
    "\n",
    "### Obiettivo della pre-elaborazione\n",
    "\n",
    "L’obiettivo di questa fase è **standardizzare l’area di interesse** all’interno delle immagini e **rimuovere le parti non informative**.  \n",
    "Questo porta a una serie di vantaggi importanti:\n",
    "\n",
    "- **Migliora la qualità dei descrittori:** eliminando il rumore dei bordi non rilevanti.\n",
    "- **Standardizza posizione e dimensione:** le immagini risultano più simili dal punto di vista geometrico.\n",
    "- **Riduce la variabilità non utile:** rende i confronti tra immagini più affidabili.\n",
    "\n",
    "---\n",
    "\n",
    "## Output della funzione\n",
    "\n",
    "La funzione restituisce una **nuova immagine**, che può avere dimensioni variabili in base al contenuto ritagliato.\n",
    "\n",
    "Questa immagine verrà utilizzata successivamente nei task di estrazione dei descrittori visivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00999225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_brain(img):\n",
    "    \"\"\"Ritaglia l'area informativa (cervello) da un'immagine.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    coords = cv2.findNonZero(thresh)\n",
    "    if coords is not None:\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        return img[y:y+h, x:x+w]\n",
    "    return img  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f3fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    return data['features'], data['labels'], data.get('filenames', [f\"img_{i}\" for i in range(len(data['features']))])\n",
    "\n",
    "\n",
    "feat_matrix_part1, lbls_part1, flname_part1 = load_features(\n",
    "    \"color_moments_part1.npz\")\n",
    "feat_matrix_part2, lbls_part2, flname_part2 = load_features(\n",
    "    \"color_moments_part2.npz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adff1e",
   "metadata": {},
   "source": [
    "# Task 1-2\n",
    "\n",
    "## Implementa un programma che **estrae e memorizza i descrittori di feature** per tutte le immagini nel set di dati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b7914",
   "metadata": {},
   "source": [
    "## **Estrazione dei Color Moments**\n",
    "\n",
    "Dopo aver ritagliato la porzione utile dell’immagine, vogliamo rappresentare numericamente le sue caratteristiche visive tramite **Color Moments**.\n",
    "\n",
    "### **Cos'è un Color Moment?**\n",
    "\n",
    "Per ogni canale colore (R, G, B) calcoliamo 3 statistiche **per ogni regione dell’immagine**:\n",
    "- **Media (Mean):** indica il colore medio.\n",
    "- **Deviazione standard (Std):** misura la variabilità dei colori (contrasto).\n",
    "- **Skewness (Assimmetria):** misura lo sbilanciamento della distribuzione dei colori verso valori più chiari o più scuri.\n",
    "\n",
    "Queste tre statistiche catturano l’aspetto cromatico di una porzione dell’immagine.\n",
    "\n",
    "---\n",
    "\n",
    "### **Suddivisione dell'immagine in una griglia 10×10**\n",
    "\n",
    "Per non perdere le informazioni spaziali, suddividiamo l’immagine in una **griglia 10×10**:\n",
    "- Ogni cella della griglia corrisponde a una porzione locale dell’immagine.\n",
    "- I Color Moments vengono calcolati separatamente per ciascuna cella.\n",
    "\n",
    "### **Output finale**\n",
    "\n",
    "Per ogni cella vengono calcolati:\n",
    "- 3 valori per R (mean, std, skewness)\n",
    "- 3 valori per G\n",
    "- 3 valori per B\n",
    "\n",
    "Totale: **3 statistiche × 3 canali × 100 celle = 900 feature per immagine**\n",
    "\n",
    "Queste 900 feature rappresentano **numericamente l’intera immagine**, mantenendo sia l’informazione cromatica che quella spaziale.\n",
    "\n",
    "---\n",
    "\n",
    "## **Vantaggi dei Color Moments**\n",
    "\n",
    "- **Calcolo semplice:** molto più rapido rispetto a descrittori complessi come SIFT o SURF.\n",
    "- **Informazioni significative:** nei contesti medici, i colori possono evidenziare strutture anomale.\n",
    "- **Rappresentazione locale:** grazie alla suddivisione in celle, preserviamo anche informazioni spaziali.\n",
    "- **Efficienza computazionale:** perfetto per dataset medi o grandi.\n",
    "\n",
    "---\n",
    "\n",
    "## **Considerazioni finali per Task 1-2**\n",
    "\n",
    "Dopo questa fase abbiamo per ogni immagine:\n",
    "- Un’area cerebrale ritagliata e priva di rumore di background.\n",
    "- Un vettore numerico di **900 feature**, pronto per il confronto tra immagini e per l’uso in algoritmi di Machine Learning.\n",
    "\n",
    "Il passo successivo sarà salvare questi descrittori in formato efficiente (`.npz`) e utilizzarli nei task successivi per la ricerca e la classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8196bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_moments(img_path):\n",
    "    \"\"\"Estrae Color Moments su una griglia 10x10 da un'immagine.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERRORE] Immagine non trovata: {img_path}\")\n",
    "        return None\n",
    "\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    img = crop_to_brain(img)\n",
    "    img = cv2.resize(img, (300, 100))  # Griglia uniforme\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    grid_h, grid_w = h // 10, w // 10\n",
    "    features = []\n",
    "\n",
    "    # Scorri ogni cella della griglia (10 righe e 10 colonne)\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            # Estrae la porzione di immagine corrispondente alla cella (slice dell'immagine)\n",
    "            cell = img[i*grid_h:(i+1)*grid_h, j*grid_w:(j+1)*grid_w]\n",
    "\n",
    "            # Per ogni canale di colore (Blue, Green, Red)\n",
    "            for channel in range(3):\n",
    "                # Estrae i pixel della cella e del canale specifico, appiattendo il risultato in un vettore 1D\n",
    "                pixels = cell[:, :, channel].flatten()\n",
    "\n",
    "                # Se la cella ha variazione cromatica (deviazione standard > 0), calcola i Color Moments\n",
    "                if np.std(pixels) > 0:\n",
    "                    mean = np.mean(pixels)   # Media dei pixel → rappresenta il colore medio\n",
    "                    std = np.std(pixels)    # Deviazione standard → rappresenta la variazione di colore\n",
    "                    sk = skew(pixels)       # Skewness (asimmetria) → indica se i pixel sono più scuri o chiari\n",
    "\n",
    "                    # In alcuni casi il calcolo della skewness può produrre NaN (non numerico); lo sostituiamo con 0\n",
    "                    if np.isnan(sk):\n",
    "                        sk = 0\n",
    "                else:\n",
    "                    # Se i pixel sono tutti uguali, mettiamo 0 per tutti i descrittori per evitare divisioni per zero\n",
    "                    mean, std, sk = 0, 0, 0\n",
    "\n",
    "                # Aggiungiamo i tre valori calcolati (mean, std, skew) alla lista delle feature\n",
    "                features.extend([mean, std, sk])\n",
    "\n",
    "    # Ritorna il vettore delle feature Color Moments estratto da tutta l’immagine\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8957034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_features(base_folder, subfolders, output_file):\n",
    "    \"\"\"Estrae le feature da immagini organizzate in sottocartelle e le salva in un file .npz.\"\"\"\n",
    "    all_features, all_filenames, all_labels = [], [], []\n",
    "\n",
    "    for label in subfolders:\n",
    "        folder_path = os.path.join(base_folder, label)\n",
    "        print(f\"[INFO] Elaboro cartella: {label}\")\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                features = extract_color_moments(img_path)\n",
    "                if features is not None:\n",
    "                    all_features.append(features)\n",
    "                    all_filenames.append(filename)\n",
    "                    all_labels.append(label)\n",
    "\n",
    "    np.savez(output_file,\n",
    "             features=np.array(all_features),\n",
    "             filenames=np.array(all_filenames),\n",
    "             labels=np.array(all_labels))\n",
    "    print(f\"[SALVATO] Features salvate in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8dc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri\n",
    "subfolders = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]\n",
    "\n",
    "# Estrazione e salvataggio\n",
    "process_and_save_features(\"Part1\", subfolders, \"color_moments_part1.npz\")\n",
    "process_and_save_features(\"Part2\", subfolders, \"color_moments_part2.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78dfcc",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implementa un programma che, **dato il **nome di un file immagine** e un valore \"k\"**, **restituisce e visualizza le k immagini più simili** in base a ciascun modello visivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6127405",
   "metadata": {},
   "source": [
    "# 🔍 Task 3 – Retrieval di Immagini Mediche Basato sui Color Moments\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "In questo task implementiamo un **sistema di retrieval**, ossia un metodo per **trovare le immagini più simili** a una data immagine di query, basandoci sulle **feature estratte (Color Moments)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Funzione principale: `find_k_similar_unified()`**\n",
    "\n",
    "Questa funzione automatizza i seguenti passaggi:\n",
    "\n",
    "\n",
    "**Estrazione delle feature della query**\n",
    "\n",
    "**Calcolo della distanza tra feature**\n",
    "\n",
    "Per determinare la similarità tra immagini possiamo scegliere **due diverse metriche di distanza**:\n",
    "\n",
    "- `Distanza Euclidea`\n",
    "- `Distanza di Mahalanobis`\n",
    "\n",
    "**Esclusione della query stessa dai risultati**\n",
    "\n",
    "**Ordinamento dei risultati**\n",
    "\n",
    "**Visualizzazione dei risultati**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94c399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_unified(base_folder, img_path, k, distance_type=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a una query utilizzando distanza euclidea o Mahalanobis.\n",
    "    \n",
    "    Args:\n",
    "        base_folder (str): Cartella base in cui cercare le immagini.\n",
    "        img_path (str): Percorso immagine query.\n",
    "        k (int): Numero di immagini da restituire.\n",
    "        distance_type (str): Tipo di distanza ('euclidean' o 'mahalanobis').\n",
    "    \"\"\"\n",
    "    # Estrai le feature dalla query\n",
    "    query_feature = extract_color_moments(img_path)\n",
    "    if query_feature is None:\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    if distance_type == \"euclidean\":\n",
    "        distances = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "\n",
    "    elif distance_type == \"mahalanobis\":\n",
    "        # Calcola matrice di covarianza e la sua inversa\n",
    "        cov = np.cov(feat_matrix_part1.T)\n",
    "        try:\n",
    "            cov_inv = np.linalg.inv(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"[ERRORE] Uso pseudoinversa per matrice non invertibile.\")\n",
    "            cov_inv = np.linalg.pinv(cov)\n",
    "\n",
    "        query_feature = query_feature.flatten()\n",
    "        distances = np.array([mahalanobis(query_feature, f, cov_inv) for f in feat_matrix_part1])\n",
    "\n",
    "        # Escludi la query stessa se già presente\n",
    "        query_filename = os.path.basename(img_path)\n",
    "        query_label = os.path.basename(os.path.dirname(img_path))\n",
    "        for i in range(len(flname_part1)):\n",
    "            if flname_part1[i] == query_filename and lbls_part1[i] == query_label:\n",
    "                distances[i] = np.inf\n",
    "                break\n",
    "    else:\n",
    "        print(f\"[ERRORE] Tipo di distanza non supportato: {distance_type}\")\n",
    "        return\n",
    "\n",
    "    # Trova i top-k\n",
    "    top_k_idx = np.argsort(distances)[:k]\n",
    "\n",
    "    # Output testuale\n",
    "    print(f\"\\nTop {k} immagini simili a: {img_path} (Distanza: {distance_type})\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        print(f\"{rank + 1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Distanza: {distances[idx]:.2f}\")\n",
    "\n",
    "    # Visualizzazione immagini\n",
    "    fig, axs = plt.subplots(1, k + 1, figsize=(15, 5))\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, idx in enumerate(top_k_idx):\n",
    "        img_match_path = os.path.join(base_folder, lbls_part1[idx], flname_part1[idx])\n",
    "        img_match = cv2.imread(img_match_path)\n",
    "        axs[i + 1].imshow(cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB))\n",
    "        axs[i + 1].set_title(f\"Rank {i + 1}\\nD={distances[idx]:.2f}\")\n",
    "        axs[i + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373057c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Part1\"\n",
    "img_path = \"Part1/brain_glioma/brain_glioma_0005.jpg\"\n",
    "\n",
    "find_k_similar_unified(base_folder, img_path, k=5, distance_type=\"euclidean\")\n",
    "find_k_similar_unified(base_folder, img_path, k=5, distance_type=\"mahalanobis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d74cc",
   "metadata": {},
   "source": [
    "# Task 4 – Confronto tra Strategie di Classificazione Basate su Distanze\n",
    "\n",
    " Implementa un programma che, dati:\n",
    "- (a) un'immagine di query della **Parte 2**,\n",
    "- (b) uno **spazio di feature** selezionato dall'utente,\n",
    "- (c) un numero intero positivo **k**,\n",
    "\n",
    "identifichi ed elenchi le **k etichette di corrispondenza più probabili**, insieme ai loro punteggi, calcolati nello **spazio di feature selezionato**.\n",
    "\n",
    "### Definizioni\n",
    "\n",
    "**Feature space:** spazio vettoriale in cui ogni immagine è rappresentata come un **vettore di caratteristiche** (feature vector).\n",
    "\n",
    "**Selected feature space:** indica **il tipo di caratteristiche** estratte per rappresentare le immagini (es. color moments, HOG, deep features, ecc.).\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "In questo task implementiamo un confronto tra **due diverse strategie di classificazione**, entrambe basate sulle distanze tra la query e le immagini del dataset.  \n",
    "\n",
    "L’obiettivo è capire a quale **classe** appartiene una immagine query analizzando le sue **feature (Color Moments)** e confrontandole con quelle delle classi note.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodo\n",
    "\n",
    "### 1️: Estrazione delle feature della query\n",
    "\n",
    "### 2: **Strategia 1: Distanza media per classe**\n",
    "\n",
    "#### Descrizione\n",
    "- Viene calcolata la **distanza Euclidea** tra la query e tutte le immagini del dataset.\n",
    "- Per ogni **classe**, viene calcolata la **media delle distanze** tra la query e tutte le immagini appartenenti a quella classe.\n",
    "\n",
    "##### Vantaggi:\n",
    "- Tiene conto di **tutte le immagini** della classe.\n",
    "- Robusta nei confronti di rumore casuale nelle singole immagini.\n",
    "\n",
    "##### Svantaggi:\n",
    "- **Sensibile agli outlier**: immagini anomale possono aumentare la distanza media della classe.\n",
    "\n",
    "### 3: **Strategia 2: Distanza dal prototipo della classe**\n",
    "\n",
    "#### Descrizione\n",
    "- Per ogni classe viene calcolato il **prototipo**, ovvero il **centroide** (media vettoriale) delle feature di tutte le immagini della classe.\n",
    "- Si calcola la distanza Euclidea tra la query e il **centroide** di ogni classe.\n",
    "\n",
    "##### Vantaggi:\n",
    "- **Robusto agli outlier**, perché ogni classe è rappresentata da un solo vettore medio.\n",
    "- Più veloce da calcolare.\n",
    "\n",
    "##### Svantaggi:\n",
    "- Potrebbe non rappresentare bene le **sottostrutture interne** della classe.\n",
    "- Se la query è simile solo ad un sottogruppo della classe, ma distante dal centroide, questa strategia può dare un risultato meno preciso.\n",
    "\n",
    "### 4️: Visualizzazione dei risultati\n",
    "\n",
    "### 5️: Analisi automatica del miglior risultato\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Confronto tra le due strategie\n",
    "\n",
    "| Strategia               | Punti di forza                                  | Limiti                                    |\n",
    "|-------------------------|--------------------------------------------------|------------------------------------------|\n",
    "| Media per classe        | Considera tutte le immagini                      | Sensibile agli outlier                   |\n",
    "| Prototipo (centroide)   | Robusto agli outlier, semplice da calcolare      | Può non catturare sottogruppi specifici  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38963d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ranking_strategies(query_img_path, k=2):\n",
    "    \"\"\"\n",
    "    Confronta le etichette predette da due strategie:\n",
    "    - distanza media per classe\n",
    "    - distanza al rappresentante (prototipo) della classe\n",
    "    Visualizza solo le top-k etichette con un grafico comparativo.\n",
    "    Inoltre stampa quale strategia sembra migliore in base alla distanza top-1.\n",
    "    \"\"\"\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "\n",
    "    query_feature = extract_color_moments(query_img_path)\n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    # ===== Strategia 1: distanza media per classe =====\n",
    "    distances_all = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "    df_all = pd.DataFrame({\n",
    "        'label': lbls_part1,\n",
    "        'distance': distances_all\n",
    "    })\n",
    "    mean_dists = df_all.groupby('label')['distance'].mean().sort_values()\n",
    "\n",
    "    # ===== Strategia 2: distanza dal prototipo (centroide) =====\n",
    "    df_features = pd.DataFrame(feat_matrix_part1)\n",
    "    df_features['label'] = lbls_part1\n",
    "    class_prototypes = df_features.groupby('label').mean().drop(columns=['label'], errors='ignore')\n",
    "    proto_vectors = class_prototypes.values\n",
    "    proto_labels = class_prototypes.index\n",
    "    proto_dists = euclidean_distances(proto_vectors, query_feature).flatten()\n",
    "    proto_dists_series = pd.Series(proto_dists, index=proto_labels).sort_values()\n",
    "\n",
    "    # ===== Prendi le top-k etichette comuni =====\n",
    "    top_k_mean = mean_dists.head(k)\n",
    "    top_k_proto = proto_dists_series.head(k)\n",
    "\n",
    "    union_labels = sorted(set(top_k_mean.index).union(set(top_k_proto.index)))\n",
    "\n",
    "    # ===== Plot solo per le top-k etichette =====\n",
    "    x = np.arange(len(union_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.bar(x - width/2, [mean_dists[label] for label in union_labels], width, label='Distanza Media')\n",
    "    ax.bar(x + width/2, [proto_dists_series[label] for label in union_labels], width, label='Distanza Prototipo')\n",
    "\n",
    "    ax.set_ylabel('Distanza')\n",
    "    ax.set_title(f\"Top-{k} Strategie - Query: {os.path.basename(query_img_path)}\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(union_labels)\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ===== Stampa i top-k risultati =====\n",
    "    print(\"\\n Top-k etichette per ciascuna strategia:\\n\")\n",
    "    print(\"Strategia: Distanza Media\")\n",
    "    print(top_k_mean)\n",
    "\n",
    "    print(\"\\n Strategia: Prototipo di Classe\")\n",
    "    print(top_k_proto)\n",
    "\n",
    "    # ===== Confronto top-1 =====\n",
    "    top1_mean_label = top_k_mean.index[0]\n",
    "    top1_mean_value = top_k_mean.iloc[0]\n",
    "    top1_proto_label = top_k_proto.index[0]\n",
    "    top1_proto_value = top_k_proto.iloc[0]\n",
    "\n",
    "    print(\"\\n==== Analisi della Strategia Migliore ====\")\n",
    "    if top1_mean_label == top1_proto_label:\n",
    "        print(f\"[OK] Entrambe le strategie concordano sulla classe '{top1_mean_label}'.\")\n",
    "        print(f\" → Distanza media: {top1_mean_value:.4f}, distanza prototipo: {top1_proto_value:.4f}\")\n",
    "    else:\n",
    "        print(f\"[DIFFERENZA] Le strategie danno risultati diversi:\")\n",
    "        print(f\" - Distanza Media: '{top1_mean_label}' con distanza {top1_mean_value:.4f}\")\n",
    "        print(f\" - Prototipo: '{top1_proto_label}' con distanza {top1_proto_value:.4f}\")\n",
    "        if top1_mean_value < top1_proto_value:\n",
    "            print(f\" → [SCELTA SUGGERITA] Preferibile 'Distanza Media' ({top1_mean_label})\")\n",
    "        else:\n",
    "            print(f\" → [SCELTA SUGGERITA] Preferibile 'Prototipo' ({top1_proto_label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_glioma/brain_glioma_1142.jpg\"\n",
    "compare_label_ranking_strategies(query_img, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32f764",
   "metadata": {},
   "source": [
    "# Task 5: Estrazione delle Semantiche Latenti\n",
    "\n",
    "Implementa un programma che, dati:\n",
    "- (a) uno dei modelli di feature disponibili,\n",
    "- (b) un valore **k** specificato dall'utente,\n",
    "- (c) una delle tre tecniche di riduzione della dimensionalità (**SVD**, **LDA**, **k-means**) scelta dall'utente,\n",
    "\n",
    "estragga le **prime k semantiche latenti** dallo spazio delle feature selezionato.\n",
    "\n",
    "### Funzionalità richieste\n",
    "\n",
    "- **Memorizzare** le semantiche latenti estratte in un file di output, **adeguatamente nominato**.\n",
    "- **Elencare** per ogni componente latente le **coppie (imageID, peso)** ordinate in ordine decrescente di peso.\n",
    "\n",
    "\n",
    "### Definizione del feature model\n",
    "\n",
    "Il **feature model** è un file contenente la rappresentazione delle feature di tutte le immagini del dataset, salvato come matrice (`feature_matrix`).  \n",
    "La matrice ha dimensioni **n × d**, dove:\n",
    "- **n** = numero di immagini,\n",
    "- **d** = numero di feature.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodo\n",
    "1. **Riduzione dimensionale** del feature space selezionato tramite la tecnica scelta.\n",
    "2. **Estrazione delle top-k componenti latenti**.\n",
    "3. Per ogni componente:\n",
    "   - Calcolare i **pesi** associati a ogni immagine.\n",
    "   - Ordinare le coppie **(imageID, peso)** in ordine decrescente di peso.\n",
    "4. **Salvare i risultati** in un file di testo con nome descrittivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe0ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_latent_space_2d(X_transformed, labels, technique, k):\n",
    "    \"\"\"Visualizza la proiezione 2D delle immagini nello spazio latente (solo per SVD/LDA).\"\"\"\n",
    "    print(f\"[DEBUG] Shape X_svd (dati trasformati): {X_transformed.shape}\")\n",
    "    if X_transformed.shape[1] < 2:\n",
    "        print(\"[INFO] Meno di 2 componenti: impossibile visualizzare in 2D.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=labels, palette=\"Set2\", s=80)\n",
    "    plt.title(f\"{technique.upper()} - Proiezione sulle prime 2 componenti latenti (k={k})\")\n",
    "    plt.xlabel(\"Componente 1\")\n",
    "    plt.ylabel(\"Componente 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_kmeans_clusters_2d(feature_matrix, labels, n_clusters):\n",
    "    \"\"\"Visualizza le immagini raggruppate da KMeans su uno spazio 2D ridotto con SVD.\"\"\"\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd.fit_transform(feature_matrix)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_labels, palette='tab10', s=80, style=labels)\n",
    "    plt.title(f\"KMeans Clustering (k={n_clusters}) con proiezione SVD 2D\")\n",
    "    plt.xlabel(\"Componente Latente 1 (da SVD)\")\n",
    "    plt.ylabel(\"Componente Latente 2 (da SVD)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "def task5_latent_semantics(feature_model_path, technique, k):\n",
    "    \"\"\"\n",
    "    Estrae i top-k concetti latenti da uno spazio di feature usando SVD, LDA o KMeans.\n",
    "    Visualizza lo spazio latente ed esporta un file .txt con i pesi associati alle immagini.\n",
    "    \"\"\"\n",
    "\n",
    "    technique = technique.lower()\n",
    "    method = \"\"\n",
    "    X_transformed = None\n",
    "    components = None\n",
    "\n",
    "    if technique == \"svd\":\n",
    "        model = TruncatedSVD(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1)\n",
    "        components = model.components_\n",
    "        method = \"svd\"\n",
    "\n",
    "    elif technique == \"lda\":\n",
    "        unique_labels = np.unique(lbls_part1)\n",
    "        max_k = len(unique_labels) - 1\n",
    "        if k > max_k:\n",
    "            print(f\"[ATTENZIONE] LDA supporta al massimo {max_k} componenti con {len(unique_labels)} classi.\")\n",
    "            k = max_k\n",
    "        model = LDA(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1, lbls_part1)\n",
    "        components = model.scalings_.T[:k]\n",
    "        method = \"lda\"\n",
    "\n",
    "    elif technique == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        model.fit(feat_matrix_part1)\n",
    "        components = model.cluster_centers_\n",
    "        X_transformed = model.transform(feat_matrix_part1)\n",
    "        method = \"kmeans\"\n",
    "    else:\n",
    "        print(\"[ERRORE] Tecnica non supportata. Usa: 'svd', 'lda', 'kmeans'\")\n",
    "        return\n",
    "\n",
    "    # Visualizzazione\n",
    "    if technique in [\"svd\", \"lda\"]:\n",
    "        plot_latent_space_2d(X_transformed, lbls_part1, technique, k)\n",
    "    elif technique == \"kmeans\":\n",
    "        plot_kmeans_clusters_2d(feat_matrix_part1, lbls_part1, k)\n",
    "\n",
    "# Creazione output\n",
    "    output_dir = os.path.join(\"task5_output\", \"latent_semantics_color_moments\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = os.path.join(output_dir, f\"latent_semantics_{method}_{base_name}_k{k}.txt\")\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for i in range(k):\n",
    "            f.write(f\"\\n--- Latent Semantic {i+1} ---\\n\")\n",
    "            if technique in [\"svd\", \"lda\"]:\n",
    "                weights = feat_matrix_part1 @ components[i].T\n",
    "                sorted_idx = np.argsort(-np.abs(weights))\n",
    "            else:  # KMeans: distanza dal centroide, più piccola = più vicino\n",
    "                weights = -X_transformed[:, i]\n",
    "                sorted_idx = np.argsort(weights)\n",
    "\n",
    "            for idx in sorted_idx:\n",
    "                image_id = flname_part1[idx]\n",
    "                f.write(f\"{image_id} | Peso: {weights[idx]:.4f} | Classe: {lbls_part1[idx]}\\n\")\n",
    "\n",
    "    print(f\"[SALVATO] Latent semantics salvati in: {out_file}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=min(feat_matrix_part1.shape)\n",
    "\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"svd\", k=2)\n",
    "#task5_latent_semantics(\"color_moments.npz\", technique=\"svd\", k=n_components)\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"lda\", k=2)\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"kmeans\", k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb836e",
   "metadata": {},
   "source": [
    "# Task 6 – Calcolo della Dimensionalità Intrinseca\n",
    "\n",
    "## Obiettivo del Task\n",
    "\n",
    "In questo task analizziamo **quanto è realmente complesso lo spazio delle feature**.  \n",
    "L'obiettivo è capire **quante dimensioni (componenti)** servono per rappresentare in modo efficace le informazioni contenute nei Color Moments.\n",
    "\n",
    "## Cos'è la dimensionalità intrinseca?\n",
    "\n",
    "Lo spazio dei Color Moments ha **900 dimensioni**, ma non tutte queste dimensioni contribuiscono allo stesso modo.  \n",
    "Molte possono contenere **rumore o informazioni ridondanti**.\n",
    "\n",
    "La **dimensionalità intrinseca** rappresenta il **numero minimo di componenti** necessarie per spiegare una certa percentuale della varianza dei dati (es. il 95%).\n",
    "\n",
    "\n",
    "##  Tecnica utilizzata: PCA (Principal Component Analysis)\n",
    "\n",
    "La PCA riduce lo spazio delle feature individuando le direzioni principali lungo cui i dati variano maggiormente.\n",
    "\n",
    "- Ordina le componenti in base alla **varianza spiegata**.\n",
    "- Permette di stimare quante componenti servono per spiegare una certa percentuale della varianza totale (**90%**, **95%**, **99%**).\n",
    "\n",
    "---\n",
    "### explained = pca.explained_variance_ratio_\n",
    "es. explained = [0.40, 0.30, 0.20, 0.07, 0.03]\n",
    "\n",
    "Un **array di float** dove ciascun **valore rappresenta la percentuale di varianza spiegata da ciascuna componente principale della PCA**.\n",
    "È ordinato dalla componente più importante a quella meno importante.\n",
    "\n",
    "Significa che **la prima componente spiega il 40% della varianza**, la seconda il 30%, ecc.\n",
    "\n",
    "### cumulative = np.cumsum(explained)\n",
    "es:cumulative = [0.40, 0.70, 0.90, 0.97, 1.00]\n",
    "\n",
    "Calcola la somma cumulativa dell’array explained, cioè la **varianza totale spiegata fino a ciascuna componente**.\n",
    "\n",
    "Significa:\n",
    "- Le prime 2 componenti spiegano il 70%\n",
    "- Le prime 3 il 90%\n",
    "- Le prime 4 il 97%, ecc.\n",
    "\n",
    "### intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "Questa riga:\n",
    "\t- Trova il primo indice in cui la varianza cumulativa raggiunge o supera una soglia (threshold), ad esempio 0.95 (95%).\n",
    "\t- np.argmax(...) restituisce il primo True nella condizione cumulative >= threshold.\n",
    "\t- Si aggiunge +1 perché gli indici Python partono da 0, ma il numero di componenti parte da 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "674d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_intrinsic_dimensionality(feature_matrix, threshold, plot=True):\n",
    "    max_components = min(feature_matrix.shape)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(feature_matrix)\n",
    "\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "    intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "    #print(f\"[INFO] Spiegazione varianza per ogni componente PCA:\\n{explained}\")\n",
    "    #print(f\"[INFO] Varianza cumulativa:\\n{cumulative}\")\n",
    "    #print(f\"[INFO] Soglia impostata: {threshold}\")\n",
    "    #print(f\"[INFO] Dimensione intrinseca stimata: {intrinsic_dim}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cumulative, marker='o', label=\"Varianza cumulativa\")\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Soglia {threshold*100:.0f}%\")\n",
    "        plt.axvline(x=intrinsic_dim, color='g', linestyle='--', label=f\"k suggerito: {intrinsic_dim}\")\n",
    "        plt.xlabel(\"Numero componenti\")\n",
    "        plt.ylabel(\"Varianza cumulativa\")\n",
    "        plt.title(\"Scelta ottimale di k (PCA/SVD)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"[INFO] k ottimale suggerito (soglia {threshold*100:.0f}%): {intrinsic_dim}\")\n",
    "    return intrinsic_dim, cumulative\n",
    "\n",
    "def suggest_k(feature_matrix, threshold_list=[0.90, 0.95, 0.99]):\n",
    "    print(f\"[INFO] Feature matrix shape: {feature_matrix.shape}\")\n",
    "    k_values = {}\n",
    "    for t in threshold_list:\n",
    "        k, _ = estimate_intrinsic_dimensionality(feature_matrix, threshold=t, plot=False)\n",
    "        k_values[t] = k\n",
    "        print(f\"Soglia {int(t*100)}% : k = {k}\")\n",
    "    return k_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7fb149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality_per_label(feature_matrix, labels, threshold):\n",
    "    label_dim_map = {}\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"[INFO] Etichette uniche trovate: {len(unique_labels)}\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        label_features = feature_matrix[indices]\n",
    "\n",
    "        if len(indices) < 2:\n",
    "            print(f\"[AVVISO] Label '{label}' ha meno di 2 campioni — ignorata.\")\n",
    "            continue\n",
    "\n",
    "        k, _ = estimate_intrinsic_dimensionality(label_features, threshold=threshold, plot=False)\n",
    "        label_dim_map[label] = k\n",
    "        print(f\" Label '{label}' : k = {k}\")\n",
    "\n",
    "    return label_dim_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f13f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola k per varie soglie\n",
    "print(\"\\Stima automatica di k in base alla varianza spiegata:\\n\")\n",
    "k_suggeriti = suggest_k(feat_matrix_part1)\n",
    "# Plot dettagliato per la soglia 95%\n",
    "estimate_intrinsic_dimensionality(feat_matrix_part1, threshold=0.95, plot=True)\n",
    "\n",
    "\n",
    "print(\"\\n Task 6b – Dimensionalità per etichetta:\\n\")\n",
    "label_dimensionalities = estimate_dimensionality_per_label(feat_matrix_part1, lbls_part1, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d77d5b",
   "metadata": {},
   "source": [
    "# Task 7 – Classificazione Basata su Spazi Latenti Separati per Classe\n",
    "\n",
    "Implementare un programma che:\n",
    "1. Per ogni etichetta unica **l**, calcoli le corrispondenti **k componenti semantiche latenti** (a scelta) associate alle immagini della **Parte 1**.\n",
    "2. Per le immagini della **Parte 2**, preveda le etichette più probabili utilizzando le **distanze/similarità calcolate nello spazio semantico latente specifico per ciascuna etichetta**.\n",
    "\n",
    "Il sistema deve inoltre calcolare e riportare:\n",
    "- **Precisione (Precision)** per ciascuna classe,\n",
    "- **Richiamo (Recall)** per ciascuna classe,\n",
    "- **Punteggio F1 (F1-score)** per ciascuna classe,\n",
    "- **Accuratezza complessiva (Accuracy)**.\n",
    "## Obiettivo del Task\n",
    "\n",
    "In questo task sviluppiamo un sistema di **classificazione** che:\n",
    "- Crea **uno spazio semantico separato per ciascuna classe**, riducendo il rischio che le caratteristiche di una classe influenzino quelle delle altre.\n",
    "- Classifica una query confrontando la sua posizione negli spazi latenti delle singole classi.\n",
    "\n",
    "L’idea di base è che ogni classe abbia **proprietà statistiche distinte**, che emergono più chiaramente se considerate separatamente.\n",
    "\n",
    "---\n",
    "\n",
    "## Concetto chiave\n",
    "\n",
    "Ogni classe ha un **proprio spazio semantico latente**, in cui le immagini della classe sono rappresentate in modo compatto.  \n",
    "Quando un'immagine deve essere classificata:\n",
    "- viene proiettata in ogni spazio latente di classe,\n",
    "- viene calcolata la distanza rispetto al **centroide latente** di ciascuna classe,\n",
    "- l'immagine viene assegnata alla classe il cui centroide è il più vicino.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodo proposto\n",
    "\n",
    "### 1 Estrazione delle semantiche latenti per classe\n",
    "\n",
    "Per ogni classe, si eseguono due trasformazioni:\n",
    "\n",
    "#### a. **Standardizzazione dei dati**\n",
    "- Ogni classe viene normalizzata separatamente (**StandardScaler**), per evitare che feature con scale diverse falsino l’analisi.\n",
    "\n",
    "#### b. **Riduzione dimensionale con Truncated SVD**\n",
    "- Si applica una riduzione dimensionale per estrarre i **componenti latenti più significativi** della classe.\n",
    "- Ogni immagine viene così rappresentata da un vettore ridotto (**es. 10 dimensioni**, contro le 900 iniziali).\n",
    "\n",
    "### 2️ Calcolo del centroide latente per ogni classe\n",
    "\n",
    "Dopo la riduzione dimensionale, per ogni classe si calcola il **centroide**, cioè il vettore medio nello spazio latente.  \n",
    "Questo centroide rappresenta il \"cuore\" statistico della classe.\n",
    "\n",
    "## Fase di classificazione\n",
    "\n",
    "Quando una nuova immagine query deve essere classificata:\n",
    "\n",
    "1. Viene **normalizzata** usando lo scaler della classe corrente.\n",
    "2. Viene **proiettata** nello spazio latente della classe usando il modello SVD della classe.\n",
    "3. Si calcola la **distanza euclidea** tra la query e il centroide della classe.\n",
    "\n",
    "L’immagine viene assegnata alla **classe con il centroide più vicino**.\n",
    "\n",
    "##  Valutazione dei risultati\n",
    "\n",
    "Abbiamo valutato le prestazioni del sistema calcolando:\n",
    "- Precisione (**Precision**)\n",
    "- Richiamo (**Recall**)\n",
    "- F1-score\n",
    "- Accuratezza complessiva (**Accuracy**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddfdf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_semantics_per_class(X, y, k=10):\n",
    "    class_models = {}\n",
    "    class_means = {}\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        X_class = X[y == label]  # Prende solo le istanze della classe corrente\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_class)  # Normalizza i dati della classe\n",
    "\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        latent = svd.fit_transform(X_scaled)  # Riduzione dimensionale con SVD\n",
    "\n",
    "        # Salva modello SVD e scaler per la classe\n",
    "        class_models[label] = {\n",
    "            'svd': svd,\n",
    "            'scaler': scaler,\n",
    "            'latent_vectors': latent\n",
    "        }\n",
    "        # Calcola la media dei vettori latenti della classe\n",
    "        class_means[label] = np.mean(latent, axis=0)\n",
    "    return class_models, class_means\n",
    "\n",
    "def predict_label(X_test, class_models, class_means):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        best_label = None\n",
    "        min_dist = float('inf')\n",
    "        for label, model in class_models.items():\n",
    "            x_scaled = model['scaler'].transform(x.reshape(1, -1))  # Normalizza x\n",
    "            x_latent = model['svd'].transform(x_scaled)  # Trasforma in spazio latente\n",
    "            dist = np.linalg.norm(x_latent - class_means[label])  # Distanza dal centroide\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_label = label\n",
    "        y_pred.append(best_label)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    labels = np.unique(y_true)\n",
    "    print(\"Per-class metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\n",
    "            f\"Class {label}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f}\")\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    print(\"[VALUTAZIONE] Report di classificazione:\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4646513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento sui dati di Part1\n",
    "class_models, class_means = compute_latent_semantics_per_class(\n",
    "    feat_matrix_part1, lbls_part1, k=10)\n",
    "\n",
    "# Predizione su Part2\n",
    "predicted_labels = predict_label(feat_matrix_part2, class_models, class_means)\n",
    "\n",
    "# Valutazione\n",
    "evaluate(lbls_part2, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d60ef8",
   "metadata": {},
   "source": [
    "## Task 8\n",
    "Implementa un programma che, per ciascuna etichetta univoca l, calcola i corrispondenti c cluster più significativi associati alle immagini della parte 1 (utilizzando l'algoritmo DBScan); <br> i cluster risultanti devono essere visualizzati sia come nuvole di punti colorate in modo diverso in uno spazio MDS a 2 dimensioni, sia come gruppi di miniature di immagini.\n",
    "\n",
    "#### Obiettivo del Task\n",
    "In questo task esploriamo la **struttura interna dei dati di ciascuna classe**, cercando di identificare eventuali **cluster naturali** (gruppi di immagini statisticamente simili) all’interno delle feature.\n",
    "\n",
    "Per farlo combiniamo due tecniche fondamentali:\n",
    "- Dimensionality Reduction (PCA, UMAP).\n",
    "- **DBSCAN:** per identificare cluster basati sulla densità dei punti nello spazio delle feature.\n",
    "\n",
    "---\n",
    "#### **Metodo utilizzato**\n",
    "\n",
    "##### 1️ - **Riduzione dimensionale**\n",
    "- Prima di applicare il clustering, riduciamo lo spazio delle feature da 900 a **50 componenti**, mantenendo la maggior parte della varianza.\n",
    "- La riduzione aiuta DBSCAN a lavorare meglio, eliminando il rumore di alta dimensionalità.\n",
    "<br>Metodi:<br>\n",
    "    - **PCA (Principal Component Analysis)**: proietta i dati lungo le direzioni di massima varianza, preservando la struttura lineare globale.\n",
    "    - **UMAP (Uniform Manifold Approximation and Projection)**: preserva sia la struttura locale che globale, più veloce e scalabile rispetto a t-SNE.\n",
    "\n",
    "\n",
    "#### 2️ **Clustering con DBSCAN**\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise):\n",
    "- Identifica i cluster come gruppi di punti vicini tra loro.\n",
    "- I punti che non appartengono a nessun cluster vengono etichettati come **rumore (-1)**.\n",
    "\n",
    "Parametri principali utilizzati:\n",
    " - **eps**             Raggio di vicinanza per considerare due punti vicini     2.0\n",
    " - **min_samples**     Minimo numero di punti per formare un cluster            3\n",
    "\n",
    "#### 3️ **Selezione dei cluster più significativi**\n",
    "Dopo aver identificato tutti i cluster, selezioniamo gli **'n' cluster più popolosi** per ciascuna classe, escludendo il rumore.\n",
    "\n",
    "#### 4️ **Visualizzazione dei risultati**\n",
    "Abbiamo rappresentato i risultati in due modi:\n",
    "- **MDS 2D (Multidimensional Scaling)** \n",
    "- **Griglie di immagini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7b6df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applichiamo tecniche di dimensionality reduction\n",
    "def reduce_features(features, method, n_components, random_state=42):\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=n_components)\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Metodo di riduzione '{method}' non supportato.\")\n",
    "\n",
    "    return reducer.fit_transform(features)\n",
    "\n",
    "#Applichiamo una tecnica di riduzione mediante reduce_feature alle features di partenza ottenendo la lista di Feature Latenti\n",
    "#Applichiamo su insieme di feature latenti StandardScaler per cercare di ottenere migliori cluster tramite DBSCAN\n",
    "\n",
    "def apply_dbscan_with_pca(features, eps, min_samples, n_components, method):\n",
    "    print(f\"Applicazione di {method} -> Riduzione a {n_components} componenti\")\n",
    "    reduced_features = reduce_features(features, method, n_components=n_components)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    reduced_scaled = scaler.fit_transform(reduced_features)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(reduced_scaled)\n",
    "    return labels\n",
    "\n",
    "#Calcola i 'c' cluster di maggior cardinalità\n",
    "def top_c_clusters(cluster_labels, c):\n",
    "    label_counts = Counter(cluster_labels)\n",
    "    label_counts.pop(-1, None) # rimozione cluster catalogato come rumore (-1)\n",
    "    if not label_counts:\n",
    "        print(\"[WARN] DBSCAN non ha trovato alcun cluster valido (solo rumore).\")\n",
    "        return []\n",
    "    \n",
    "    # Estraiamo i 'c' cluster più frequenti\n",
    "    most_common = label_counts.most_common(c)\n",
    "    top = [int(lbl) for lbl, _ in most_common]\n",
    "    \n",
    "    if len(top) < c:\n",
    "        print(f\"[WARN] DBSCAN ha trovato solo {len(top)} cluster (meno di {c}).\")\n",
    "    return top\n",
    "\n",
    "\n",
    "#Applichiamo al risultato di DBSCAN l'algoritmo di MDS\n",
    "def plot_mds_clusters(features, cluster_labels, top_clusters, metric):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    #Generazione di una nuova matrice basata sul parametro metric (es. 'cosine')\n",
    "    D = pairwise_distances(features_scaled, metric=metric)\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    Y = mds.fit_transform(D)\n",
    "\n",
    "    cmap= matplotlib.colormaps['tab10']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(Y)):\n",
    "        lbl = cluster_labels[i]\n",
    "        if lbl in top_clusters:\n",
    "            color_idx = top_clusters.index(lbl)\n",
    "            plt.scatter(Y[i,0], Y[i,1], color=cmap(color_idx), s=30, edgecolor='k', linewidth=0.2)\n",
    "        else:\n",
    "            # punti rumore o cluster “non top”\n",
    "            plt.scatter(Y[i,0], Y[i,1], color='lightgray', s=8)\n",
    "    \n",
    "    plt.title(f\"MDS 2D - Top {len(top_clusters)} cluster\")\n",
    "    plt.xlabel(\"MDS 1\")\n",
    "    plt.ylabel(\"MDS 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Genera blocco di immagini trovate per cluster\n",
    "def show_cluster_thumbnails(images, cluster_labels, top_clusters, thumb_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    images: lista (o array) di percorsi file (lunghezza N), \n",
    "            ossia a images[i] corrisponde features[i].\n",
    "    cluster_labels: array (N,) di cluster per ogni immagine.\n",
    "    top_clusters: lista dei c cluster (int) che vogliamo visualizzare.\n",
    "    thumb_size: dimensione (w,h) di ogni miniatura.\n",
    "    Per ogni cluster ∈ top_clusters stampa a video (o fa plt.show) \n",
    "    una griglia di miniature (fino a ~16-25 alla volta).\n",
    "    \"\"\"\n",
    "    for cluster_id in top_clusters:\n",
    "        # Indici di tutte le immagini che appartengono a questo cluster\n",
    "        idxs = [i for i, cl in enumerate(cluster_labels) if cl == cluster_id]\n",
    "        print(f\"[INFO] Cluster {cluster_id}: {len(idxs)} immagini trovate\")\n",
    "\n",
    "        # Se vogliamo limitare a N miniatura per cluster (tipo 16):\n",
    "        max_display = min(len(idxs), 16)\n",
    "        n = int(np.ceil(np.sqrt(max_display)))  # facciamo una griglia n×n\n",
    "        plt.figure(figsize=(n, n))\n",
    "\n",
    "        for j, i_img in enumerate(idxs[:max_display]):\n",
    "            img = Image.open(images[i_img]).convert('RGB')\n",
    "            img_thumb = img.resize(thumb_size, Image.LANCZOS)\n",
    "            \n",
    "            ax = plt.subplot(n, n, j+1)\n",
    "            plt.imshow(img_thumb)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Cluster {cluster_id} – {len(idxs)} immagini (mostrate: {max_display})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "426a6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "def db_scan_detection(eps, min_samples, n_components, c, method):\n",
    "\n",
    "    # Costruisce l’elenco dei full path per tutte le immagini\n",
    "    base_folder = \"Part1\"  # o path assoluto \"/Users/.../Parte1\"\n",
    "    images_full = [os.path.join(base_folder, lbl, fname) for fname, lbl in zip(flname_part1, lbls_part1)]\n",
    "\n",
    "    # Scorre ogni label di Parte1 ed applico DBSCAN+PCA\n",
    "    unique_labels = np.unique(lbls_part1)  # es. [\"Glioma\",\"Meningioma\",\"Pituitary\"]\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"[INFO] Elaboro label: {lbl}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        #Estrae le righe di feat_matrix_part1 / flname_part1 corrispondenti\n",
    "        mask_lbl = (lbls_part1 == lbl)\n",
    "        features_label = feat_matrix_part1[mask_lbl]   # shape = (n_i, d)\n",
    "        images_label = np.array(images_full)[mask_lbl]\n",
    "\n",
    "        #Chiama la tua funzione PCA + DBSCAN\n",
    "        cluster_labels = apply_dbscan_with_pca(\n",
    "            features_label,\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            n_components=n_components,\n",
    "            method = method\n",
    "        )\n",
    "        print(f\"[INFO] Cluster-labels trovati: {np.unique(cluster_labels)}\")\n",
    "\n",
    "        # Trova i c cluster più grandi\n",
    "        top_clusters = top_c_clusters(cluster_labels, c)\n",
    "        print(f\"[INFO] Top {c} cluster (per dimensione): {top_clusters}\")\n",
    "\n",
    "        # MDS‐2D + scatter plot del clustering\n",
    "        print(f\"[INFO] Disegno MDS 2D per i cluster di '{lbl}' …\")\n",
    "\n",
    "        plot_mds_clusters(\n",
    "            features_label,\n",
    "            cluster_labels,\n",
    "            top_clusters,\n",
    "            metric='cosine'\n",
    "        )\n",
    "\n",
    "        # Creo le miniature di ogni cluster “significativo”\n",
    "        print(f\"[INFO] Genero miniature per ciascun cluster di '{lbl}' …\")\n",
    "        show_cluster_thumbnails(\n",
    "            images_label,      # array di stringhe di percorsi\n",
    "            cluster_labels,    # array di int di lunghezza n_i\n",
    "            top_clusters,      # la lista dei c indici di cluster\n",
    "            thumb_size=(64, 64)\n",
    "        )\n",
    "\n",
    "    print(\"\\n[FINITO] Task 8 completato per tutte le label di Parte1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5068fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 5.0            # valore DBSCAN di esempio\n",
    "min_samples = 3      # valore DBSCAN di esempio\n",
    "n_components = 50    # quante dimensioni tenere con PCA PRIMA di DBSCAN\n",
    "c = 3                # quanti cluster “significativi” voglio prendere per ciascuna label\n",
    "method = 'umap'      # umap or pca\n",
    "db_scan_detection(eps, min_samples, n_components, c, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a962a5",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "Implementa un programma che, dati le immagini della parte 1:\n",
    " - crea un classificatore m-NN (per una m specificata dall'utente),\n",
    " - crea un classificatore ad albero decisionale,<br>\n",
    "Per questo task, puoi utilizzare lo spazio delle feature di tua scelta.<br>\n",
    "Per le immagini della parte 2, prevede le etichette più probabili utilizzando il classificatore selezionato dall'utente.<br>\n",
    "Il sistema dovrebbe anche fornire valori di precisione, richiamo e punteggio F1 per etichetta, nonché un valore di accuratezza complessiva.    \n",
    "\n",
    "#### **Obiettivo del Task**\n",
    "In questo task testiamo due approcci di **classificazione supervisionata**, usando come feature i **Color Moments** estratti in precedenza.  \n",
    "L’obiettivo è **prevedere la classe** di ciascuna immagine del dataset **Parte 2**, dopo aver addestrato i classificatori sulla **Parte 1**.\n",
    "\n",
    "\n",
    "#### **Classificatori utilizzati**\n",
    "\n",
    " - k-NN (k Nearest Neighbors)\n",
    "<br>Parametri:<br>\n",
    "     - **k = 5**, valore bilanciato che riduce il rischio di overfitting.\n",
    "\n",
    "\n",
    "- Decision Tree\n",
    "<br>Parametri:<br>\n",
    "     - Parametri di default → l’albero viene costruito senza limitazioni particolari sulla profondità.\n",
    "\n",
    "#### **Metodo**\n",
    "\n",
    "1. Training:\n",
    "   - I classificatori vengono addestrati sulle feature del dataset **Parte 1**, per ciascuna immagine e classe nota.\n",
    "2. Testing:\n",
    "   - I modelli vengono testati sulle feature delle immagini della **Parte 2**, simulando un vero scenario di classificazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta il valore di m per l'm-NN\n",
    "m = 5  # Modifica questo valore in base alle tue necessità\n",
    "\n",
    "# Addestramento m-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=m)\n",
    "knn_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Addestramento Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Predizioni su Part2\n",
    "pred_knn = knn_model.predict(feat_matrix_part2)\n",
    "pred_dt = dt_model.predict(feat_matrix_part2)\n",
    "\n",
    "# Valutazione m-NN\n",
    "print(\"Risultati m-NN:\")\n",
    "print(classification_report(lbls_part2, pred_knn))\n",
    "print(\"Accuratezza complessiva m-NN:\", accuracy_score(lbls_part2, pred_knn))\n",
    "\n",
    "# Valutazione Decision Tree\n",
    "print(\"Risultati Decision Tree:\")\n",
    "print(classification_report(lbls_part2, pred_dt))\n",
    "print(\"Accuratezza complessiva Decision Tree:\", accuracy_score(lbls_part2, pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24873257",
   "metadata": {},
   "source": [
    "# Task 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc517e5",
   "metadata": {},
   "source": [
    "### 10a:\n",
    "Implementa uno strumento di Locality Sensitive Hashing (LSH) (per la distanza euclidea) che prende come input (a) il numero di livelli, L, (b) il numero di hash per livello, h, e (c) un insieme di vettori come input e crea una struttura di indice in memoria contenente l'insieme di vettori dato. \n",
    "\n",
    "Vedi:\n",
    "\"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\" (di Alexandr Andoni e Piotr Indyk). Communications of the ACM, vol. 51, no. 1, 2008, pp. 117-122.    \n",
    "\n",
    "### 10b:\n",
    "Implementa un algoritmo di ricerca di immagini simili utilizzando questa struttura di indice che memorizza le immagini della parte 1 e un modello visivo di tua scelta (il modello visivo combinato deve avere almeno 256 dimensioni): per una data immagine di query e un numero intero t, \n",
    " \n",
    "*visualizza le t immagini più simili,\n",
    "*fornisce il numero di immagini univoche e il numero complessivo di immagini considerate durante il processo.    \n",
    "\n",
    "\n",
    "#### **Obiettivo del Task**\n",
    "\n",
    "Fino ad ora abbiamo calcolato la similarità tra immagini confrontando la query con tutte le immagini del dataset.  \n",
    "Questo approccio è efficace, ma **computazionalmente molto costoso**, soprattutto per dataset di grandi dimensioni.\n",
    "\n",
    "In questo task introduciamo il **Locality Sensitive Hashing (LSH)**, una tecnica che consente di **ridurre il numero di confronti**, accelerando la ricerca delle immagini simili.\n",
    "\n",
    "#### **Metodo adottato**\n",
    "\n",
    " - 1 Creazione dell’indice LSH\n",
    "<br>Abbiamo implementato un indice chiamato **LSH_EuclideanQuantized**, progettato per la **distanza Euclidea**.<br>  \n",
    "    <br>Ogni vettore viene:<br>\n",
    "     - **centrato**, sottraendo la media globale,\n",
    "     - **normalizzato** a norma unitaria (**L2 norm**).\n",
    "\n",
    "- 2️ Ricerca della query\n",
    "    <br>Per cercare immagini simili a una query:<br>\n",
    "     - Si estrae il vettore dei Color Moments,\n",
    "     - Si centra e normalizza con gli stessi parametri del training,\n",
    "     - Si cercano i bucket hash più rilevanti (in tutti i livelli hash),\n",
    "     - Si calcolano le **distanze Euclidee reali** solo con i candidati recuperati dai bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b424686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH_EuclideanQuantized:\n",
    "    \"\"\"\n",
    "    LSH per distanza Euclidea con quantizzazione (p-stable).\n",
    "    num_layers = L, num_hashes = h, dim = D, r = bucket width.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_hashes: int, dim: int, r: float):\n",
    "        self.L = num_layers\n",
    "        self.h = num_hashes\n",
    "        self.d = dim\n",
    "        self.r = r\n",
    "\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.L)]\n",
    "        self.a_vectors = [\n",
    "            [np.random.randn(self.d) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.b_offsets = [\n",
    "            [np.random.uniform(0, self.r) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.data_vectors = None\n",
    "\n",
    "    def _compute_hash_tuple(self, vec: np.ndarray, layer_idx: int) -> tuple:\n",
    "        bits = []\n",
    "        for j in range(self.h):\n",
    "            a_j = self.a_vectors[layer_idx][j]\n",
    "            b_j = self.b_offsets[layer_idx][j]\n",
    "            proj = float(np.dot(a_j, vec) + b_j)\n",
    "            h_val = int(np.floor(proj / self.r))\n",
    "            bits.append(h_val)\n",
    "        return tuple(bits)\n",
    "\n",
    "    def index(self, vectors: np.ndarray):\n",
    "        self.data_vectors = vectors\n",
    "        N, D = vectors.shape\n",
    "        assert D == self.d, f\"Dimensione vettore ({D}) ≠ D di LSH ({self.d}).\"\n",
    "        for idx in range(N):\n",
    "            v = vectors[idx]\n",
    "            for l in range(self.L):\n",
    "                key = self._compute_hash_tuple(v, l)\n",
    "                self.hash_tables[l][key].append(idx)\n",
    "\n",
    "    def query(self, q_vec: np.ndarray, top_t: int = 5):\n",
    "        assert q_vec.shape[0] == self.d, \"Dimensione query ≠ D.\"\n",
    "        candidati = set()\n",
    "        total_checked = 0\n",
    "        for l in range(self.L):\n",
    "            h_tuple = self._compute_hash_tuple(q_vec, l)\n",
    "            bucket = self.hash_tables[l].get(h_tuple, [])\n",
    "            total_checked += len(bucket)\n",
    "            candidati.update(bucket)\n",
    "\n",
    "        risultati = []\n",
    "        for idx in candidati:\n",
    "            v_i = self.data_vectors[idx]\n",
    "            dist = np.linalg.norm(v_i - q_vec)\n",
    "            risultati.append((idx, dist))\n",
    "        risultati.sort(key=lambda x: x[1])\n",
    "        top_results = risultati[:top_t]\n",
    "        return top_results, len(candidati), total_checked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Center + L2 normalize su Part1\n",
    "mean_vec = np.mean(feat_matrix_part1, axis=0)\n",
    "feat_centered = feat_matrix_part1 - mean_vec\n",
    "feat_normed = normalize(feat_centered, norm='l2', axis=1)\n",
    "\n",
    "# 2) Parametri LSH con quantizzazione\n",
    "D = feat_normed.shape[1]      # ad esempio 900\n",
    "L = 5                         # numero di layer (esempio)\n",
    "h = 5                        # numero di hash per layer (esempio)\n",
    "r = 5.0                       # bucket width, da sperimentare\n",
    "\n",
    "# 3) Creo l'indice\n",
    "lsh_quant = LSH_EuclideanQuantized(num_layers=L, num_hashes=h, dim=D, r=r)\n",
    "lsh_quant.index(feat_normed)\n",
    "\n",
    "print(f\"[INFO] Indice LSH-Quant creato. D={D}, L={L}, h={h}, r={r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "022f8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_lsh_quant(base_folder: str, img_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a img_path (di Part2) usando lsh_quant costruito su Part1.\n",
    "    \"\"\"\n",
    "    # 1) Estrai feature raw (900-dim)\n",
    "    raw_q = np.array(extract_color_moments(img_path), dtype=np.float32)\n",
    "\n",
    "    # 2) Center + normalize (stesso mean_vec usato su Part1)\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # 3) Chiamata LSH\n",
    "    top_results, unique_count, total_checked = lsh_quant.query(q_normed, top_t=k)\n",
    "\n",
    "    # 4) Stampo i risultati testuali\n",
    "    print(f\"\\n[LSH-Quant] Top {k} simili a: {img_path}\")\n",
    "    for rank, (idx, dist) in enumerate(top_results, start=1):\n",
    "        label = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        print(f\"  {rank}. {fname} | Classe: {label} | Distanza Euclidea: {dist:.2f}\")\n",
    "    print(f\"[LSH-Quant] Immagini uniche considerate: {unique_count}\")\n",
    "    print(f\"[LSH-Quant] Immagini totali controllate: {total_checked}\")\n",
    "\n",
    "    # 5) Visualizzazione (query + k risultati)\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(4*(k+1), 4))\n",
    "    img_q = cv2.imread(img_path)\n",
    "    img_q = cv2.cvtColor(img_q, cv2.COLOR_BGR2RGB)\n",
    "    axs[0].imshow(img_q)\n",
    "    axs[0].set_title(\"Query (LSH-Quant)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, (idx, dist) in enumerate(top_results, start=1):\n",
    "        lab = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        full_path = os.path.join(base_folder, lab, fname)\n",
    "        img_match = cv2.imread(full_path)\n",
    "        img_match = cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img_match)\n",
    "        axs[i].set_title(f\"Rank {i}\\nd={dist:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ce3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo su un'immagine di Part2 ---\n",
    "query_path = \"Part2/brain_menin/brain_menin_1003.jpg\"\n",
    "\n",
    "print(query_path);\n",
    "k = 5                         # numero di immagini simili da visualizzare\n",
    "\n",
    "# Eseguo la ricerca LSH\n",
    "find_k_similar_lsh_quant(\"Part1\", query_path, k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
