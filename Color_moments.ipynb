{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "get_ipython().system('{sys.executable} -m pip install --quiet numpy pandas matplotlib seaborn scikit-learn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b8fc2",
   "metadata": {},
   "source": [
    "# Color Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "from sklearn.manifold import MDS\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eaae8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58732e0",
   "metadata": {},
   "source": [
    "## Descrizione della funzione `crop_to_brain()`\n",
    "\n",
    "Questa funzione prevede i seguenti passaggi principali:\n",
    "\n",
    "1. **Conversione in scala di grigi:** semplifica l’immagine riducendola a un solo canale di intensità luminosa.\n",
    "2. **Applicazione di una soglia binaria:** vengono considerati significativi solo i pixel con intensità superiore a **10** (per eliminare il nero dello sfondo).\n",
    "3. **Calcolo del bounding box:** viene individuato il rettangolo più piccolo che contiene tutti i pixel significativi.\n",
    "4. **Ritaglio dell’immagine:** l’immagine viene ritagliata lungo i bordi del bounding box, lasciando solo l’area cerebrale.\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo della pre-elaborazione\n",
    "\n",
    "L’obiettivo di questa fase è **standardizzare l’area di interesse** all’interno delle immagini e **rimuovere le parti non informative**.  \n",
    "Questo porta a una serie di vantaggi importanti:\n",
    "\n",
    "- ✅ **Migliora la qualità dei descrittori:** eliminando il rumore dei bordi non rilevanti.\n",
    "- ✅ **Standardizza posizione e dimensione:** le immagini risultano più simili dal punto di vista geometrico.\n",
    "- ✅ **Riduce la variabilità non utile:** rende i confronti tra immagini più affidabili.\n",
    "\n",
    "---\n",
    "\n",
    "## Output della funzione\n",
    "\n",
    "La funzione restituisce una **nuova immagine**, che può avere dimensioni variabili in base al contenuto ritagliato.\n",
    "\n",
    "Questa immagine verrà utilizzata successivamente nei task di estrazione dei descrittori visivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00999225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_brain(img):\n",
    "    \"\"\"Ritaglia l'area informativa (cervello) da un'immagine.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    coords = cv2.findNonZero(thresh)\n",
    "    if coords is not None:\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        return img[y:y+h, x:x+w]\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    return img  # fallback\n",
    "img = cv2.imread(\"Part1/brain_glioma/brain_glioma_0101.jpg\")\n",
    "\n",
    "# Ritaglia il cervello\n",
    "cropped_img = crop_to_brain(img)\n",
    "\n",
    "# Converte da BGR (usato da OpenCV) a RGB (usato da matplotlib)\n",
    "cropped_rgb = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Mostra l'immagine\n",
    "plt.imshow(cropped_rgb)\n",
    "plt.title(\"Cervello ritagliato\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    return data['features'], data['labels'], data.get('filenames', [f\"img_{i}\" for i in range(len(data['features']))])\n",
    "\n",
    "\n",
    "feat_matrix_part1, lbls_part1, flname_part1 = load_features(\n",
    "    \"color_moments_part1.npz\")\n",
    "feat_matrix_part2, lbls_part2, flname_part2 = load_features(\n",
    "    \"color_moments_part2.npz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adff1e",
   "metadata": {},
   "source": [
    "# Task 1-2\n",
    "\n",
    "## Implementa un programma che **estrae e memorizza i descrittori di feature** per tutte le immagini nel set di dati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b7914",
   "metadata": {},
   "source": [
    "## Estrazione dei Color Moments**\n",
    "\n",
    "Dopo aver ritagliato la porzione utile dell’immagine, vogliamo rappresentare numericamente le sue caratteristiche visive tramite **Color Moments**.\n",
    "\n",
    "### **Cos'è un Color Moment?**\n",
    "\n",
    "Per ogni canale colore (R, G, B) calcoliamo 3 statistiche **per ogni regione dell’immagine**:\n",
    "- **Media (Mean):** indica il colore medio.\n",
    "- **Deviazione standard (Std):** misura la variabilità dei colori (contrasto).\n",
    "- **Skewness (Assimmetria):** misura lo sbilanciamento della distribuzione dei colori verso valori più chiari o più scuri.\n",
    "\n",
    "Queste tre statistiche catturano l’aspetto cromatico di una porzione dell’immagine.\n",
    "\n",
    "---\n",
    "\n",
    "### **Suddivisione dell'immagine in una griglia 10×10**\n",
    "\n",
    "Per non perdere le informazioni spaziali, suddividiamo l’immagine in una **griglia 10×10**:\n",
    "- Ogni cella della griglia corrisponde a una porzione locale dell’immagine.\n",
    "- I Color Moments vengono calcolati separatamente per ciascuna cella.\n",
    "\n",
    "### **Output finale**\n",
    "\n",
    "Per ogni cella vengono calcolati:\n",
    "- 3 valori per R (mean, std, skewness)\n",
    "- 3 valori per G\n",
    "- 3 valori per B\n",
    "\n",
    "Totale: **3 statistiche × 3 canali × 100 celle = 900 feature per immagine**\n",
    "\n",
    "Queste 900 feature rappresentano **numericamente l’intera immagine**, mantenendo sia l’informazione cromatica che quella spaziale.\n",
    "\n",
    "---\n",
    "\n",
    "## **Vantaggi dei Color Moments**\n",
    "\n",
    "- **Calcolo semplice:** molto più rapido rispetto a descrittori complessi come SIFT o SURF.\n",
    "- **Informazioni significative:** nei contesti medici, i colori possono evidenziare strutture anomale.\n",
    "- **Rappresentazione locale:** grazie alla suddivisione in celle, preserviamo anche informazioni spaziali.\n",
    "- **Efficienza computazionale:** perfetto per dataset medi o grandi.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔬 **Considerazioni finali per Task 1-2**\n",
    "\n",
    "Dopo questa fase abbiamo per ogni immagine:\n",
    "- Un’area cerebrale ritagliata e priva di rumore di background.\n",
    "- Un vettore numerico di **900 feature**, pronto per il confronto tra immagini e per l’uso in algoritmi di Machine Learning.\n",
    "\n",
    "Il passo successivo sarà salvare questi descrittori in formato efficiente (`.npz`) e utilizzarli nei task successivi per la ricerca e la classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8196bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mean_heatmap(img_path):\n",
    "    # Estrai le feature come già fai nella tua pipeline\n",
    "    feats = extract_color_moments(img_path)          # 900 valori\n",
    "    m = np.reshape(feats, (10, 10, 3, 3))            # (righe, colonne, canali, momenti)\n",
    "    mean_gray = m[:, :, 0, 0]                        # 1° canale, 1° momento (= media)\n",
    "\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.imshow(mean_gray, cmap='hot', interpolation='nearest')\n",
    "    plt.title('Mean intensity per cell')\n",
    "    plt.colorbar(); plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_moments(img_path):\n",
    "    \"\"\"Estrae Color Moments su una griglia 10x10 da un'immagine.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERRORE] Immagine non trovata: {img_path}\")\n",
    "        return None\n",
    "\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    img = crop_to_brain(img)\n",
    "    img = cv2.resize(img, (300, 100))  # Griglia uniforme\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    grid_h, grid_w = h // 10, w // 10\n",
    "    features = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            cell = img[i*grid_h:(i+1)*grid_h, j*grid_w:(j+1)*grid_w]\n",
    "            for channel in range(3):\n",
    "                pixels = cell[:, :, channel].flatten()\n",
    "                if np.std(pixels) > 0:\n",
    "                    mean = np.mean(pixels)\n",
    "                    std = np.std(pixels)\n",
    "                    sk = skew(pixels)\n",
    "                    if np.isnan(sk): sk = 0\n",
    "                else:\n",
    "                    mean, std, sk = 0, 0, 0\n",
    "                features.extend([mean, std, sk])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8957034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_features(base_folder, subfolders, output_file):\n",
    "    \"\"\"Estrae le feature da immagini organizzate in sottocartelle e le salva in un file .npz.\"\"\"\n",
    "    all_features, all_filenames, all_labels = [], [], []\n",
    "\n",
    "    for label in subfolders:\n",
    "        folder_path = os.path.join(base_folder, label)\n",
    "        print(f\"[INFO] Elaboro cartella: {label}\")\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                features = extract_color_moments(img_path)\n",
    "                if features is not None:\n",
    "                    all_features.append(features)\n",
    "                    all_filenames.append(filename)\n",
    "                    all_labels.append(label)\n",
    "\n",
    "    np.savez(output_file,\n",
    "             features=np.array(all_features),\n",
    "             filenames=np.array(all_filenames),\n",
    "             labels=np.array(all_labels))\n",
    "    print(f\"[SALVATO] Features salvate in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574cce3",
   "metadata": {},
   "source": [
    "Alcuni plot di visualizzazione:\n",
    "    -Come mi divide in 10*10 griglie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40171a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Inserisci il path a una immagine del dataset\n",
    "    img_path = \"Part1/brain_menin/brain_menin_0050.jpg\"\n",
    "\n",
    "    # Visualizza la griglia\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = crop_to_brain(img)\n",
    "    img = cv2.resize(img, (300, 100))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for i in range(1, 10):\n",
    "        ax.axhline(i * 10, color='white', linewidth=0.5)\n",
    "        ax.axvline(i * 30, color='white', linewidth=0.5)\n",
    "    ax.set_title(\"Griglia 10x10 sull'immagine\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Heatmap media canale R\n",
    "    features = extract_color_moments(img_path)\n",
    "    mean_r = np.array([features[i*9 + 6] for i in range(100)]).reshape(10, 10)\n",
    "    sns.heatmap(mean_r, cmap='gray')\n",
    "    plt.title(\"Media valore canale per cella\")\n",
    "    plt.xlabel(\"Cella\")\n",
    "    plt.ylabel(\"Valore\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2dfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    img_path = \"Part1/brain_menin/brain_menin_0050.jpg\"\n",
    "\n",
    "    # Caricamento immagine e pre-processing\n",
    "    img = cv2.imread(img_path)\n",
    "    img = crop_to_brain(img)\n",
    "    #img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Immagine non trovata.\")\n",
    "\n",
    "    img = cv2.resize(img, (300, 100))\n",
    "\n",
    "    # Estrarre le feature\n",
    "    features = extract_color_moments(img_path)\n",
    "\n",
    "    # Estrai solo le medie del canale grigio (che è il primo dei 3 momenti per ogni cella)\n",
    "    mean_vals = [features[i * 3] for i in range(100)]\n",
    "    mean_matrix = np.array(mean_vals).reshape(10, 10)\n",
    "\n",
    "    # Normalizza le medie tra 0 e 1\n",
    "    norm_means = (mean_matrix - np.min(mean_matrix)) / (np.max(mean_matrix) - np.min(mean_matrix))\n",
    "\n",
    "    # Crea figura\n",
    "    fig, ax = plt.subplots(figsize=(8, 3))\n",
    "    ax.imshow(img, cmap='gray')\n",
    "\n",
    "    # Sovrappone rettangoli colorati trasparenti per ogni cella\n",
    "    # h, w = img.shape\n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "    cell_h, cell_w = h // 10, w // 10\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            intensity = norm_means[i, j]\n",
    "            rect = plt.Rectangle((j * cell_w, i * cell_h), cell_w, cell_h,\n",
    "                                 linewidth=0.5, edgecolor='white',\n",
    "                                 facecolor=plt.cm.viridis(intensity), alpha=0.4)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    ax.set_title(\"Overlay: Media Intensità su Griglia 10x10\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assicurati che queste funzioni siano già state definite prima!\n",
    "# - extract_color_moments_gray\n",
    "# - crop_to_brain\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_path = \"Part1/brain_menin/brain_menin_0050.jpg\" \n",
    "img = cv2.imread(img_path)\n",
    "img = crop_to_brain(img)\n",
    "img = cv2.resize(img, (300, 100))\n",
    "\n",
    "features = extract_color_moments(img_path)\n",
    "# Estrai le tre matrici 10x10: mean, std, skew\n",
    "mean_mat = np.array([features[i*3]     for i in range(100)]).reshape(10, 10)\n",
    "std_mat  = np.array([features[i*3 + 1] for i in range(100)]).reshape(10, 10)\n",
    "skw_mat  = np.array([features[i*3 + 2] for i in range(100)]).reshape(10, 10)\n",
    "\n",
    "# Utility per normalizzazione\n",
    "def norm01(arr):\n",
    "    arr = arr.astype(np.float32)\n",
    "    return (arr - arr.min()) / (arr.max() - arr.min() + 1e-7)\n",
    "\n",
    "# Funzione per upscaling\n",
    "def upscale(mat, size=(300, 100)):\n",
    "    return cv2.resize(mat, size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Funzione per overlay heat-map su immagine grigia\n",
    "def overlay(gray_img, heat, alpha=0.6, cmap='turbo'):\n",
    "    colored = plt.colormaps.get_cmap(cmap)(heat)[..., :3] * 255\n",
    "    colored = colored.astype(np.uint8)\n",
    "    #overlay = cv2.addWeighted(colored, alpha, cv2.cvtColor(gray_img, cv2.COLOR_GRAY2RGB), 1-alpha, 0)\n",
    "    overlay = cv2.addWeighted(colored, alpha, img, 1-alpha, 0)\n",
    "    return overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_std = (std_mat > 10).astype(np.uint8)\n",
    "heat_std = upscale(mask_std * 255)\n",
    "overlay_std = overlay(img, norm01(heat_std), cmap='plasma')\n",
    "\n",
    "plt.imshow(overlay_std)\n",
    "plt.title(\"Zone a variabilità elevata (std > 30)\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_color_moments(\"Part1/brain_glioma/brain_glioma_0001.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8dc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri\n",
    "subfolders = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]\n",
    "\n",
    "# Estrazione e salvataggio\n",
    "process_and_save_features(\"Part1\", subfolders, \"color_moments_part1.npz\")\n",
    "process_and_save_features(\"Part2\", subfolders, \"color_moments_part2.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78dfcc",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implementa un programma che, **dato il **nome di un file immagine** e un valore \"k\"**, **restituisce e visualizza le k immagini più simili** in base a ciascun modello visivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6127405",
   "metadata": {},
   "source": [
    "# 🔍 Task 3 – Retrieval di Immagini Mediche Basato sui Color Moments\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "In questo task implementiamo un **sistema di retrieval**, ossia un metodo per **trovare le immagini più simili** a una data immagine di query, basandoci sulle **feature estratte (Color Moments)**.\n",
    "\n",
    "Questa procedura è fondamentale per:\n",
    "- Identificare rapidamente immagini simili in grandi dataset.\n",
    "- Analizzare quali immagini hanno caratteristiche visive più vicine tra loro.\n",
    "- Supportare l’analisi diagnostica trovando casi simili a quello in esame.\n",
    "\n",
    "---\n",
    "\n",
    "### **Funzione principale: `find_k_similar_unified()`**\n",
    "\n",
    "Questa funzione automatizza i seguenti passaggi:\n",
    "\n",
    "\n",
    "**Estrazione delle feature della query**\n",
    "\n",
    "In questo modo query e dataset sono nello stesso spazio vettoriale e possono essere confrontati.\n",
    "\n",
    "**Calcolo della distanza tra feature**\n",
    "\n",
    "Per determinare la similarità tra immagini possiamo scegliere **due diverse metriche di distanza**:\n",
    "\n",
    "####  a) Distanza Euclidea\n",
    "- Formula classica della distanza geometrica tra vettori.\n",
    "- **Veloce da calcolare**, ma non tiene conto della varianza delle singole feature.\n",
    "\n",
    "\n",
    "####  b)  Distanza di Mahalanobis\n",
    "- Tiene conto della covarianza tra le feature, normalizzando i contributi di ogni dimensione.\n",
    "- Più robusta, considera la distribuzione statistica dei dati.\n",
    "\n",
    "**Esclusione della query stessa dai risultati**\n",
    "\n",
    "Se la query è già presente nel dataset, viene esclusa dal risultato per evitare che venga restituita come immagine più simile a sé stessa.\n",
    "Nel codice, questo viene fatto impostando la distanza dell’immagine query a infinito.\n",
    "\n",
    "**Ordinamento dei risultati**\n",
    "\n",
    "Le immagini vengono ordinate in base alla distanza calcolata e vengono selezionati i k risultati più vicini:\n",
    "\n",
    "**Visualizzazione dei risultati**\n",
    "\n",
    "---\n",
    "\n",
    "### Analisi Dei Risultati:\n",
    "- La distanza euclidea ha un forte incremento nella distanza tra i primi risultati e quelli successivi, segno che la Euclidea non gestisce bene la variabilità interna delle feature.\n",
    "-  La distanza Mahalanobis fornisce una valutazione più equilibrata e statisticamente coerente, individuando immagini simili dal punto di vista statistico, anche se appartengono a classi diverse\n",
    "- La distanza Mahalanobis è più adatta quando vogliamo capire quali immagini hanno una distribuzione delle feature simile, anche se appartengono a classi diverse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_unified(base_folder, img_path, k, distance_type=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a una query utilizzando distanza euclidea o Mahalanobis.\n",
    "    \n",
    "    Args:\n",
    "        base_folder (str): Cartella base in cui cercare le immagini.\n",
    "        img_path (str): Percorso immagine query.\n",
    "        k (int): Numero di immagini da restituire.\n",
    "        distance_type (str): Tipo di distanza ('euclidean' o 'mahalanobis').\n",
    "    \"\"\"\n",
    "    # Estrai le feature dalla query\n",
    "    query_feature = extract_color_moments(img_path)\n",
    "    if query_feature is None:\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    if distance_type == \"euclidean\":\n",
    "        distances = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "\n",
    "    elif distance_type == \"mahalanobis\":\n",
    "        # Calcola matrice di covarianza e la sua inversa\n",
    "        cov = np.cov(feat_matrix_part1.T)\n",
    "        try:\n",
    "            cov_inv = np.linalg.inv(cov)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"[ERRORE] Uso pseudoinversa per matrice non invertibile.\")\n",
    "            cov_inv = np.linalg.pinv(cov)\n",
    "\n",
    "        query_feature = query_feature.flatten()  \n",
    "        distances = np.array([mahalanobis(query_feature, f, cov_inv) for f in feat_matrix_part1])\n",
    "\n",
    "        # Escludi la query stessa se già presente\n",
    "        query_filename = os.path.basename(img_path)\n",
    "        query_label = os.path.basename(os.path.dirname(img_path))\n",
    "        for i in range(len(flname_part1)):\n",
    "            if flname_part1[i] == query_filename and lbls_part1[i] == query_label:\n",
    "                distances[i] = np.inf\n",
    "                break\n",
    "    else:\n",
    "        print(f\"[ERRORE] Tipo di distanza non supportato: {distance_type}\")\n",
    "        return\n",
    "\n",
    "    # Trova i top-k\n",
    "    top_k_idx = np.argsort(distances)[:k]\n",
    "\n",
    "    # Output testuale\n",
    "    print(f\"\\nTop {k} immagini simili a: {img_path} (Distanza: {distance_type})\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        print(f\"{rank + 1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Distanza: {distances[idx]:.2f}\")\n",
    "\n",
    "    # Visualizzazione immagini\n",
    "    fig, axs = plt.subplots(1, k + 1, figsize=(15, 5))\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, idx in enumerate(top_k_idx):\n",
    "        img_match_path = os.path.join(base_folder, lbls_part1[idx], flname_part1[idx])\n",
    "        img_match = cv2.imread(img_match_path)\n",
    "        axs[i + 1].imshow(cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB))\n",
    "        axs[i + 1].set_title(f\"Rank {i + 1}\\nD={distances[idx]:.2f}\")\n",
    "        axs[i + 1].axis('off')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(distances, bins=30, kde=True)\n",
    "    plt.title(f\"Distribuzione delle distanze - {distance_type.capitalize()}\")\n",
    "    plt.xlabel(\"Distanza\")\n",
    "    plt.ylabel(\"Frequenza\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.plot(range(1, k + 1), distances[top_k_idx], marker='o')\n",
    "    plt.title(\"Distanza vs. Rank\")\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"Distanza\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373057c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Part1\"\n",
    "img_path = \"Part1/brain_glioma/brain_glioma_0005.jpg\"\n",
    "\n",
    "find_k_similar_unified(base_folder, img_path, k=5, distance_type=\"euclidean\")\n",
    "find_k_similar_unified(base_folder, img_path, k=5, distance_type=\"mahalanobis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d74cc",
   "metadata": {},
   "source": [
    "# Task 4 – Confronto tra Strategie di Classificazione Basate su Distanze\n",
    "\n",
    " Implementa un programma che, dati:\n",
    "- (a) un'immagine di query della **Parte 2**,\n",
    "- (b) uno **spazio di feature** selezionato dall'utente,\n",
    "- (c) un numero intero positivo **k**,\n",
    "\n",
    "identifichi ed elenchi le **k etichette di corrispondenza più probabili**, insieme ai loro punteggi, calcolati nello **spazio di feature selezionato**.\n",
    "\n",
    "### Definizioni\n",
    "\n",
    "**Feature space:** spazio vettoriale in cui ogni immagine è rappresentata come un **vettore di caratteristiche** (feature vector).\n",
    "\n",
    "**Selected feature space:** indica **il tipo di caratteristiche** estratte per rappresentare le immagini (es. color moments, HOG, deep features, ecc.).\n",
    "\n",
    "## Obiettivo\n",
    "\n",
    "In questo task implementiamo un confronto tra **due diverse strategie di classificazione**, entrambe basate sulle distanze tra la query e le immagini del dataset.  \n",
    "\n",
    "L’obiettivo è capire a quale **classe** appartiene una immagine query analizzando le sue **feature (Color Moments)** e confrontandole con quelle delle classi note.\n",
    "\n",
    "---\n",
    "\n",
    "## Descrizione del codice\n",
    "\n",
    "### 1️: Estrazione delle feature della query\n",
    "\n",
    "### 2: **Strategia 1: Distanza media per classe**\n",
    "\n",
    "#### Descrizione\n",
    "- Viene calcolata la **distanza Euclidea** tra la query e tutte le immagini del dataset.\n",
    "- Per ogni **classe**, viene calcolata la **media delle distanze** tra la query e tutte le immagini appartenenti a quella classe.\n",
    "\n",
    "#### Vantaggi:\n",
    "- Tiene conto di **tutte le immagini** della classe.\n",
    "- Robusta nei confronti di rumore casuale nelle singole immagini.\n",
    "\n",
    "#### Svantaggi:\n",
    "- **Sensibile agli outlier**: immagini anomale possono aumentare la distanza media della classe.\n",
    "\n",
    "### 3: **Strategia 2: Distanza dal prototipo della classe**\n",
    "\n",
    "#### Descrizione\n",
    "- Per ogni classe viene calcolato il **prototipo**, ovvero il **centroide** (media vettoriale) delle feature di tutte le immagini della classe.\n",
    "- Si calcola la distanza Euclidea tra la query e il **centroide** di ogni classe.\n",
    "\n",
    "#### Vantaggi:\n",
    "- **Robusto agli outlier**, perché ogni classe è rappresentata da un solo vettore medio.\n",
    "- Più veloce da calcolare.\n",
    "\n",
    "#### Svantaggi:\n",
    "- Potrebbe non rappresentare bene le **sottostrutture interne** della classe.\n",
    "- Se la query è simile solo ad un sottogruppo della classe, ma distante dal centroide, questa strategia può dare un risultato meno preciso.\n",
    "\n",
    "### 4️: Visualizzazione dei risultati\n",
    "\n",
    "Questo confronto aiuta a capire quale strategia restituisce la classe più vicina alla query.\n",
    "\n",
    "### 5️: Analisi automatica del miglior risultato\n",
    "\n",
    "La funzione confronta i risultati delle due strategie e segnala se **entrambi i metodi identificano la stessa classe**.  \n",
    "In caso contrario, suggerisce la strategia che ha ottenuto la distanza più bassa.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Analisi dei risultati\n",
    "\n",
    "- La **strategia con il centroide** fornisce distanze più basse e quindi una rappresentazione più efficace del contenuto medio della classe.\n",
    "- Entrambe le strategie concordano sulla classe corretta, ma il centroide è più rappresentativo in termini numerici.\n",
    "\n",
    "## Confronto tra le due strategie\n",
    "\n",
    "| Strategia               | Punti di forza                                  | Limiti                                    |\n",
    "|-------------------------|--------------------------------------------------|------------------------------------------|\n",
    "| Media per classe        | Considera tutte le immagini                      | Sensibile agli outlier                   |\n",
    "| Prototipo (centroide)   | Robusto agli outlier, semplice da calcolare      | Può non catturare sottogruppi specifici  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38963d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ranking_strategies(query_img_path, k=2):\n",
    "    \"\"\"\n",
    "    Confronta le etichette predette da due strategie:\n",
    "    - distanza media per classe\n",
    "    - distanza al rappresentante (prototipo) della classe\n",
    "    Visualizza solo le top-k etichette con un grafico comparativo.\n",
    "    Inoltre stampa quale strategia sembra migliore in base alla distanza top-1.\n",
    "    \"\"\"\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "\n",
    "    query_feature = extract_color_moments(query_img_path)\n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    # ===== Strategia 1: distanza media per classe =====\n",
    "    distances_all = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "    df_all = pd.DataFrame({\n",
    "        'label': lbls_part1,\n",
    "        'distance': distances_all\n",
    "    })\n",
    "    mean_dists = df_all.groupby('label')['distance'].mean().sort_values()\n",
    "    \n",
    "    # ===== Strategia 2: distanza dal prototipo (centroide) =====\n",
    "    df_features = pd.DataFrame(feat_matrix_part1)\n",
    "    df_features['label'] = lbls_part1\n",
    "    class_prototypes = df_features.groupby('label').mean().drop(columns=['label'], errors='ignore')\n",
    "    proto_vectors = class_prototypes.values\n",
    "    proto_labels = class_prototypes.index\n",
    "    proto_dists = euclidean_distances(proto_vectors, query_feature).flatten()\n",
    "    proto_dists_series = pd.Series(proto_dists, index=proto_labels).sort_values()\n",
    "    \n",
    "    # ===== Prendi le top-k etichette comuni =====\n",
    "    top_k_mean = mean_dists.head(k)\n",
    "    top_k_proto = proto_dists_series.head(k)\n",
    "\n",
    "    union_labels = sorted(set(top_k_mean.index).union(set(top_k_proto.index)))\n",
    "\n",
    "    # ===== Plot solo per le top-k etichette =====\n",
    "    x = np.arange(len(union_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.bar(x - width/2, [mean_dists[label] for label in union_labels], width, label='Distanza Media')\n",
    "    ax.bar(x + width/2, [proto_dists_series[label] for label in union_labels], width, label='Distanza Prototipo')\n",
    "\n",
    "    ax.set_ylabel('Distanza')\n",
    "    ax.set_title(f\"Top-{k} Strategie - Query: {os.path.basename(query_img_path)}\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(union_labels)\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # ===== Stampa i top-k risultati =====\n",
    "    print(\"\\n Top-k etichette per ciascuna strategia:\\n\")\n",
    "    print(\"Strategia: Distanza Media\")\n",
    "    print(top_k_mean)\n",
    "\n",
    "    print(\"\\n Strategia: Prototipo di Classe\")\n",
    "    print(top_k_proto)\n",
    "\n",
    "    # ===== Confronto top-1 =====\n",
    "    top1_mean_label = top_k_mean.index[0]\n",
    "    top1_mean_value = top_k_mean.iloc[0]\n",
    "    top1_proto_label = top_k_proto.index[0]\n",
    "    top1_proto_value = top_k_proto.iloc[0]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,3))\n",
    "    for lab in union_labels:\n",
    "        ax.plot([0, 1], [mean_dists[lab], proto_dists_series[lab]], marker='o', label=lab)\n",
    "\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Media\", \"Prototipo\"])\n",
    "    ax.set_ylabel(\"Distanza\")\n",
    "    ax.set_title(f\"Slope Chart top-{k}\")\n",
    "    ax.grid(True, axis='y', ls='--', lw=.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05,1), loc='upper left', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.scatter(mean_dists, proto_dists_series)\n",
    "\n",
    "    # evidenzia la query \"top-1\" (opzionale)\n",
    "    ax.scatter(mean_dists[top1_mean_label], proto_dists_series[top1_mean_label],\n",
    "            s=120, edgecolor='k', facecolor='red', zorder=3, label='Top-1 Media')\n",
    "\n",
    "    for label in mean_dists.index:\n",
    "        ax.text(mean_dists[label], proto_dists_series[label], label,\n",
    "                fontsize=7, alpha=.7)\n",
    "\n",
    "    ax.plot([mean_dists.min(), mean_dists.max()],\n",
    "            [proto_dists_series.min(), proto_dists_series.max()],\n",
    "            ls='--', lw=.8, color='gray')\n",
    "    ax.set_xlabel(\"Distanza Media per classe\")\n",
    "    ax.set_ylabel(\"Distanza Prototipo per classe\")\n",
    "    ax.set_title(\"Confronto strategie su tutte le classi\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.grid(True, ls=\"--\", lw=.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"\\n==== Analisi della Strategia Migliore ====\")\n",
    "    if top1_mean_label == top1_proto_label:\n",
    "        print(f\"[OK] Entrambe le strategie concordano sulla classe '{top1_mean_label}'.\")\n",
    "        print(f\" → Distanza media: {top1_mean_value:.4f}, distanza prototipo: {top1_proto_value:.4f}\")\n",
    "    else:\n",
    "        print(f\"[DIFFERENZA] Le strategie danno risultati diversi:\")\n",
    "        print(f\" - Distanza Media: '{top1_mean_label}' con distanza {top1_mean_value:.4f}\")\n",
    "        print(f\" - Prototipo: '{top1_proto_label}' con distanza {top1_proto_value:.4f}\")\n",
    "        if top1_mean_value < top1_proto_value:\n",
    "            print(f\" → [SCELTA SUGGERITA] Preferibile 'Distanza Media' ({top1_mean_label})\")\n",
    "        else:\n",
    "            print(f\" → [SCELTA SUGGERITA] Preferibile 'Prototipo' ({top1_proto_label})\")\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "query_img = \"Part2/brain_glioma/brain_glioma_1142.jpg\"\n",
    "compare_label_ranking_strategies(query_img, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c91b4",
   "metadata": {},
   "source": [
    "Che cosa mostra\n",
    "\n",
    "Sull’asse X ci sono solo due categorie fittizie: Media (distanza media fra la query e tutte le immagini di ciascuna classe) e Prototipo (distanza fra la query e il centroide della classe).\n",
    "\n",
    "Ogni linea rappresenta una classe fra le migliori (k = 2): brain_glioma e brain_tumor.\n",
    "\n",
    "L’altezza sul­l’asse Y è la distanza (più basso ⇒ più vicino alla query).\n",
    "\n",
    "Come leggerlo\n",
    "\n",
    "Entrambe le linee scendono in modo netto: significa che, per queste due classi, la distanza al prototipo è molto più bassa della distanza media.\n",
    "\n",
    "brain_glioma passa da ≈ 560 a ≈ 382.\n",
    "\n",
    "brain_tumor da ≈ 600 a ≈ 393.\n",
    "\n",
    "Visivamente il prototipo “vince” su tutta la linea: la strategia “centroide” identifica rappresentanti più prossimi alla query rispetto alla strategia “media per classe”.\n",
    "\n",
    "Da evidenziare nel testo: “Per le due etichette candidate il prototipo riduce la distanza del ~30 % rispetto alla media, suggerendo che gli esemplari di classe sono distribuiti in modo eterogeneo: il centroide è più rappresentativo di quanto non lo sia la distanza media calcolata su tutte le immagini”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32f764",
   "metadata": {},
   "source": [
    "# Task 5: Estrazione delle Semantiche Latenti\n",
    "\n",
    "Implementa un programma che, dati:\n",
    "- (a) uno dei modelli di feature disponibili,\n",
    "- (b) un valore **k** specificato dall'utente,\n",
    "- (c) una delle tre tecniche di riduzione della dimensionalità (**SVD**, **LDA**, **k-means**) scelta dall'utente,\n",
    "\n",
    "estragga le **prime k semantiche latenti** dallo spazio delle feature selezionato.\n",
    "\n",
    "### Funzionalità richieste\n",
    "\n",
    "- **Memorizzare** le semantiche latenti estratte in un file di output, **adeguatamente nominato**.\n",
    "- **Elencare** per ogni componente latente le **coppie (imageID, peso)** ordinate in ordine decrescente di peso.\n",
    "\n",
    "\n",
    "### Definizione del feature model\n",
    "\n",
    "Il **feature model** è un file contenente la rappresentazione delle feature di tutte le immagini del dataset, salvato come matrice (`feature_matrix`).  \n",
    "La matrice ha dimensioni **n × d**, dove:\n",
    "- **n** = numero di immagini,\n",
    "- **d** = numero di feature.\n",
    "\n",
    "Esempio di feature model: `color_moments.npz`.\n",
    "\n",
    "---\n",
    "\n",
    "## Procedura\n",
    "\n",
    "### Input\n",
    "- Un **feature model** (es. `color_moments.npz`).\n",
    "- Un valore **k** che indica il numero di componenti latenti da estrarre.\n",
    "- Una tecnica di riduzione dimensionale (**SVD**, **LDA**, o **k-means**).\n",
    "\n",
    "### Operazioni\n",
    "1. **Riduzione dimensionale** del feature space selezionato tramite la tecnica scelta.\n",
    "2. **Estrazione delle top-k componenti latenti**.\n",
    "3. Per ogni componente:\n",
    "   - Calcolare i **pesi** associati a ogni immagine.\n",
    "   - Ordinare le coppie **(imageID, peso)** in ordine decrescente di peso.\n",
    "\n",
    "4. **Salvare i risultati** in un file di testo con nome descrittivo, ad esempio:  \n",
    "   `latent_semantics_svd_color_moments_k3.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "## Significato di **k**\n",
    "\n",
    "| Tecnica   | Significato di k                                           |\n",
    "|-----------|------------------------------------------------------------|\n",
    "| **SVD**   | Numero di componenti principali → riduzione mantenendo la variazione globale. |\n",
    "| **LDA**   | Numero di direzioni discriminanti → riduzione focalizzata sulla separazione tra le classi. |\n",
    "| **k-means** | Numero di cluster → suddivisione delle immagini in gruppi simili (senza usare le etichette). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_latent_space_2d(X_transformed, labels, technique, k):\n",
    "    \"\"\"Visualizza la proiezione 2D delle immagini nello spazio latente (solo per SVD/LDA).\"\"\"\n",
    "    print(f\"[DEBUG] Shape X_svd (dati trasformati): {X_transformed.shape}\")\n",
    "    if X_transformed.shape[1] < 2:\n",
    "        print(\"[INFO] Meno di 2 componenti: impossibile visualizzare in 2D.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=labels, palette=\"Set2\", s=80)\n",
    "    plt.title(f\"{technique.upper()} - Proiezione sulle prime 2 componenti latenti (k={k})\")\n",
    "    plt.xlabel(\"Componente 1\")\n",
    "    plt.ylabel(\"Componente 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_kmeans_clusters_2d(feature_matrix, labels, n_clusters):\n",
    "    \"\"\"Visualizza le immagini raggruppate da KMeans su uno spazio 2D ridotto con SVD.\"\"\"\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd.fit_transform(feature_matrix)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_labels, palette='tab10', s=80, style=labels)\n",
    "    plt.title(f\"KMeans Clustering (k={n_clusters}) con proiezione SVD 2D\")\n",
    "    plt.xlabel(\"Componente Latente 1 (da SVD)\")\n",
    "    plt.ylabel(\"Componente Latente 2 (da SVD)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot degli istogrammi dei pesi in una matrice\n",
    "def plottings(k, weights):\n",
    "\n",
    "    plt.figure(figsize=(4*k, 4))  # k subplot orizzontali\n",
    "    for i in range(k):\n",
    "        plt.subplot(1, k, i+1)\n",
    "        sns.histplot(weights[:, i], bins=30, kde=True)\n",
    "        plt.title(f\"Latent Semantic {i+1}\")\n",
    "        plt.xlabel(\"Weight\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot della distribuzione delle classi top-n in una matrice\n",
    "    top_n = 50\n",
    "\n",
    "    # Calcola il numero di righe e colonne per la griglia\n",
    "    if k <= 3:\n",
    "        rows, cols = 1, k\n",
    "    elif k <= 6:\n",
    "        rows, cols = 2, 3\n",
    "    elif k <= 9:\n",
    "        rows, cols = 3, 3\n",
    "    elif k <= 12:\n",
    "        rows, cols = 3, 4\n",
    "    else:\n",
    "        rows = int(np.ceil(np.sqrt(k)))\n",
    "        cols = int(np.ceil(k / rows))\n",
    "\n",
    "    # Crea una figura con subplot in griglia\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "\n",
    "    # Se k=1, axes non è un array, quindi lo rendiamo tale\n",
    "    if k == 1:\n",
    "        axes = [axes]\n",
    "    # Se abbiamo solo una riga o una colonna, flatten l'array\n",
    "    elif rows == 1 or cols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i in range(k):\n",
    "        top_idx = np.argsort(-np.abs(weights[:, i]))[:top_n]\n",
    "        top_classes = [lbls_part1[idx] for idx in top_idx]\n",
    "        class_counts = Counter(top_classes)\n",
    "\n",
    "        # Plot sul subplot corrente\n",
    "        ax = axes[i]\n",
    "        classes = list(class_counts.keys())\n",
    "        counts = list(class_counts.values())\n",
    "\n",
    "        sns.barplot(x=classes, y=counts, ax=ax)\n",
    "        ax.set_title(f\"Top-{top_n} Class Distribution - Semantic {i+1}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Nasconde i subplot vuoti se k < rows*cols\n",
    "    for i in range(k, rows*cols):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def task5_latent_semantics(feature_model_path, technique, k):\n",
    "    \"\"\"\n",
    "    Estrae i top-k concetti latenti da uno spazio di feature usando SVD, LDA o KMeans.\n",
    "    Visualizza lo spazio latente ed esporta un file .txt con i pesi associati alle immagini.\n",
    "    \"\"\"\n",
    "\n",
    "    technique = technique.lower()\n",
    "    method = \"\"\n",
    "    X_transformed = None\n",
    "    components = None\n",
    "\n",
    "    if technique == \"svd\":\n",
    "        model = TruncatedSVD(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1)\n",
    "        components = model.components_\n",
    "        method = \"svd\"\n",
    "\n",
    "    elif technique == \"lda\":\n",
    "        unique_labels = np.unique(lbls_part1)\n",
    "        max_k = len(unique_labels) - 1\n",
    "        if k > max_k:\n",
    "            print(f\"[ATTENZIONE] LDA supporta al massimo {max_k} componenti con {len(unique_labels)} classi.\")\n",
    "            k = max_k\n",
    "        model = LDA(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1, lbls_part1)\n",
    "        components = model.scalings_.T[:k]\n",
    "        method = \"lda\"\n",
    "\n",
    "    elif technique == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        model.fit(feat_matrix_part1)\n",
    "        components = model.cluster_centers_\n",
    "        X_transformed = model.transform(feat_matrix_part1)\n",
    "        method = \"kmeans\"\n",
    "    else:\n",
    "        print(\"[ERRORE] Tecnica non supportata. Usa: 'svd', 'lda', 'kmeans'\")\n",
    "        return\n",
    "\n",
    "    # Visualizzazione\n",
    "    if technique in [\"svd\", \"lda\"]:\n",
    "        plot_latent_space_2d(X_transformed, lbls_part1, technique, k)\n",
    "    elif technique == \"kmeans\":\n",
    "        plot_kmeans_clusters_2d(feat_matrix_part1, lbls_part1, k)\n",
    "\n",
    "# Creazione output\n",
    "    output_dir = os.path.join(\"task5_output\", \"latent_semantics_color_moments\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = os.path.join(output_dir, f\"tutti_i_pesi_{method}_{base_name}_k{k}.txt\")\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        # intestazione opzionale\n",
    "        header = \"ImageID | Class\" + \"\".join([f\" | Peso{i+1}\" for i in range(k)]) + \"\\n\"\n",
    "        f.write(header)\n",
    "        \n",
    "        # scegli la matrice trasformata giusta in base alla tecnica\n",
    "        if technique in [\"svd\", \"lda\"]:\n",
    "            latent_matrix = X_transformed  # shape: (n_immagini, k)\n",
    "        else:  # kmeans: X_transformed è la distanza da ciascun centroide\n",
    "            latent_matrix = -X_transformed  # invertiamo il segno per coerenza\n",
    "\n",
    "        for idx, img in enumerate(flname_part1):\n",
    "            weights = latent_matrix[idx]\n",
    "            line = f\"{img} | {lbls_part1[idx]}\" + \"\".join([f\" | {w:.4f}\" for w in weights]) + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "    weights = feat_matrix_part1 @ components.T  # Calcolo completo dei pesi\n",
    "\n",
    "    plottings(k, weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=min(feat_matrix_part1.shape)\n",
    "\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"svd\", k=10)\n",
    "#task5_latent_semantics(\"color_moments.npz\", technique=\"svd\", k=n_components)\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"lda\", k=2)\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"kmeans\", k=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de147093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "def plot_latent_space_with_images(X_2d, images_paths, title=\"Spazio Latente 2D\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Componente 1\")\n",
    "    ax.set_ylabel(\"Componente 2\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    for i, path in enumerate(images_paths):\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (32, 32))  # thumbnail\n",
    "        im = OffsetImage(img, zoom=0.6)\n",
    "        ab = AnnotationBbox(im, (X_2d[i, 0], X_2d[i, 1]), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b87d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### explained = pca.explained_variance_ratio_\n",
    "es. explained = [0.40, 0.30, 0.20, 0.07, 0.03]\n",
    "\n",
    "Un **array di float** dove ciascun **valore rappresenta la percentuale di varianza spiegata da ciascuna componente principale della PCA**.\n",
    "È ordinato dalla componente più importante a quella meno importante.\n",
    "\n",
    "Significa che **la prima componente spiega il 40% della varianza**, la seconda il 30%, ecc.\n",
    "\n",
    "### cumulative = np.cumsum(explained)\n",
    "es:cumulative = [0.40, 0.70, 0.90, 0.97, 1.00]\n",
    "\n",
    "Calcola la somma cumulativa dell’array explained, cioè la **varianza totale spiegata fino a ciascuna componente**.\n",
    "\n",
    "Significa:\n",
    "- Le prime 2 componenti spiegano il 70%\n",
    "- Le prime 3 il 90%\n",
    "- Le prime 4 il 97%, ecc.\n",
    "\n",
    "### intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "Questa riga:\n",
    "\t- Trova il primo indice in cui la varianza cumulativa raggiunge o supera una soglia (threshold), ad esempio 0.95 (95%).\n",
    "\t- np.argmax(...) restituisce il primo True nella condizione cumulative >= threshold.\n",
    "\t- Si aggiunge +1 perché gli indici Python partono da 0, ma il numero di componenti parte da 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb836e",
   "metadata": {},
   "source": [
    "# 📏 Task 6 – Calcolo della Dimensionalità Intrinseca\n",
    "\n",
    "## Obiettivo del Task\n",
    "\n",
    "In questo task analizziamo **quanto è realmente complesso lo spazio delle feature**.  \n",
    "L'obiettivo è capire **quante dimensioni (componenti)** servono per rappresentare in modo efficace le informazioni contenute nei Color Moments.\n",
    "\n",
    "## Cos'è la dimensionalità intrinseca?\n",
    "\n",
    "Lo spazio dei Color Moments ha **900 dimensioni**, ma non tutte queste dimensioni contribuiscono allo stesso modo.  \n",
    "Molte possono contenere **rumore o informazioni ridondanti**.\n",
    "\n",
    "La **dimensionalità intrinseca** rappresenta il **numero minimo di componenti** necessarie per spiegare una certa percentuale della varianza dei dati (es. il 95%).\n",
    "\n",
    "\n",
    "##  Tecnica utilizzata: PCA (Principal Component Analysis)\n",
    "\n",
    "La PCA riduce lo spazio delle feature individuando le direzioni principali lungo cui i dati variano maggiormente.\n",
    "\n",
    "- Ordina le componenti in base alla **varianza spiegata**.\n",
    "- Permette di stimare quante componenti servono per spiegare una certa percentuale della varianza totale (**90%**, **95%**, **99%**).\n",
    "\n",
    "---\n",
    "### explained = pca.explained_variance_ratio_\n",
    "es. explained = [0.40, 0.30, 0.20, 0.07, 0.03]\n",
    "\n",
    "Un **array di float** dove ciascun **valore rappresenta la percentuale di varianza spiegata da ciascuna componente principale della PCA**.\n",
    "È ordinato dalla componente più importante a quella meno importante.\n",
    "\n",
    "Significa che **la prima componente spiega il 40% della varianza**, la seconda il 30%, ecc.\n",
    "\n",
    "### cumulative = np.cumsum(explained)\n",
    "es:cumulative = [0.40, 0.70, 0.90, 0.97, 1.00]\n",
    "\n",
    "Calcola la somma cumulativa dell’array explained, cioè la **varianza totale spiegata fino a ciascuna componente**.\n",
    "\n",
    "Significa:\n",
    "- Le prime 2 componenti spiegano il 70%\n",
    "- Le prime 3 il 90%\n",
    "- Le prime 4 il 97%, ecc.\n",
    "\n",
    "### intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "Questa riga:\n",
    "\t- Trova il primo indice in cui la varianza cumulativa raggiunge o supera una soglia (threshold), ad esempio 0.95 (95%).\n",
    "\t- np.argmax(...) restituisce il primo True nella condizione cumulative >= threshold.\n",
    "\t- Si aggiunge +1 perché gli indici Python partono da 0, ma il numero di componenti parte da 1.\n",
    "\n",
    "---\n",
    "\n",
    "##  Analisi effettuate\n",
    "\n",
    "### 1️ **Analisi globale su tutto il dataset (Parte 1)**\n",
    "\n",
    "Abbiamo applicato la PCA su **tutte le immagini del dataset di training** (Parte 1), ottenendo i seguenti risultati:\n",
    "\n",
    "| Soglia di varianza spiegata | Componenti necessarie (k) |\n",
    "|-----------------------------|--------------------------|\n",
    "| 90%                         | 66                       |\n",
    "| 95%                         | 102                      |\n",
    "| 99%                         | 167                      |\n",
    "\n",
    "📈 **Interpretazione dei risultati:**\n",
    "- Per spiegare il **95% della varianza** bastano **102 componenti** → poco più del 10% delle dimensioni originali.\n",
    "- Più di **700 dimensioni su 900 sono ridondanti**.\n",
    "\n",
    "### 2️ **Analisi separata per ciascuna classe**\n",
    "\n",
    "Abbiamo ripetuto l’analisi **per ogni singola classe**, per verificare se alcune classi sono statisticamente più semplici o più complesse.\n",
    "\n",
    "| Classe         | Componenti per spiegare il 95% della varianza |\n",
    "|----------------|-----------------------------------------------|\n",
    "| brain_glioma   | 91                                            |\n",
    "| brain_menin    | 94                                            |\n",
    "| brain_tumor    | 93                                            |\n",
    "\n",
    "📈 **Conclusione:**\n",
    "- Le tre classi hanno **complessità interne molto simili**.\n",
    "- Nessuna classe è particolarmente più semplice o più complessa delle altre.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Confronto tra dataset completo e singole classi\n",
    "\n",
    "| Analisi          | Componenti per 95% varianza |\n",
    "|------------------|----------------------------|\n",
    "| Dataset completo | 102                        |\n",
    "| brain_glioma     | 91                         |\n",
    "| brain_menin      | 94                         |\n",
    "| brain_tumor      | 93                         |\n",
    "\n",
    "🧠 **Analisi finale:**\n",
    "- La dimensionalità globale (102) è leggermente più alta di quella delle singole classi (~90).\n",
    "- La variabilità globale deriva **soprattutto dalla variabilità interna delle classi**, e non tanto dalle differenze tra classi diverse.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusioni del Task 6\n",
    "\n",
    "- Il dataset ha una **forte ridondanza**: circa il **90% delle dimensioni non aggiunge informazione significativa**.\n",
    "- Bastano circa **100 componenti** per rappresentare il dataset con il 95% della varianza.\n",
    "- Anche all’interno delle singole classi la dimensionalità necessaria è simile (~90 componenti).\n",
    "- Una riduzione dello spazio a **90-100 dimensioni** è raccomandata per rendere più efficienti i modelli successivi, **senza perdere qualità**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_intrinsic_dimensionality(feature_matrix, threshold, plot=True):\n",
    "    max_components = min(feature_matrix.shape)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(feature_matrix)\n",
    "\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "\n",
    "    #print(f\"[INFO] Spiegazione varianza per ogni componente PCA:\\n{explained}\")\n",
    "    #print(f\"[INFO] Varianza cumulativa:\\n{cumulative}\")\n",
    "    #print(f\"[INFO] Soglia impostata: {threshold}\")\n",
    "    #print(f\"[INFO] Dimensione intrinseca stimata: {intrinsic_dim}\")\n",
    "\n",
    "\n",
    "    if threshold >= 1.0:\n",
    "        intrinsic_dim = len(cumulative)\n",
    "    else:\n",
    "        intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cumulative, marker='o', label=\"Varianza cumulativa\")\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Soglia {threshold*100:.0f}%\")\n",
    "        plt.axvline(x=intrinsic_dim, color='g', linestyle='--', label=f\"k suggerito: {intrinsic_dim}\")\n",
    "        plt.xlabel(\"Numero componenti\")\n",
    "        plt.ylabel(\"Varianza cumulativa\")\n",
    "        plt.title(\"Scelta ottimale di k (PCA/SVD)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"[INFO] k ottimale suggerito (soglia {threshold*100:.0f}%): {intrinsic_dim}\")\n",
    "    return intrinsic_dim, cumulative\n",
    "\n",
    "def suggest_k(feature_matrix, threshold_list=[0.90, 0.95, 0.99]):\n",
    "    print(f\"[INFO] Feature matrix shape: {feature_matrix.shape}\")\n",
    "    k_values = {}\n",
    "    for t in threshold_list:\n",
    "        k, _ = estimate_intrinsic_dimensionality(feature_matrix, threshold=t, plot=False)\n",
    "        k_values[t] = k\n",
    "        print(f\"Soglia {int(t*100)}% : k = {k}\")\n",
    "    return k_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality_per_label(feature_matrix, labels, threshold):\n",
    "    label_dim_map = {}\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"[INFO] Etichette uniche trovate: {len(unique_labels)}\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        label_features = feature_matrix[indices]\n",
    "\n",
    "        if len(indices) < 2:\n",
    "            print(f\"[AVVISO] Label '{label}' ha meno di 2 campioni — ignorata.\")\n",
    "            continue\n",
    "\n",
    "        k, _ = estimate_intrinsic_dimensionality(label_features, threshold=threshold, plot=False)\n",
    "        label_dim_map[label] = k\n",
    "        print(f\" Label '{label}' : k = {k}\")\n",
    "\n",
    "    return label_dim_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f13f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola k per varie soglie\n",
    "print(\"\\Stima automatica di k in base alla varianza spiegata:\\n\")\n",
    "k_suggeriti = suggest_k(feat_matrix_part1)\n",
    "# Plot dettagliato per la soglia 95%\n",
    "estimate_intrinsic_dimensionality(feat_matrix_part1, threshold=1.00, plot=True)\n",
    "\n",
    "\n",
    "print(\"\\n Task 6b – Dimensionalità per etichetta:\\n\")\n",
    "label_dimensionalities = estimate_dimensionality_per_label(feat_matrix_part1, lbls_part1, threshold=1.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d77d5b",
   "metadata": {},
   "source": [
    "# Task 7 – Classificazione Basata su Spazi Latenti Separati per Classe\n",
    "\n",
    "Implementare un programma che:\n",
    "1. Per ogni etichetta unica **l**, calcoli le corrispondenti **k componenti semantiche latenti** (a scelta) associate alle immagini della **Parte 1**.\n",
    "2. Per le immagini della **Parte 2**, preveda le etichette più probabili utilizzando le **distanze/similarità calcolate nello spazio semantico latente specifico per ciascuna etichetta**.\n",
    "\n",
    "Il sistema deve inoltre calcolare e riportare:\n",
    "- **Precisione (Precision)** per ciascuna classe,\n",
    "- **Richiamo (Recall)** per ciascuna classe,\n",
    "- **Punteggio F1 (F1-score)** per ciascuna classe,\n",
    "- **Accuratezza complessiva (Accuracy)**.\n",
    "## Obiettivo del Task\n",
    "\n",
    "In questo task sviluppiamo un sistema di **classificazione** che:\n",
    "- Crea **uno spazio semantico separato per ciascuna classe**, riducendo il rischio che le caratteristiche di una classe influenzino quelle delle altre.\n",
    "- Classifica una query confrontando la sua posizione negli spazi latenti delle singole classi.\n",
    "\n",
    "L’idea di base è che ogni classe abbia **proprietà statistiche distinte**, che emergono più chiaramente se considerate separatamente.\n",
    "\n",
    "---\n",
    "\n",
    "## Concetto chiave\n",
    "\n",
    "Ogni classe ha un **proprio spazio semantico latente**, in cui le immagini della classe sono rappresentate in modo compatto.  \n",
    "Quando un'immagine deve essere classificata:\n",
    "- viene proiettata in ogni spazio latente di classe,\n",
    "- viene calcolata la distanza rispetto al **centroide latente** di ciascuna classe,\n",
    "- l'immagine viene assegnata alla classe il cui centroide è il più vicino.\n",
    "## Metodo proposto\n",
    "\n",
    "### 1 Estrazione delle semantiche latenti per classe\n",
    "\n",
    "Per ogni classe, si eseguono due trasformazioni:\n",
    "\n",
    "#### a. **Standardizzazione dei dati**\n",
    "- Ogni classe viene normalizzata separatamente (**StandardScaler**), per evitare che feature con scale diverse falsino l’analisi.\n",
    "\n",
    "#### b. **Riduzione dimensionale con Truncated SVD**\n",
    "- Si applica una riduzione dimensionale per estrarre i **componenti latenti più significativi** della classe.\n",
    "- Ogni immagine viene così rappresentata da un vettore ridotto (**es. 10 dimensioni**, contro le 900 iniziali).\n",
    "\n",
    "---\n",
    "\n",
    "### 2️ Calcolo del centroide latente per ogni classe\n",
    "\n",
    "Dopo la riduzione dimensionale, per ogni classe si calcola il **centroide**, cioè il vettore medio nello spazio latente.  \n",
    "Questo centroide rappresenta il \"cuore\" statistico della classe.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Fase di classificazione\n",
    "\n",
    "Quando una nuova immagine query deve essere classificata:\n",
    "\n",
    "1. Viene **normalizzata** usando lo scaler della classe corrente.\n",
    "2. Viene **proiettata** nello spazio latente della classe usando il modello SVD della classe.\n",
    "3. Si calcola la **distanza euclidea** tra la query e il centroide della classe.\n",
    "\n",
    "L’immagine viene assegnata alla **classe con il centroide più vicino**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Valutazione dei risultati\n",
    "\n",
    "Abbiamo valutato le prestazioni del sistema calcolando:\n",
    "- Precisione (**Precision**)\n",
    "- Richiamo (**Recall**)\n",
    "- F1-score\n",
    "- Accuratezza complessiva (**Accuracy**)\n",
    "\n",
    "---\n",
    "\n",
    "### **Risultati ottenuti**\n",
    "\n",
    "| Classe         | Precisione | Recall | F1-score |\n",
    "|----------------|------------|--------|----------|\n",
    "| brain_glioma   | 0.55       | 0.41   | 0.47     |\n",
    "| brain_menin    | 0.46       | 0.59   | 0.52     |\n",
    "| brain_tumor    | 0.30       | 0.30   | 0.30     |\n",
    "\n",
    "**Accuratezza complessiva:** 43%\n",
    "\n",
    "---\n",
    "\n",
    "### **Analisi dei risultati**\n",
    "\n",
    "- La classe **brain_glioma** ha ottenuto una precisione relativamente alta (**0.55**), ma il recall è più basso (**0.41**): il sistema riconosce bene le immagini corrette, ma ne perde molte.\n",
    "- La classe **brain_menin** mostra un recall più elevato (**0.59**), ma una precisione inferiore (**0.46**): il sistema trova molte immagini della classe corretta, ma inserisce anche falsi positivi.\n",
    "- La classe **brain_tumor** è la più difficile da riconoscere, con risultati bassi sia in precisione che in recall (**0.30**).\n",
    "\n",
    "---\n",
    "\n",
    "## Punti di forza e limiti del metodo\n",
    "\n",
    "### Punti di forza:\n",
    "- Ogni classe ha **un proprio spazio semantico specializzato**, riducendo il rischio di confondere feature tra classi diverse.\n",
    "- Sistema relativamente semplice e interpretabile.\n",
    "- La classificazione avviene in uno spazio ridotto (**più efficiente** computazionalmente).\n",
    "\n",
    "### Limiti:\n",
    "- La **distanza euclidea** non tiene conto della varianza delle componenti latenti: feature più variabili dovrebbero pesare meno.\n",
    "- L’uso di un numero fisso di componenti (**es. 10**) potrebbe essere troppo basso o troppo alto a seconda della complessità della classe.\n",
    "- Le immagini **ambigue** rischiano di essere classificate in modo casuale.\n",
    "\n",
    "---\n",
    "\n",
    "## Possibili miglioramenti futuri\n",
    "\n",
    "- Sostituire la distanza Euclidea con la **distanza di Mahalanobis**, più robusta alle variazioni statistiche.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusione del Task 7\n",
    "\n",
    "Questo task ha introdotto una pipeline di classificazione basata su **spazi latenti separati per classe**, che rappresenta un’evoluzione rispetto all’utilizzo di un unico spazio globale.\n",
    "\n",
    "Il metodo è semplice e interpretabile, ma richiede miglioramenti per raggiungere performance competitive.  \n",
    "Costituisce però **una solida base** su cui costruire metodi più raffinati.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_semantics_per_class(X, y, k=10):\n",
    "    class_models = {}\n",
    "    class_means = {}\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        X_class = X[y == label]  # Prende solo le istanze della classe corrente\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_class)  # Normalizza i dati della classe\n",
    "\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        latent = svd.fit_transform(X_scaled)  # Riduzione dimensionale con SVD\n",
    "\n",
    "        # Salva modello SVD e scaler per la classe\n",
    "        class_models[label] = {\n",
    "            'svd': svd,\n",
    "            'scaler': scaler,\n",
    "            'latent_vectors': latent\n",
    "        }\n",
    "        # Calcola la media dei vettori latenti della classe\n",
    "        class_means[label] = np.mean(latent, axis=0)\n",
    "    return class_models, class_means\n",
    "\n",
    "def predict_label(X_test, class_models, class_means):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        best_label = None\n",
    "        min_dist = float('inf')\n",
    "        for label, model in class_models.items():\n",
    "            x_scaled = model['scaler'].transform(x.reshape(1, -1))  # Normalizza x\n",
    "            x_latent = model['svd'].transform(x_scaled)  # Trasforma in spazio latente\n",
    "            dist = np.linalg.norm(x_latent - class_means[label])  # Distanza dal centroide\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_label = label\n",
    "        y_pred.append(best_label)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    labels = np.unique(y_true)\n",
    "    print(\"Per-class metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\n",
    "            f\"Class {label}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f}\")\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    print(\"[VALUTAZIONE] Report di classificazione:\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4646513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento sui dati di Part1\n",
    "class_models, class_means = compute_latent_semantics_per_class(\n",
    "    feat_matrix_part1, lbls_part1, k=10)\n",
    "\n",
    "# Predizione su Part2\n",
    "predicted_labels = predict_label(feat_matrix_part2, class_models, class_means)\n",
    "\n",
    "# Valutazione\n",
    "evaluate(lbls_part2, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# Ricostruisci tutti i vettori latenti da Part2 dopo trasformazione per ciascuna classe\n",
    "X_latent = []\n",
    "for i, x in enumerate(feat_matrix_part2):\n",
    "    label = predicted_labels[i]\n",
    "    model = class_models[label]\n",
    "    x_scaled = model['scaler'].transform(x.reshape(1, -1))\n",
    "    x_latent = model['svd'].transform(x_scaled)\n",
    "    X_latent.append(x_latent[0])\n",
    "\n",
    "X_latent = np.array(X_latent)\n",
    "\n",
    "# t-SNE (o PCA se preferisci)\n",
    "X_2d = TSNE(n_components=2, perplexity=30, random_state=0).fit_transform(X_latent)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_2d[:,0], y=X_2d[:,1], hue=lbls_part2, style=predicted_labels, palette=\"Set2\", s=60)\n",
    "plt.title(\"Distribuzione immagini Part2 nello spazio latente per classe\")\n",
    "plt.xlabel(\"t-SNE dim 1\")\n",
    "plt.ylabel(\"t-SNE dim 2\")\n",
    "plt.grid(True); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef126143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(lbls_part2, predicted_labels, labels=np.unique(lbls_part2))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=np.unique(lbls_part2),\n",
    "            yticklabels=np.unique(lbls_part2))\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix – Task 7\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864cc02",
   "metadata": {},
   "source": [
    "# Task 8 – Analisi dei Cluster Latenti con DBSCAN e Visualizzazione con MDS\n",
    "\n",
    "Implementa un programma che, per ciascuna etichetta univoca **l**, calcoli i **c cluster più significativi** associati alle immagini della **Parte 1**, utilizzando l'algoritmo **DBSCAN**.\n",
    "\n",
    "I cluster risultanti devono essere visualizzati in due modalità:\n",
    "- Come **nuvole di punti** colorate in modo diverso, proiettate in uno spazio a **2 dimensioni** tramite **MDS (Multidimensional Scaling)**.\n",
    "- Come **gruppi di miniature di immagini**, dove ogni gruppo rappresenta un cluster distinto.\n",
    "\n",
    "## Obiettivo del Task\n",
    "\n",
    "In questo task esploriamo la **struttura interna dei dati di ciascuna classe**, cercando di identificare eventuali **cluster naturali** (gruppi di immagini statisticamente simili) all’interno delle feature Color Moments.\n",
    "\n",
    "Per farlo combiniamo due tecniche fondamentali:\n",
    "- **PCA:** per ridurre la dimensionalità dei dati ed eliminare ridondanze.\n",
    "- **DBSCAN:** per identificare cluster basati sulla densità dei punti nello spazio delle feature.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodo utilizzato\n",
    "\n",
    "### 1️ **Riduzione dimensionale con PCA**\n",
    "- Prima di applicare il clustering, riduciamo lo spazio delle feature da 900 a **50 componenti**, mantenendo la maggior parte della varianza.\n",
    "- La riduzione aiuta DBSCAN a lavorare meglio, eliminando il rumore di alta dimensionalità.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️ **Clustering con DBSCAN**\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise):\n",
    "- Identifica i cluster come gruppi di punti vicini tra loro.\n",
    "- I punti che non appartengono a nessun cluster vengono etichettati come **rumore (-1)**.\n",
    "\n",
    "#### Parametri principali utilizzati:\n",
    "| Parametro     | Significato                                                | Valore usato |\n",
    "|---------------|-------------------------------------------------------------|--------------|\n",
    "| **eps**       | Raggio di vicinanza per considerare due punti vicini       | 2.0          |\n",
    "| **min_samples** | Minimo numero di punti per formare un cluster              | 3            |\n",
    "\n",
    "---\n",
    "\n",
    "### 3️ **Selezione dei cluster più significativi**\n",
    "Dopo aver identificato tutti i cluster, selezioniamo i **3 cluster più popolosi** per ciascuna classe, escludendo i punti considerati rumore.\n",
    "\n",
    "---\n",
    "\n",
    "### 4️ **Visualizzazione dei risultati**\n",
    "\n",
    "Abbiamo rappresentato i risultati in due modi:\n",
    "- Con **MDS 2D (Multidimensional Scaling):** proietta i punti in uno spazio bidimensionale mantenendo approssimativamente le distanze tra di essi.\n",
    "- Con **griglie di immagini:** mostriamo le miniature delle immagini appartenenti ai cluster principali.\n",
    "\n",
    "---\n",
    "\n",
    "##  Risultati ottenuti per ciascuna classe\n",
    "\n",
    "---\n",
    "\n",
    "###  Classe: brain_glioma\n",
    "\n",
    "- Cluster trovati da DBSCAN: **[-1, 0, 1, 2]**\n",
    "- Top 3 cluster più popolosi: **[1, 2, 0]**\n",
    "\n",
    "| Cluster | Numero di immagini |\n",
    "|--------|---------------------|\n",
    "| 1      | 4                   |\n",
    "| 2      | 4                   |\n",
    "| 0      | 3                   |\n",
    "\n",
    "---\n",
    "\n",
    "###  Classe: brain_menin\n",
    "\n",
    "- Cluster trovati da DBSCAN: **[-1, 0, 1]**\n",
    "- Top cluster: **[1, 0]**\n",
    "\n",
    "| Cluster | Numero di immagini |\n",
    "|--------|---------------------|\n",
    "| 1      | 3                   |\n",
    "| 0      | 3                   |\n",
    "\n",
    "---\n",
    "\n",
    "###  Classe: brain_tumor\n",
    "\n",
    "- Cluster trovati da DBSCAN: **[-1, 0]**\n",
    "- Top cluster: **[0]**\n",
    "\n",
    "| Cluster | Numero di immagini |\n",
    "|--------|---------------------|\n",
    "| 0      | 3                   |\n",
    "\n",
    "---\n",
    "\n",
    "##  Analisi dei risultati\n",
    "\n",
    "- La classe **brain_glioma** mostra una **maggiore varietà interna**, con più cluster distinti → suggerisce che questa classe contiene sottotipi visivamente differenti.\n",
    "- La classe **brain_menin** evidenzia **meno varietà**, con 2 cluster principali.\n",
    "- La classe **brain_tumor** appare più omogenea, con un solo cluster significativo.\n",
    "\n",
    "###  Rumore\n",
    "- In tutte le classi, **DBSCAN ha trovato molti punti etichettati come rumore (-1)**.\n",
    "- Ciò indica che **una parte consistente dei dati non si aggrega facilmente in gruppi compatti**, suggerendo una **complessità statistica elevata** o una bassa densità dei dati.\n",
    "\n",
    "---\n",
    "\n",
    "## Parametri consigliati per DBSCAN (da risultati empirici)\n",
    "\n",
    "| Parametro     | Intervallo consigliato |\n",
    "|---------------|------------------------|\n",
    "| eps           | 1.5 - 2.5              |\n",
    "| min_samples   | 3 - 5                  |\n",
    "| PCA components| 30 - 50                |\n",
    "\n",
    "Valori troppo bassi di **eps** generano troppi piccoli cluster o rumore; valori troppo alti unificano tutto in un unico cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusioni del Task 8\n",
    "\n",
    "- Anche su un dataset relativamente semplice (Color Moments), è possibile trovare **strutture interne complesse**.\n",
    "- La classe **brain_glioma** sembra essere **la più eterogenea**, mentre **brain_tumor** è più uniforme.\n",
    "- I risultati confermano che le feature contengono **variazioni interne alle classi**, e che alcuni sottogruppi statistici possono emergere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b65a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_dbscan_with_pca(features, eps=2.0, min_samples=3, n_components=50):\n",
    "    \"\"\"\n",
    "    Riduce 'features' a 'n_components' dimensioni con PCA, quindi applica DBSCAN\n",
    "    e restituisce l'array di cluster-labels (interi) di lunghezza = numero di righe in 'features'.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] PCA -> Riduzione a {n_components} componenti\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    reduced_scaled = scaler.fit_transform(reduced_features)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(reduced_scaled)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def top_c_clusters(cluster_labels, c):\n",
    "    \"\"\"\n",
    "    cluster_labels: array di interi di lunghezza N.\n",
    "    c: numero di cluster \"più popolosi\" che vogliamo restituire.\n",
    "    Restituisce la lista dei c valori di cluster (escludendo -1) \n",
    "    ordinati in base alla dimensione (numero di occorrenze) decrescente.\n",
    "    Se DBSCAN ha trovato meno di c cluster, restituisce tutti quelli disponibili.\n",
    "    \"\"\"\n",
    "    # Conteggio delle occorrenze per ogni etichetta di cluster\n",
    "    label_counts = Counter(cluster_labels)\n",
    "    # Rimuovo il rumore (-1) se presente\n",
    "    label_counts.pop(-1, None)\n",
    "    \n",
    "    if not label_counts:\n",
    "        print(\"[WARN] DBSCAN non ha trovato alcun cluster valido (solo rumore).\")\n",
    "        return []\n",
    "    \n",
    "    # Estraiamo i c cluster più frequenti\n",
    "    most_common = label_counts.most_common(c)  # es. [(label1, count1), (label2, count2), ...]\n",
    "    top = [int(lbl) for lbl, _ in most_common]\n",
    "    \n",
    "    # Se DBSCAN ha trovato meno di c cluster, most_common contiene già tutti\n",
    "    if len(top) < c:\n",
    "        print(f\"[WARN] DBSCAN ha trovato solo {len(top)} cluster (meno di {c}).\")\n",
    "    return top\n",
    "\n",
    "\n",
    "def plot_mds_clusters(features, cluster_labels, top_clusters, metric='euclidean'):\n",
    "    \"\"\"\n",
    "    features: array (N, d) delle tue feature originali (senza aver fatto PCA).\n",
    "    cluster_labels: array (N,) con i risultati DBSCAN.\n",
    "    top_clusters: lista di interi pari ai cluster \"significativi\" (col più grandi).\n",
    "    metric: la distanza da usare per MDS (default 'euclidean').\n",
    "    Mostra un grafico 2D (scatter) con i punti appartenenti ai top_clusters colorati diversamente,\n",
    "    tutti gli altri (cluster minori o -1) in grigio chiaro.\n",
    "    \"\"\"\n",
    "    # 1) Normalizzo / scalizzo le features originali\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # 2) Calcolo la matrice di distanze (facoltativo) e poi MDS → Y (N×2)\n",
    "    #    Qui usiamo MDS “direttamente” su features_scaled, che di default assume euclidea.\n",
    "    mds = MDS(n_components=2, random_state=42, dissimilarity='euclidean')\n",
    "    Y = mds.fit_transform(features_scaled)\n",
    "\n",
    "    # 3) Plotto i punti\n",
    "    import matplotlib\n",
    "    cmap= matplotlib.colormaps['tab10']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(Y)):\n",
    "        lbl = cluster_labels[i]\n",
    "        if lbl in top_clusters:\n",
    "            color_idx = top_clusters.index(lbl)\n",
    "            plt.scatter(Y[i,0], Y[i,1], color=cmap(color_idx), s=30, edgecolor='k', linewidth=0.2)\n",
    "        else:\n",
    "            # punti rumore o cluster “non top”\n",
    "            plt.scatter(Y[i,0], Y[i,1], color='lightgray', s=8)\n",
    "\n",
    "    plt.title(f\"MDS 2D – Top {len(top_clusters)} cluster\")\n",
    "    plt.xlabel(\"MDS 1\")\n",
    "    plt.ylabel(\"MDS 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_cluster_thumbnails(images, cluster_labels, top_clusters, thumb_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    images: lista (o array) di percorsi file (lunghezza N), \n",
    "            ossia a images[i] corrisponde features[i].\n",
    "    cluster_labels: array (N,) di cluster per ogni immagine.\n",
    "    top_clusters: lista dei c cluster (int) che vogliamo visualizzare.\n",
    "    thumb_size: dimensione (w,h) di ogni miniatura.\n",
    "    Per ogni cluster ∈ top_clusters stampa a video (o fa plt.show) \n",
    "    una griglia di miniature (fino a ~16‐25 alla volta).\n",
    "    \"\"\"\n",
    "    for cluster_id in top_clusters:\n",
    "        # Indici di tutte le immagini che appartengono a questo cluster\n",
    "        idxs = [i for i, cl in enumerate(cluster_labels) if cl == cluster_id]\n",
    "        print(f\"[INFO] Cluster {cluster_id}: {len(idxs)} immagini trovate\")\n",
    "\n",
    "        # Se vogliamo limitare a N miniatura per cluster (tipo 16):\n",
    "        max_display = min(len(idxs), 16)\n",
    "        n = int(np.ceil(np.sqrt(max_display)))  # facciamo una griglia n×n\n",
    "        plt.figure(figsize=(n, n))\n",
    "\n",
    "        for j, i_img in enumerate(idxs[:max_display]):\n",
    "            img = Image.open(images[i_img]).convert('RGB')\n",
    "            img_thumb = img.resize(thumb_size, Image.LANCZOS)\n",
    "\n",
    "\n",
    "            ax = plt.subplot(n, n, j+1)\n",
    "            plt.imshow(img_thumb)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Cluster {cluster_id} – {len(idxs)} immagini (mostrate: {max_display})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ddc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# BLOCCHETTO PER TASK 8 (RIUSO FUNZIONI)\n",
    "# ================================\n",
    "\n",
    "# 3) PARAMETRI (modificabili a piacere)\n",
    "eps = 2.0            # valore DBSCAN di esempio\n",
    "min_samples = 3      # valore DBSCAN di esempio\n",
    "n_components = 50    # quante dimensioni tenere con PCA PRIMA di DBSCAN\n",
    "c = 3                # quanti cluster “significativi” voglio prendere per ciascuna label\n",
    "\n",
    "# 4) Creare una cartella di output (facoltativo)\n",
    "output_base = \"./results_task8\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# 3) Costruisco l’elenco dei full path per tutte le immagini\n",
    "base_folder = \"Part1\"  # o path assoluto \"/Users/.../Parte1\"\n",
    "images_full = [ os.path.join(base_folder, lbl, fname)\n",
    "                for fname, lbl in zip(flname_part1, lbls_part1) ]\n",
    "\n",
    "# 5) SCORRO OGNI LABEL DI Parte1 E APPLICO DBSCAN+PCA\n",
    "unique_labels = np.unique(lbls_part1)  # es. [\"Glioma\",\"Meningioma\",\"Pituitary\"]\n",
    "\n",
    "for lbl in unique_labels:\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"[INFO] Elaboro label: {lbl}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # 5.1) Estraggo le righe di feat_matrix_part1 / flname_part1 corrispondenti\n",
    "    mask_lbl = (lbls_part1 == lbl)\n",
    "    features_label = feat_matrix_part1[mask_lbl]   # shape = (n_i, d)\n",
    "    images_label   = np.array(images_full)[mask_lbl]\n",
    "\n",
    "    # 5.2) Chiamo la tua funzione che fa PCA + DBSCAN\n",
    "    cluster_labels = apply_dbscan_with_pca(\n",
    "        features_label,\n",
    "        eps=eps,\n",
    "        min_samples=min_samples,\n",
    "        n_components=n_components\n",
    "    )\n",
    "    print(f\"[INFO] Cluster-labels trovati: {np.unique(cluster_labels)}\")\n",
    "\n",
    "    # 5.3) Trovo i c cluster più grandi\n",
    "    top_clusters = top_c_clusters(cluster_labels, c)\n",
    "    print(f\"[INFO] Top {c} cluster (per dimensione): {top_clusters}\")\n",
    "\n",
    "    # 5.4) Creo sotto-cartella di output per questa label\n",
    "    out_dir_lbl = os.path.join(output_base, f\"label_{lbl}\")\n",
    "    os.makedirs(out_dir_lbl, exist_ok=True)\n",
    "\n",
    "    # 5.5) MDS‐2D + scatter plot del clustering\n",
    "    print(f\"[INFO] Disegno MDS 2D per i cluster di '{lbl}' …\")\n",
    "    # (ATTENZIONE: plot_mds_clusters in genere “fa plt.show()” a video.\n",
    "    #  Se vuoi salvare l’immagine invece di far vedere a notebook, \n",
    "    #  devi modificare leggermente quella funzione per usare plt.savefig())\n",
    "    plot_mds_clusters(\n",
    "        features_label,\n",
    "        cluster_labels,\n",
    "        top_clusters,\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    # Se invece vuoi **salvare** l’immagine in PNG anziché fare “show()”:\n",
    "    #    plt.savefig(os.path.join(out_dir_lbl, f\"{lbl}_MDS_clusters.png\"))\n",
    "    #    plt.close()\n",
    "\n",
    "    # 5.6) Creo le miniature di ogni cluster “significativo”\n",
    "    print(f\"[INFO] Genero miniature per ciascun cluster di '{lbl}' …\")\n",
    "    show_cluster_thumbnails(\n",
    "        images_label,      # array di stringhe di percorsi\n",
    "        cluster_labels,    # array di int di lunghezza n_i\n",
    "        top_clusters,      # la lista dei c indici di cluster\n",
    "        thumb_size=(64, 64)\n",
    "    )\n",
    "    # Anche qui, di default quella funzione fa plt.show() per ogni cluster.\n",
    "    # Se vuoi salvare le figure in file, modifica show_cluster_thumbnails in\n",
    "    # modo che setti un outpath e faccia plt.savefig().\n",
    "\n",
    "print(\"\\n[FINITO] Task 8 completato per tutte le label di Parte1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667ee99",
   "metadata": {},
   "source": [
    "# Task 9 - Classificazione Supervisionata con k-NN e Decision Tree\n",
    "\n",
    "Implementa un programma che, date le immagini della **Parte 1**:\n",
    "\n",
    "- Crei un classificatore **m-NN** (con **m** specificato dall'utente).\n",
    "- Crei un classificatore basato su **albero decisionale**.\n",
    "\n",
    "Per questo task puoi utilizzare **lo spazio delle feature a tua scelta**.\n",
    "\n",
    "Il programma deve poi:\n",
    "- Applicare il classificatore selezionato dall'utente per prevedere le etichette più probabili delle immagini della **Parte 2**.\n",
    "- Calcolare e visualizzare:\n",
    "  - **Precisione (Precision)** per etichetta,\n",
    "  - **Richiamo (Recall)** per etichetta,\n",
    "  - **Punteggio F1 (F1-score)** per etichetta,\n",
    "  - **Accuratezza complessiva (Accuracy)** del classificatore.\n",
    "\n",
    "## Obiettivo del Task\n",
    "\n",
    "In questo task testiamo due approcci di **classificazione supervisionata**, usando come feature i **Color Moments** estratti in precedenza.  \n",
    "L’obiettivo è **prevedere la classe** di ciascuna immagine del dataset **Parte 2**, dopo aver addestrato i classificatori sulla **Parte 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## Classificatori utilizzati\n",
    "\n",
    "### 1️ **k-NN (k Nearest Neighbors)**\n",
    "\n",
    "- Ogni immagine viene classificata in base alla **classe più rappresentata tra i suoi k vicini più vicini** nello spazio delle feature.\n",
    "- Semplice ed efficace per spazi vettoriali ridotti.\n",
    "\n",
    "Parametri usati:\n",
    "- **k = 5**, valore bilanciato che riduce il rischio di overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️ **Decision Tree**\n",
    "\n",
    "- Crea un **albero decisionale** che suddivide lo spazio delle feature in base a soglie ottimali per separare le classi.\n",
    "- Modello interpretabile ma potenzialmente sensibile al rumore.\n",
    "\n",
    "Parametri usati:\n",
    "- Parametri di default → l’albero viene costruito senza limitazioni particolari sulla profondità.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodo\n",
    "\n",
    "1. **Training:**\n",
    "   - I classificatori vengono addestrati sulle feature del dataset **Parte 1**, per ciascuna immagine e classe nota.\n",
    "2. **Testing:**\n",
    "   - I modelli vengono testati sulle feature delle immagini della **Parte 2**, simulando un vero scenario di classificazione.\n",
    "\n",
    "---\n",
    "\n",
    "## **Risultati – k-NN (k = 5)**\n",
    "\n",
    "| Classe         | Precisione | Recall | F1-score |\n",
    "|----------------|------------|--------|----------|\n",
    "| brain_glioma   | 0.78       | 0.85   | 0.81     |\n",
    "| brain_menin    | 0.84       | 0.63   | 0.72     |\n",
    "| brain_tumor    | 0.74       | 0.85   | 0.79     |\n",
    "\n",
    "**Accuratezza complessiva:** **77.7%**\n",
    "\n",
    "### Analisi:\n",
    "- La classe **brain_glioma** viene riconosciuta bene (**85% recall**).\n",
    "- La classe **brain_menin** è più difficile da riconoscere (**63% recall**), anche se la precisione è alta.\n",
    "- La classe **brain_tumor** mostra un buon bilanciamento tra precisione e recall.\n",
    "\n",
    "Nel complesso, il k-NN ha ottenuto **buoni risultati**, grazie alla semplicità ed efficacia in spazi di feature ridotti.\n",
    "\n",
    "---\n",
    "\n",
    "## **Risultati – Decision Tree**\n",
    "\n",
    "| Classe         | Precisione | Recall | F1-score |\n",
    "|----------------|------------|--------|----------|\n",
    "| brain_glioma   | 0.75       | 0.76   | 0.75     |\n",
    "| brain_menin    | 0.64       | 0.62   | 0.63     |\n",
    "| brain_tumor    | 0.71       | 0.72   | 0.71     |\n",
    "\n",
    "**Accuratezza complessiva:** **70%**\n",
    "\n",
    "### Analisi:\n",
    "- Le prestazioni sono inferiori rispetto al k-NN su tutte le classi.\n",
    "- Il Decision Tree ottiene un comportamento **equilibrato**, ma globalmente meno performante.\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto tra i due modelli\n",
    "\n",
    "| Modello        | Accuracy | Precisione media | Recall medio | F1-score medio | Classe più riconosciuta | Classe più difficile |\n",
    "|----------------|----------|------------------|--------------|----------------|------------------------|----------------------|\n",
    "| **k-NN (k=5)** | 77.7%    | 78%              | 78%          | 77%            | brain_glioma (85% recall) | brain_menin (63% recall) |\n",
    "| **Decision Tree** | 70.0%    | 70%              | 70%          | 70%            | brain_glioma (76% recall) | brain_menin (62% recall) |\n",
    "\n",
    "**Conclusioni del confronto:**\n",
    "- Il **k-NN** è il classificatore migliore in questo scenario, probabilmente grazie alla sua capacità di sfruttare la **distribuzione locale** delle feature.\n",
    "- Il **Decision Tree**, pur essendo più interpretabile, sembra soffrire di più la **complessità e il rumore** nei dati.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusioni del Task 9\n",
    "\n",
    "- Con algoritmi **semplici**, come k-NN e Decision Tree, si ottengono già **buoni risultati di base**.\n",
    "- La classe **brain_menin** rimane la più difficile da riconoscere, suggerendo che le sue feature non sono ben distinte dalle altre classi.\n",
    "- Il dataset, nonostante la riduzione dimensionale e la pulizia dei dati, richiede probabilmente **modelli più sofisticati** per raggiungere performance elevate (es. Random Forest, SVM, o modelli deep learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta il valore di m per l'm-NN\n",
    "m = 5  # Modifica questo valore in base alle tue necessità\n",
    "\n",
    "# Addestramento m-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=m)\n",
    "knn_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Addestramento Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Predizioni su Part2\n",
    "pred_knn = knn_model.predict(feat_matrix_part2)\n",
    "pred_dt = dt_model.predict(feat_matrix_part2)\n",
    "\n",
    "# Valutazione m-NN\n",
    "print(\"Risultati m-NN:\")\n",
    "print(classification_report(lbls_part2, pred_knn))\n",
    "print(\"Accuratezza complessiva m-NN:\", accuracy_score(lbls_part2, pred_knn))\n",
    "\n",
    "# Valutazione Decision Tree\n",
    "print(\"Risultati Decision Tree:\")\n",
    "print(classification_report(lbls_part2, pred_dt))\n",
    "print(\"Accuratezza complessiva Decision Tree:\", accuracy_score(lbls_part2, pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_knn = confusion_matrix(lbls_part2, pred_knn, labels=labels)\n",
    "cm_dt  = confusion_matrix(lbls_part2, pred_dt, labels=labels)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', ax=axs[0], cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "axs[0].set_title(f\"Confusion Matrix – m-NN (m={m})\")\n",
    "\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', ax=axs[1], cmap='Oranges',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "axs[1].set_title(\"Confusion Matrix – Decision Tree\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae27b9",
   "metadata": {},
   "source": [
    "# Task 10: Locality Sensitive Hashing e Ricerca di Immagini Simili\n",
    "\n",
    "## 10a: Creazione dell'indice LSH\n",
    "\n",
    "Implementa uno strumento di **Locality Sensitive Hashing (LSH)** (per la distanza euclidea), che prende come input:\n",
    "- **L**: Numero di livelli,\n",
    "- **h**: Numero di hash per livello,\n",
    "- Un insieme di vettori (feature delle immagini).\n",
    "\n",
    "Il programma deve creare **una struttura di indice in memoria** contenente l'insieme di vettori dato.\n",
    "\n",
    "> Riferimento:  \n",
    "> \"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\",  \n",
    "> Alexandr Andoni e Piotr Indyk, *Communications of the ACM*, vol. 51, no. 1, 2008, pp. 117–122.\n",
    "\n",
    "---\n",
    "\n",
    "## 10b: Ricerca di immagini simili con LSH\n",
    "\n",
    "Implementa un algoritmo di ricerca di immagini simili utilizzando la struttura LSH creata in 11a, memorizzando le immagini della **Parte 1** e un modello visivo di tua scelta (**il modello visivo combinato deve avere almeno 256 dimensioni**).\n",
    "\n",
    "Per una data immagine di query e un numero intero **t**, il programma deve:\n",
    "\n",
    "- Visualizzare le **t immagini più simili**.\n",
    "- Mostrare:\n",
    "  - Il **numero di immagini uniche** considerate,\n",
    "  - Il **numero totale di immagini analizzate** durante il processo.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd340fa8",
   "metadata": {},
   "source": [
    "## Obiettivo del Task\n",
    "\n",
    "Fino ad ora abbiamo calcolato la similarità tra immagini confrontando la query con tutte le immagini del dataset.  \n",
    "Questo approccio è efficace, ma **computazionalmente molto costoso**, soprattutto per dataset di grandi dimensioni.\n",
    "\n",
    "In questo task introduciamo il **Locality Sensitive Hashing (LSH)**, una tecnica che consente di **ridurre il numero di confronti**, accelerando la ricerca delle immagini simili.\n",
    "\n",
    "---\n",
    "\n",
    "## Cos'è il Locality Sensitive Hashing (LSH)?\n",
    "\n",
    "LSH è una tecnica che:\n",
    "- Indicizza i vettori in modo che quelli **simili abbiano alta probabilità di finire nello stesso bucket**;\n",
    "- I vettori distanti finiscono in bucket diversi, evitando confronti inutili.\n",
    "\n",
    "### Vantaggi principali\n",
    "- Ricerca **approssimata**, ma molto veloce.\n",
    "- Buon compromesso tra **accuratezza e tempo di calcolo**, soprattutto su dataset grandi.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodo adottato\n",
    "\n",
    "### 1 Creazione dell’indice LSH\n",
    "\n",
    "Abbiamo implementato un indice chiamato **LSH_EuclideanQuantized**, progettato per la **distanza Euclidea**.  \n",
    "Ogni vettore viene:\n",
    "- **centrato**, sottraendo la media globale,\n",
    "- **normalizzato** a norma unitaria (**L2 norm**).\n",
    "\n",
    "| Parametro | Significato                                                | Valore utilizzato |\n",
    "|-----------|------------------------------------------------------------|--------------------|\n",
    "| **D**     | Dimensione dei vettori (Color Moments)                      | 900                |\n",
    "| **L**     | Numero di livelli hash (più livelli = maggiore affidabilità)| 5                  |\n",
    "| **h**     | Numero di funzioni hash per livello                         | 5                  |\n",
    "| **r**     | Larghezza dei bucket hash (controlla la granularità)        | 5.0                |\n",
    "\n",
    "---\n",
    "\n",
    "### 2️ Ricerca della query\n",
    "\n",
    "Per cercare immagini simili a una query:\n",
    "- Si estrae il vettore dei Color Moments,\n",
    "- Si centra e normalizza con gli stessi parametri del training,\n",
    "- Si cercano i bucket hash più rilevanti (in tutti i livelli hash),\n",
    "- Si calcolano le **distanze Euclidee reali** solo con i candidati recuperati dai bucket.\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio completo di risultati ottenuti\n",
    "\n",
    "Abbiamo cercato le **5 immagini più simili** a:\n",
    "I risultati ottenuti sono i seguenti:\n",
    "\n",
    "| #  | Nome File              | Classe        | Distanza Euclidea |\n",
    "|----|------------------------|---------------|-------------------|\n",
    "| 1  | brain_glioma_0596.jpg  | brain_glioma  | 0.66              |\n",
    "| 2  | brain_menin_0554.jpg   | brain_menin   | 0.87              |\n",
    "| 3  | brain_tumor_0622.jpg   | brain_tumor   | 0.89              |\n",
    "| 4  | brain_menin_0585.jpg   | brain_menin   | 0.93              |\n",
    "| 5  | brain_menin_0520.jpg   | brain_menin   | 0.93              |\n",
    "\n",
    "---\n",
    "\n",
    "### Analisi dei risultati\n",
    "\n",
    "- I primi risultati includono correttamente immagini della classe **brain_menin**, anche se il primo risultato è un **brain_glioma** → questo riflette le **limitazioni dei Color Moments**, che non separano perfettamente le classi.\n",
    "- La **distanza Euclidea** tra query e immagini trovate è abbastanza bassa (<1), segno che i risultati sono visivamente simili.\n",
    "- L’LSH ha evitato di confrontare l’intero dataset:\n",
    "  - Immagini **uniche** considerate: **2442**\n",
    "  - Immagini **totali** processate (su tutti i livelli hash): **4055**\n",
    "  \n",
    "Senza LSH avremmo dovuto confrontare circa **3000 immagini**, quindi l’efficienza è migliorata notevolmente.\n",
    "\n",
    "---\n",
    "\n",
    "## Parametri suggeriti per LSH (dai test)\n",
    "\n",
    "| Parametro | Range consigliato | Effetti |\n",
    "|-----------|--------------------|---------|\n",
    "| **L**     | 5 - 8              | Aumenta l'affidabilità, ma anche il costo computazionale |\n",
    "| **h**     | 4 - 8              | Più hash function → bucket più precisi |\n",
    "| **r**     | 3.0 - 7.0          | Bucket larghi → risultati più approssimati; bucket stretti → meno candidati trovati |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusioni del Task 10\n",
    "\n",
    "|  Pro                                                 |  Contro                                                |\n",
    "|--------------------------------------------------------|----------------------------------------------------------|\n",
    "| Ricerca molto più veloce rispetto al confronto esaustivo | I risultati sono approssimati, non sempre perfetti       |\n",
    "| Riduzione drastica dei confronti                        | Richiede tuning fine dei parametri (L, h, r)            |\n",
    "| Utile su dataset grandi e ad alta dimensionalità        | Potrebbe non funzionare bene con dati molto disomogenei |\n",
    "\n",
    "---\n",
    "\n",
    "### Riflessioni finali\n",
    "\n",
    "- LSH dimostra di essere una **soluzione efficace** per accelerare il retrieval su dataset numerosi.\n",
    "- Non sostituisce il confronto esaustivo in termini di precisione assoluta, ma ne riduce enormemente i costi computazionali.\n",
    "- In ambito medico può essere utilizzato per ottenere **risultati preliminari rapidi**, eventualmente raffinati successivamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b424686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH_EuclideanQuantized:\n",
    "    \"\"\"\n",
    "    LSH per distanza Euclidea con quantizzazione (p-stable).\n",
    "    num_layers = L, num_hashes = h, dim = D, r = bucket width.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_hashes: int, dim: int, r: float):\n",
    "        self.L = num_layers\n",
    "        self.h = num_hashes\n",
    "        self.d = dim\n",
    "        self.r = r\n",
    "\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.L)]\n",
    "        self.a_vectors = [\n",
    "            [np.random.randn(self.d) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.b_offsets = [\n",
    "            [np.random.uniform(0, self.r) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.data_vectors = None\n",
    "\n",
    "    def _compute_hash_tuple(self, vec: np.ndarray, layer_idx: int) -> tuple:\n",
    "        bits = []\n",
    "        for j in range(self.h):\n",
    "            a_j = self.a_vectors[layer_idx][j]\n",
    "            b_j = self.b_offsets[layer_idx][j]\n",
    "            proj = float(np.dot(a_j, vec) + b_j)\n",
    "            h_val = int(np.floor(proj / self.r))\n",
    "            bits.append(h_val)\n",
    "        return tuple(bits)\n",
    "\n",
    "    def index(self, vectors: np.ndarray):\n",
    "        self.data_vectors = vectors\n",
    "        N, D = vectors.shape\n",
    "        assert D == self.d, f\"Dimensione vettore ({D}) ≠ D di LSH ({self.d}).\"\n",
    "        for idx in range(N):\n",
    "            v = vectors[idx]\n",
    "            for l in range(self.L):\n",
    "                key = self._compute_hash_tuple(v, l)\n",
    "                self.hash_tables[l][key].append(idx)\n",
    "\n",
    "    def query(self, q_vec: np.ndarray, top_t: int = 5):\n",
    "        assert q_vec.shape[0] == self.d, \"Dimensione query ≠ D.\"\n",
    "        candidati = set()\n",
    "        total_checked = 0\n",
    "        for l in range(self.L):\n",
    "            h_tuple = self._compute_hash_tuple(q_vec, l)\n",
    "            bucket = self.hash_tables[l].get(h_tuple, [])\n",
    "            total_checked += len(bucket)\n",
    "            candidati.update(bucket)\n",
    "\n",
    "        risultati = []\n",
    "        for idx in candidati:\n",
    "            v_i = self.data_vectors[idx]\n",
    "            dist = np.linalg.norm(v_i - q_vec)\n",
    "            risultati.append((idx, dist))\n",
    "        risultati.sort(key=lambda x: x[1])\n",
    "        top_results = risultati[:top_t]\n",
    "        return top_results, len(candidati), total_checked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Center + L2 normalize su Part1\n",
    "mean_vec = np.mean(feat_matrix_part1, axis=0)\n",
    "feat_centered = feat_matrix_part1 - mean_vec\n",
    "feat_normed = normalize(feat_centered, norm='l2', axis=1)\n",
    "\n",
    "# 2) Parametri LSH con quantizzazione\n",
    "D = feat_normed.shape[1]      # ad esempio 900\n",
    "L = 5                         # numero di layer (esempio)\n",
    "h = 5                        # numero di hash per layer (esempio)\n",
    "r = 5.0                       # bucket width, da sperimentare\n",
    "\n",
    "# 3) Creo l'indice\n",
    "lsh_quant = LSH_EuclideanQuantized(num_layers=L, num_hashes=h, dim=D, r=r)\n",
    "lsh_quant.index(feat_normed)\n",
    "\n",
    "print(f\"[INFO] Indice LSH-Quant creato. D={D}, L={L}, h={h}, r={r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_lsh_quant(base_folder: str, img_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a img_path (di Part2) usando lsh_quant costruito su Part1.\n",
    "    \"\"\"\n",
    "    # 1) Estrai feature raw (900-dim)\n",
    "    raw_q = np.array(extract_color_moments(img_path), dtype=np.float32)\n",
    "\n",
    "    # 2) Center + normalize (stesso mean_vec usato su Part1)\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # 3) Chiamata LSH\n",
    "    top_results, unique_count, total_checked = lsh_quant.query(q_normed, top_t=k)\n",
    "\n",
    "    # 4) Stampo i risultati testuali\n",
    "    print(f\"\\n[LSH-Quant] Top {k} simili a: {img_path}\")\n",
    "    for rank, (idx, dist) in enumerate(top_results, start=1):\n",
    "        label = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        print(f\"  {rank}. {fname} | Classe: {label} | Distanza Euclidea: {dist:.2f}\")\n",
    "    print(f\"[LSH-Quant] Immagini uniche considerate: {unique_count}\")\n",
    "    print(f\"[LSH-Quant] Immagini totali controllate: {total_checked}\")\n",
    "\n",
    "    # 5) Visualizzazione (query + k risultati)\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(4*(k+1), 4))\n",
    "    img_q = cv2.imread(img_path)\n",
    "    img_q = cv2.cvtColor(img_q, cv2.COLOR_BGR2RGB)\n",
    "    axs[0].imshow(img_q)\n",
    "    axs[0].set_title(\"Query (LSH-Quant)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, (idx, dist) in enumerate(top_results, start=1):\n",
    "        lab = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        full_path = os.path.join(base_folder, lab, fname)\n",
    "        img_match = cv2.imread(full_path)\n",
    "        img_match = cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img_match)\n",
    "        axs[i].set_title(f\"Rank {i}\\nd={dist:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ce3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo su un'immagine di Part2 ---\n",
    "query_path = \"Part2/brain_menin/brain_menin_1003.jpg\"\n",
    "\n",
    "print(query_path);\n",
    "k = 5                         # numero di immagini simili da visualizzare\n",
    "\n",
    "# Eseguo la ricerca LSH\n",
    "find_k_similar_lsh_quant(\"Part1\", query_path, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def compare_lsh_vs_brute(query_path, t=5):\n",
    "    # Feature query\n",
    "    raw_q = np.array(extract_color_moments(query_path), dtype=np.float32)\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # --- LSH ---\n",
    "    t0 = time.time()\n",
    "    lsh_results, uniq_lsh, total_lsh = lsh_quant.query(q_normed, top_t=t)\n",
    "    t_lsh = time.time() - t0\n",
    "\n",
    "    # --- Brute force ---\n",
    "    t1 = time.time()\n",
    "    dists = np.linalg.norm(feat_normed - q_normed, axis=1)\n",
    "    brute_top = np.argsort(dists)[:t]\n",
    "    t_brute = time.time() - t1\n",
    "\n",
    "    mean_lsh = np.mean([d for _, d in lsh_results])\n",
    "    mean_brute = np.mean(dists[brute_top])\n",
    "\n",
    "    print(f\"\\nTempo LSH: {t_lsh:.4f}s | Tempo brute: {t_brute:.4f}s\")\n",
    "    print(f\"Distanza media LSH: {mean_lsh:.2f} | brute: {mean_brute:.2f}\")\n",
    "    print(f\"Immagini controllate (LSH): {total_lsh} | brute: {feat_normed.shape[0]}\")\n",
    "\n",
    "    # Barplot confronto\n",
    "    metrics = ['Immagini controllate']\n",
    "    values_lsh = [t_lsh, mean_lsh, total_lsh]\n",
    "    values_bf = [t_brute, mean_brute, feat_normed.shape[0]]\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, values_lsh, width, label='LSH')\n",
    "    plt.bar(x + width/2, values_bf, width, label='Brute Force')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.title(\"Confronto LSH vs Ricerca esaustiva\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lsh_vs_brute(\"C:/Users/martj/OneDrive/Desktop/DBmulti/Part1/brain_menin/brain_menin_0001.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
