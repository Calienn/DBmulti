{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "get_ipython().system('{sys.executable} -m pip install --quiet numpy pandas matplotlib seaborn scikit-learn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b8fc2",
   "metadata": {},
   "source": [
    "Color Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eaae8",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58732e0",
   "metadata": {},
   "source": [
    "Crop automatico per isolare il cervello - Task1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00999225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_brain(img):\n",
    "    \"\"\"Ritaglia l'area informativa (cervello) da un'immagine.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    coords = cv2.findNonZero(thresh)\n",
    "    if coords is not None:\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        return img[y:y+h, x:x+w]\n",
    "    return img  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    return data['features'], data['labels'], data.get('filenames', [f\"img_{i}\" for i in range(len(data['features']))])\n",
    "\n",
    "\n",
    "feat_matrix_part1, lbls_part1, flname_part1 = load_features(\n",
    "    \"color_moments_part1.npz\")\n",
    "feat_matrix_part2, lbls_part2, flname_part2 = load_features(\n",
    "    \"color_moments_part2.npz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80adff1e",
   "metadata": {},
   "source": [
    "### Task 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6714dba",
   "metadata": {},
   "source": [
    "Estrazione Color Moments su griglia - Task 1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8196bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_moments(img_path):\n",
    "    \"\"\"Estrae Color Moments su una griglia 10x10 da un'immagine.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERRORE] Immagine non trovata: {img_path}\")\n",
    "        return None\n",
    "\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    img = crop_to_brain(img)\n",
    "    img = cv2.resize(img, (300, 100))  # Griglia uniforme\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    grid_h, grid_w = h // 10, w // 10\n",
    "    features = []\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            cell = img[i*grid_h:(i+1)*grid_h, j*grid_w:(j+1)*grid_w]\n",
    "            for channel in range(3):\n",
    "                pixels = cell[:, :, channel].flatten()\n",
    "                if np.std(pixels) > 0:\n",
    "                    mean = np.mean(pixels)\n",
    "                    std = np.std(pixels)\n",
    "                    sk = skew(pixels)\n",
    "                    if np.isnan(sk): sk = 0\n",
    "                else:\n",
    "                    mean, std, sk = 0, 0, 0\n",
    "                features.extend([mean, std, sk])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0122b",
   "metadata": {},
   "source": [
    " Estrazione feature da più cartelle e salvataggio in .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8957034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_features(base_folder, subfolders, output_file):\n",
    "    \"\"\"Estrae le feature da immagini organizzate in sottocartelle e le salva in un file .npz.\"\"\"\n",
    "    all_features, all_filenames, all_labels = [], [], []\n",
    "\n",
    "    for label in subfolders:\n",
    "        folder_path = os.path.join(base_folder, label)\n",
    "        print(f\"[INFO] Elaboro cartella: {label}\")\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                features = extract_color_moments(img_path)\n",
    "                if features is not None:\n",
    "                    all_features.append(features)\n",
    "                    all_filenames.append(filename)\n",
    "                    all_labels.append(label)\n",
    "\n",
    "    np.savez(output_file,\n",
    "             features=np.array(all_features),\n",
    "             filenames=np.array(all_filenames),\n",
    "             labels=np.array(all_labels))\n",
    "    print(f\"[SALVATO] Features salvate in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf78dfcc",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd46fed",
   "metadata": {},
   "source": [
    "Ricerca immagini simili (Euclidea) - Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0add08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar(base_folder, img_path, k):\n",
    "    \"\"\"Trova le k immagini più simili in base alla distanza euclidea.\"\"\"\n",
    "    query_feature = extract_color_moments(img_path)\n",
    "    if query_feature is None:\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "    distances = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "    top_k_idx = np.argsort(distances)[:k]\n",
    "\n",
    "    print(f\"\\nTop {k} immagini simili a: {img_path}\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        print(f\"{rank+1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Distanza: {distances[idx]:.2f}\")\n",
    "\n",
    "    # Visualizzazione\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(15, 5))\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, idx in enumerate(top_k_idx):\n",
    "        img_match = cv2.imread(os.path.join(base_folder, lbls_part1[idx], flname_part1[idx]))\n",
    "        axs[i+1].imshow(cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB))\n",
    "        axs[i+1].set_title(f\"Rank {i+1}\\nD={distances[idx]:.2f}\")\n",
    "        axs[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab512b0",
   "metadata": {},
   "source": [
    "Ricerca immagini simili (Mahalanobis) - Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fd45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_mahalanobis(base_folder, img_path, k):\n",
    "    \"\"\"Trova le k immagini più simili usando distanza di Mahalanobis, escludendo la query.\"\"\"\n",
    "    # Estrai le feature dalla query\n",
    "    query_feature = extract_color_moments(img_path)\n",
    "    if query_feature is None:\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature)\n",
    "\n",
    "    # Calcola matrice di covarianza delle feature\n",
    "    cov = np.cov(feat_matrix_part1.T)\n",
    "\n",
    "    # Inversione con fallback alla pseudoinversa\n",
    "    try:\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"[ERRORE] Uso pseudoinversa per matrice non invertibile.\")\n",
    "        cov_inv = np.linalg.pinv(cov)\n",
    "\n",
    "    # Calcola distanza di Mahalanobis tra la query e tutte le immagini\n",
    "    distances = np.array([\n",
    "        mahalanobis(query_feature, f, cov_inv) for f in feat_matrix_part1\n",
    "    ])\n",
    "\n",
    "    # Opzionale: escludi la query stessa (distanza 0)\n",
    "    # === Escludi la query basandoti sul path ===\n",
    "    query_filename = os.path.basename(img_path)\n",
    "    query_label = os.path.basename(os.path.dirname(img_path))\n",
    "\n",
    "    for i in range(len(flname_part1)):\n",
    "        if flname_part1[i] == query_filename and lbls_part1[i] == query_label:\n",
    "            distances[i] = np.inf\n",
    "            break\n",
    "\n",
    "    # Seleziona i top-k indici a distanza minima\n",
    "    top_k_idx = np.argsort(distances)[:k]\n",
    "    top_k_scores = distances[top_k_idx]\n",
    "\n",
    "    # Output testuale\n",
    "    print(f\"\\nTop {k} immagini simili (Mahalanobis): {img_path}\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        print(\n",
    "            f\"{rank+1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Distanza: {top_k_scores[rank]:.2f}\")\n",
    "\n",
    "    # Visualizza le immagini\n",
    "    fig, axs = plt.subplots(1, k + 1, figsize=(15, 5))\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, idx in enumerate(top_k_idx):\n",
    "        img_match_path = os.path.join(\n",
    "            base_folder, lbls_part1[idx], flname_part1[idx])\n",
    "        img_match = cv2.imread(img_match_path)\n",
    "        axs[i + 1].imshow(cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB))\n",
    "        axs[i + 1].set_title(f\"Rank {i + 1}\\nD={top_k_scores[i]:.2f}\")\n",
    "        axs[i + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a2b6e",
   "metadata": {},
   "source": [
    "Esecuzione: Estrazione e salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8dc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri\n",
    "subfolders = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]\n",
    "\n",
    "# Estrazione e salvataggio\n",
    "process_and_save_features(\"Part1\", subfolders, \"color_moments_part1.npz\")\n",
    "process_and_save_features(\"Part2\", subfolders, \"color_moments_part2.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dbd1e1",
   "metadata": {},
   "source": [
    "Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27746597",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"Part1\"\n",
    "query_img = \"Part1/brain_glioma/brain_glioma_0005.jpg\"\n",
    "find_k_similar(base_folder, query_img, k=5)\n",
    "find_k_similar_mahalanobis(base_folder, query_img, k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889fab2",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3292ca",
   "metadata": {},
   "source": [
    "Task 4:\n",
    "\n",
    "\t1.\tAccetti come input:\n",
    "\t    •\tun’immagine di query (da “Part2”)\n",
    "\t    •\tuna scelta dell’utente sul tipo di feature space\n",
    "\t    •\tun valore intero k <= 2.\n",
    "\t2.\tCalcoli le features dell’immagine di query secondo il feature space selezionato.\n",
    "\t3.\tCalcoli la distanza tra la query e tutte le immagini del dataset (es. Euclidea o Mahalanobis).\n",
    "\t4.\tRaggruppi le distanze per label e selezioni le k classi (etichette) più simili in media.\n",
    "\t5.\tStampi/ritorni una classifica delle k etichette più probabili con il rispettivo punteggio (es. distanza  media o somma inversa delle distanze)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67210468",
   "metadata": {},
   "source": [
    "Un feature space è lo spazio vettoriale dove ogni immagine è rappresentata come un vettore di caratteristiche (feature vector).\n",
    "\n",
    "Il selected feature space indica quale tipo di caratteristiche estrai per rappresentare l’immagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f42b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_top_k_labels(query_img_path,k):\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "\n",
    "    # Estrazione delle feature dalla query (basata su Color Moments)\n",
    "    query_feature = extract_color_moments(query_img_path)\n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "    # Calcolo della distanza euclidea tra la query e tutte le immagini nel dataset\n",
    "    distances = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "\n",
    "    # Creazione di un DataFrame per semplificare le operazioni successive\n",
    "    df = pd.DataFrame({\n",
    "        'filename': flname_part1,  # nome immagine\n",
    "        'label': lbls_part1,        # etichetta (classe)\n",
    "        'distance': distances   # distanza dalla query\n",
    "    })\n",
    "\n",
    "    # Calcola la distanza media per classe\n",
    "    avg_dist_per_label = df.groupby('label')['distance'].mean().sort_values()\n",
    "\n",
    "    # Prendi le k classi più simili (minore distanza = maggiore similarità)\n",
    "    top_k_labels = avg_dist_per_label.head(k)\n",
    "\n",
    "    print(f\"\\nClassifica delle {k} etichette più probabili per la query:\")\n",
    "    for i, (label, score) in enumerate(top_k_labels.items(), 1):\n",
    "        print(f\"{i}. {label} | Score (distanza media): {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_k_labels_by_prototypes(query_img_path, k=2):\n",
    "    \"\"\"\n",
    "    Predice le k etichette più probabili per una query image confrontandola\n",
    "    con i prototipi (centroidi) di ciascuna classe nello spazio delle feature.\n",
    "\n",
    "    Parametri:\n",
    "    - query_img_path: percorso dell'immagine di query (parte 2)\n",
    "    - k: numero massimo di etichette da restituire (k <= 2)\n",
    "\n",
    "    Strategia:\n",
    "    - Calcola la media (prototipo) delle feature per ciascuna classe\n",
    "    - Confronta la query con ogni prototipo usando distanza euclidea\n",
    "    - Restituisce le k etichette con distanza minore (cioè più simili)\n",
    "    \"\"\"\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "\n",
    "    query_feature = extract_color_moments(query_img_path)\n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    # Costruzione DataFrame per calcolo dei prototipi per classe\n",
    "    df_features = pd.DataFrame(feat_matrix_part1)\n",
    "    df_features['label'] = lbls_part1\n",
    "\n",
    "    # Calcolo dei centroidi (prototipi) per ciascuna classe\n",
    "    class_prototypes = df_features.groupby('label').mean()\n",
    "\n",
    "    # Rimozione della colonna \"label\" dal prototipo (è diventato indice)\n",
    "    prototype_vectors = class_prototypes.values\n",
    "    prototype_labels = class_prototypes.index\n",
    "\n",
    "    # Calcolo della distanza della query da ciascun prototipo\n",
    "    distances = euclidean_distances(prototype_vectors, query_feature).flatten()\n",
    "\n",
    "    # Ordinamento per distanza crescente\n",
    "    sorted_indices = np.argsort(distances)\n",
    "    top_k = [(prototype_labels[i], distances[i]) for i in sorted_indices[:k]]\n",
    "\n",
    "    # Output\n",
    "    print(f\"\\n Classifica delle {k} etichette più vicine ai prototipi:\")\n",
    "    for i, (label, dist) in enumerate(top_k, 1):\n",
    "        print(f\"{i}. {label} | Distanza dal prototipo: {dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fe5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_glioma/brain_glioma_1112.jpg\"\n",
    "predict_top_k_labels_by_prototypes(query_img, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_glioma/brain_glioma_1112.jpg\"\n",
    "predict_top_k_labels(query_img,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c262152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ranking_strategies(query_img_path, k=2):\n",
    "    \"\"\"\n",
    "    Confronta le etichette predette da due strategie:\n",
    "    - distanza media per classe\n",
    "    - distanza al rappresentante (prototipo) della classe\n",
    "    Visualizza solo le top-k etichette con un grafico comparativo.\n",
    "    \"\"\"\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "\n",
    "    query_feature = extract_color_moments(query_img_path)\n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Feature non estratte.\")\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    # ===== Strategia 1: distanza media per classe =====\n",
    "    distances_all = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "    df_all = pd.DataFrame({\n",
    "        'label': lbls_part1,\n",
    "        'distance': distances_all\n",
    "    })\n",
    "    mean_dists = df_all.groupby('label')['distance'].mean().sort_values()\n",
    "\n",
    "    # ===== Strategia 2: distanza dal prototipo (centroide) =====\n",
    "    df_features = pd.DataFrame(feat_matrix_part1)\n",
    "    df_features['label'] = lbls_part1\n",
    "    class_prototypes = df_features.groupby('label').mean().drop(columns=['label'], errors='ignore')\n",
    "    proto_vectors = class_prototypes.values\n",
    "    proto_labels = class_prototypes.index\n",
    "    proto_dists = euclidean_distances(proto_vectors, query_feature).flatten()\n",
    "    proto_dists_series = pd.Series(proto_dists, index=proto_labels).sort_values()\n",
    "\n",
    "    # ===== Prendi le top-k etichette comuni =====\n",
    "    top_k_mean = mean_dists.head(k)\n",
    "    top_k_proto = proto_dists_series.head(k)\n",
    "\n",
    "    union_labels = sorted(set(top_k_mean.index).union(set(top_k_proto.index)))\n",
    "\n",
    "    # ===== Plot solo per le top-k etichette =====\n",
    "    x = np.arange(len(union_labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.bar(x - width/2, [mean_dists[label] for label in union_labels], width, label='Distanza Media')\n",
    "    ax.bar(x + width/2, [proto_dists_series[label] for label in union_labels], width, label='Distanza Prototipo')\n",
    "\n",
    "    ax.set_ylabel('Distanza')\n",
    "    ax.set_title(f\"Top-{k} Strategie - Query: {os.path.basename(query_img_path)}\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(union_labels)\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ===== Stampa i top-k risultati =====\n",
    "    print(\"\\n Top-k etichette per ciascuna strategia:\\n\")\n",
    "    print(\"Strategia: Distanza Media\")\n",
    "    print(top_k_mean)\n",
    "\n",
    "    print(\"\\n Strategia: Prototipo di Classe\")\n",
    "    print(top_k_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_glioma/brain_glioma_1142.jpg\"\n",
    "compare_label_ranking_strategies(query_img, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ef5c7",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa6891",
   "metadata": {},
   "source": [
    "Task 5\n",
    "\n",
    "feature model è il file dei dati delle feature che già estratto. Una rappresentazione di tutte le immagini del dataset in un certo feature space, salvata come matrice (feature_matrix), dove ogni riga è un’immagine (matrice n x d, dove n è il numero di immagini, e d il numero di feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd02c5",
   "metadata": {},
   "source": [
    "Dato:\n",
    "\n",
    "\t•\tun feature model (es. color_moments.npz)\n",
    "\t•\tun valore k\n",
    "\t•\tuna tecnica di riduzione dimensionale (SVD, LDA, k-means)\n",
    "\n",
    "Esegui:\n",
    "\n",
    "\t•\tRiduzione delle dimensioni nel feature space selezionato\n",
    "\t•\tEstrai le top-k componenti latenti\n",
    "\t•\tPer ogni componente, produci e salva: (immagine, peso) ordinati per importanza\n",
    "\t•\tSalva tutto in un file con nome chiaro (es. latent_semantics_svd_color_moments_k3.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398219e6",
   "metadata": {},
   "source": [
    "Cosa rappresenta K ? numero di componenti latenti che vuoi estrarre.\n",
    "\n",
    "SVD --> K è ilnumero di componenti principali - Riduce le dimensioni mantenendo variazione.\n",
    "\n",
    "LDA --> k è il numero di direzioni discriminanti - Riduce le dimensioni separando meglio le classi.\n",
    "\n",
    "k-means --> k è il numero di cluster - Divide le immagini in k gruppi simili (senza usare etichette)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_latent_space_2d(X_transformed, labels, technique, k):\n",
    "    \"\"\"Visualizza la proiezione 2D delle immagini nello spazio latente (solo per SVD/LDA).\"\"\"\n",
    "    print(f\"[DEBUG] Shape X_svd (dati trasformati): {X_transformed.shape}\")\n",
    "    if X_transformed.shape[1] < 2:\n",
    "        print(\"[INFO] Meno di 2 componenti: impossibile visualizzare in 2D.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=labels, palette=\"Set2\", s=80)\n",
    "    plt.title(f\"{technique.upper()} - Proiezione sulle prime 2 componenti latenti (k={k})\")\n",
    "    plt.xlabel(\"Componente 1\")\n",
    "    plt.ylabel(\"Componente 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_kmeans_clusters_2d(feature_matrix, labels, n_clusters):\n",
    "    \"\"\"Visualizza le immagini raggruppate da KMeans su uno spazio 2D ridotto con SVD.\"\"\"\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd.fit_transform(feature_matrix)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_labels, palette='tab10', s=80, style=labels)\n",
    "    plt.title(f\"KMeans Clustering (k={n_clusters}) con proiezione SVD 2D\")\n",
    "    plt.xlabel(\"Componente Latente 1 (da SVD)\")\n",
    "    plt.ylabel(\"Componente Latente 2 (da SVD)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "def task5_latent_semantics(feature_model_path, technique, k):\n",
    "    \"\"\"\n",
    "    Estrae i top-k concetti latenti da uno spazio di feature usando SVD, LDA o KMeans.\n",
    "    Visualizza lo spazio latente ed esporta un file .txt con i pesi associati alle immagini.\n",
    "    \"\"\"\n",
    "\n",
    "    technique = technique.lower()\n",
    "    method = \"\"\n",
    "    X_transformed = None\n",
    "    components = None\n",
    "\n",
    "    if technique == \"svd\":\n",
    "        model = TruncatedSVD(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1)\n",
    "        components = model.components_\n",
    "        method = \"svd\"\n",
    "\n",
    "    elif technique == \"lda\":\n",
    "        unique_labels = np.unique(lbls_part1)\n",
    "        max_k = len(unique_labels) - 1\n",
    "        if k > max_k:\n",
    "            print(f\"[ATTENZIONE] LDA supporta al massimo {max_k} componenti con {len(unique_labels)} classi.\")\n",
    "            k = max_k\n",
    "        model = LDA(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1, lbls_part1)\n",
    "        components = model.scalings_.T[:k]\n",
    "        method = \"lda\"\n",
    "\n",
    "    elif technique == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        model.fit(feat_matrix_part1)\n",
    "        components = model.cluster_centers_\n",
    "        X_transformed = model.transform(feat_matrix_part1)\n",
    "        method = \"kmeans\"\n",
    "    else:\n",
    "        print(\"[ERRORE] Tecnica non supportata. Usa: 'svd', 'lda', 'kmeans'\")\n",
    "        return\n",
    "\n",
    "    # Visualizzazione\n",
    "    if technique in [\"svd\", \"lda\"]:\n",
    "        plot_latent_space_2d(X_transformed, lbls_part1, technique, k)\n",
    "    elif technique == \"kmeans\":\n",
    "        plot_kmeans_clusters_2d(feat_matrix_part1, lbls_part1, k)\n",
    "\n",
    "    # Creazione output\n",
    "    os.makedirs(\"task5_output\", exist_ok=True)\n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = os.path.join(\"task5_output\", f\"latent_semantics_{method}_{base_name}_k{k}.txt\")\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for i in range(k):\n",
    "            f.write(f\"\\n--- Latent Semantic {i+1} ---\\n\")\n",
    "            if technique in [\"svd\", \"lda\"]:\n",
    "                weights = feat_matrix_part1 @ components[i].T\n",
    "                sorted_idx = np.argsort(-np.abs(weights))\n",
    "            else:  # KMeans: distanza dal centroide, più piccola = più vicino\n",
    "                weights = -X_transformed[:, i]\n",
    "                sorted_idx = np.argsort(weights)\n",
    "\n",
    "            for idx in sorted_idx:\n",
    "                image_id = flname_part1[idx]\n",
    "                f.write(f\"{image_id} | Peso: {weights[idx]:.4f} | Classe: {lbls_part1[idx]}\\n\")\n",
    "\n",
    "    print(f\"[SALVATO] Latent semantics salvati in: {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=min(feat_matrix_part1.shape)\n",
    "\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"svd\", k=5)\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"svd\", k=n_components)\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"lda\", k=2)\n",
    "task5_latent_semantics(\"color_moments.npz\", technique=\"kmeans\", k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f0428",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a5bf8",
   "metadata": {},
   "source": [
    "Obiettivo del Task 6\n",
    "\n",
    "Trasformazione in uno spazio latente ortonormale: Utilizzando una tecnica di riduzione dimensionale, trasformiamo l’insieme delle feature originali in un nuovo spazio le cui componenti sono ortonormali.\n",
    "\n",
    "Concentrazione del potere discriminante: Le componenti vengono ordinate in base alla varianza spiegata (cioè, la potenza discriminante). In questo modo, le prime componenti (quelle con varianza più elevata) contengono la maggior parte dell’informazione utile, mentre le componenti successive sono meno significative.\n",
    "\n",
    "Dimensionalità intrinseca: Se il numero di componenti latenti (m) ottenute è uguale al numero di feature di partenza, la trasformazione preserva esattamente tutte le distanze ed angoli (cioè, la trasformazione è invertibile e non viene persa informazione). Se invece la dimensionalità intrinseca è minore di m, allora potrai scegliere di mantenere solo quelle componenti più significative. In questo caso, le distanze e gli angoli nel database trasformato saranno approssimati, introducendo un errore proporzionale alla quantità di informazione “persa” trascurando le componenti meno significative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa844a",
   "metadata": {},
   "source": [
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "Questa riga:\n",
    "\tOttiene un array di float dove ciascun valore rappresenta la percentuale di varianza spiegata da ciascuna componente principale della PCA.\n",
    "\n",
    "\tÈ ordinato dalla componente più importante a quella meno importante.\n",
    "\n",
    "esempio: explained = [0.40, 0.30, 0.20, 0.07, 0.03]\n",
    "\n",
    "Significa:\n",
    "    La prima componente spiega il 40% della varianza\n",
    "\n",
    "\tLa seconda il 30%, ecc.\n",
    "\n",
    "\n",
    "\n",
    "cumulative = np.cumsum(explained)\n",
    "\n",
    "Calcola la somma cumulativa dell’array explained, cioè la varianza totale spiegata fino a ciascuna componente.\n",
    "\n",
    "es:cumulative = [0.40, 0.70, 0.90, 0.97, 1.00]\n",
    "\n",
    "Significa:\n",
    "\t•\tLe prime 2 componenti spiegano il 70%\n",
    "\t•\tLe prime 3 il 90%\n",
    "\t•\tLe prime 4 il 97%, ecc.\n",
    "\n",
    "\n",
    "intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "Questa riga:\n",
    "\t•\tTrova il primo indice in cui la varianza cumulativa raggiunge o supera una soglia (threshold), ad esempio 0.95 (95%).\n",
    "\t•\tnp.argmax(...) restituisce il primo True nella condizione cumulative >= threshold.\n",
    "\t•\tSi aggiunge +1 perché gli indici Python partono da 0, ma il numero di componenti parte da 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12aee35",
   "metadata": {},
   "source": [
    "studio del k migliore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_intrinsic_dimensionality(feature_matrix, threshold, plot=True):\n",
    "    max_components = min(feature_matrix.shape)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(feature_matrix)\n",
    "\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "    intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "    #print(f\"[INFO] Spiegazione varianza per ogni componente PCA:\\n{explained}\")\n",
    "    #print(f\"[INFO] Varianza cumulativa:\\n{cumulative}\")\n",
    "    #print(f\"[INFO] Soglia impostata: {threshold}\")\n",
    "    #print(f\"[INFO] Dimensione intrinseca stimata: {intrinsic_dim}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cumulative, marker='o', label=\"Varianza cumulativa\")\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Soglia {threshold*100:.0f}%\")\n",
    "        plt.axvline(x=intrinsic_dim, color='g', linestyle='--', label=f\"k suggerito: {intrinsic_dim}\")\n",
    "        plt.xlabel(\"Numero componenti\")\n",
    "        plt.ylabel(\"Varianza cumulativa\")\n",
    "        plt.title(\"Scelta ottimale di k (PCA/SVD)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"[INFO] k ottimale suggerito (soglia {threshold*100:.0f}%): {intrinsic_dim}\")\n",
    "    return intrinsic_dim, cumulative\n",
    "\n",
    "def suggest_k(feature_matrix, threshold_list=[0.90, 0.95, 0.99]):\n",
    "    print(f\"[INFO] Feature matrix shape: {feature_matrix.shape}\")\n",
    "    k_values = {}\n",
    "    for t in threshold_list:\n",
    "        k, _ = estimate_intrinsic_dimensionality(feature_matrix, threshold=t, plot=False)\n",
    "        k_values[t] = k\n",
    "        print(f\"Soglia {int(t*100)}% : k = {k}\")\n",
    "    return k_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality_per_label(feature_matrix, labels, threshold):\n",
    "    label_dim_map = {}\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"[INFO] Etichette uniche trovate: {len(unique_labels)}\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        label_features = feature_matrix[indices]\n",
    "\n",
    "        if len(indices) < 2:\n",
    "            print(f\"[AVVISO] Label '{label}' ha meno di 2 campioni — ignorata.\")\n",
    "            continue\n",
    "\n",
    "        k, _ = estimate_intrinsic_dimensionality(label_features, threshold=threshold, plot=False)\n",
    "        label_dim_map[label] = k\n",
    "        print(f\" Label '{label}' : k = {k}\")\n",
    "\n",
    "    return label_dim_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f13f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola k per varie soglie\n",
    "print(\"\\Stima automatica di k in base alla varianza spiegata:\\n\")\n",
    "k_suggeriti = suggest_k(feat_matrix_part1)\n",
    "# Plot dettagliato per la soglia 95%\n",
    "estimate_intrinsic_dimensionality(feat_matrix_part1, threshold=0.95, plot=True)\n",
    "\n",
    "\n",
    "print(\"\\n Task 6b – Dimensionalità per etichetta:\\n\")\n",
    "label_dimensionalities = estimate_dimensionality_per_label(feat_matrix_part1, lbls_part1, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c6970",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba93a0",
   "metadata": {},
   "source": [
    "Implementare un programma che:\n",
    "\n",
    "1) per ogni etichetta unica l, calcoli la corrispondente k semantica latente (a scelta)\n",
    "associata alle immagini della parte 1 \n",
    "\n",
    "2) per le immagini della parte 2, preveda le etichette più probabili utilizzando le distanze/similitudini calcolate\n",
    "in base alla semantica latente specifica dell'etichetta.\n",
    "Il sistema deve inoltre fornire i valori di precisione, richiamo e punteggio F1 per etichetta, nonché\n",
    "un valore di accuratezza complessiva.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aaa7d5",
   "metadata": {},
   "source": [
    "1. Estrazione della semantica latente per ogni classe\n",
    "Funzione: compute_latent_semantics_per_class(X, y, k)\n",
    "\n",
    "Input:\n",
    "- X: Matrice delle caratteristiche delle immagini della Parte 1.\n",
    "- y: Vettore delle etichette corrispondenti.\n",
    "- k: Numero di componenti latenti da estrarre tramite SVD.\n",
    "\n",
    "Output:\n",
    "- class_models: Contiene scaler, modello SVD e vettori latenti per ciascuna classe.\n",
    "- class_means: Contiene il vettore medio (centroide latente) per ciascuna classe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f2964",
   "metadata": {},
   "source": [
    "2 Funzione: predict_label(X_test, class_models, class_means)\n",
    "\n",
    "Input:\n",
    "- X_test: Matrice delle immagini da classificare.\n",
    "- class_models: Modelli latenti per ciascuna classe.\n",
    "- class_means: Centroidi dei vettori latenti per ciascuna classe.\n",
    "\n",
    "Output:\n",
    "- y_pred: Lista delle etichette previste per ciascuna immagine.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158e54c",
   "metadata": {},
   "source": [
    "Funzioni: evaluate(y_true, y_pred), evaluate_predictions(true_labels, predicted_labels)\n",
    "\n",
    "Metriche calcolate:\n",
    "- Precisione per classe\n",
    "- Recall per classe\n",
    "- F1 score per classe\n",
    "- Accuratezza complessiva\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1eca69",
   "metadata": {},
   "source": [
    "Ogni classe ha uno spazio semantico proprio, dove le immagini sono rappresentate in modo compatto.\n",
    "L'immagine da classificare è proiettata in ogni spazio latente e assegnata alla classe più simile in base alla distanza dal centroide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfdf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_semantics_per_class(X, y, k=10):\n",
    "    class_models = {}\n",
    "    class_means = {}\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        X_class = X[y == label]  # Prende solo le istanze della classe corrente\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_class)  # Normalizza i dati della classe\n",
    "\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        latent = svd.fit_transform(X_scaled)  # Riduzione dimensionale con SVD\n",
    "\n",
    "        # Salva modello SVD e scaler per la classe\n",
    "        class_models[label] = {\n",
    "            'svd': svd,\n",
    "            'scaler': scaler,\n",
    "            'latent_vectors': latent\n",
    "        }\n",
    "        # Calcola la media dei vettori latenti della classe\n",
    "        class_means[label] = np.mean(latent, axis=0)\n",
    "    return class_models, class_means\n",
    "\n",
    "def predict_label(X_test, class_models, class_means):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        best_label = None\n",
    "        min_dist = float('inf')\n",
    "        for label, model in class_models.items():\n",
    "            x_scaled = model['scaler'].transform(x.reshape(1, -1))  # Normalizza x\n",
    "            x_latent = model['svd'].transform(x_scaled)  # Trasforma in spazio latente\n",
    "            dist = np.linalg.norm(x_latent - class_means[label])  # Distanza dal centroide\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_label = label\n",
    "        y_pred.append(best_label)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    labels = np.unique(y_true)\n",
    "    print(\"Per-class metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\n",
    "            f\"Class {label}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f}\")\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    print(\"[VALUTAZIONE] Report di classificazione:\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4646513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento sui dati di Part1\n",
    "class_models, class_means = compute_latent_semantics_per_class(\n",
    "    feat_matrix_part1, lbls_part1, k=10)\n",
    "\n",
    "# Predizione su Part2\n",
    "predicted_labels = predict_label(feat_matrix_part2, class_models, class_means)\n",
    "\n",
    "# Valutazione\n",
    "evaluate(lbls_part2, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864cc02",
   "metadata": {},
   "source": [
    "Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b65a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from PIL import Image\n",
    "\n",
    "def apply_dbscan_with_pca(features, eps=2.0, min_samples=3, n_components=50):\n",
    "    \"\"\"\n",
    "    Riduce 'features' a 'n_components' dimensioni con PCA, quindi applica DBSCAN\n",
    "    e restituisce l'array di cluster-labels (interi) di lunghezza = numero di righe in 'features'.\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] PCA -> Riduzione a {n_components} componenti\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    reduced_scaled = scaler.fit_transform(reduced_features)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(reduced_scaled)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def top_c_clusters(cluster_labels, c):\n",
    "    \"\"\"\n",
    "    cluster_labels: array di interi di lunghezza N.\n",
    "    c: numero di cluster \"più popolosi\" che vogliamo restituire.\n",
    "    Restituisce la lista dei c valori di cluster (escludendo -1) \n",
    "    ordinati in base alla dimensione (numero di occorrenze) decrescente.\n",
    "    Se DBSCAN ha trovato meno di c cluster, restituisce tutti quelli disponibili.\n",
    "    \"\"\"\n",
    "    # Conteggio delle occorrenze per ogni etichetta di cluster\n",
    "    label_counts = Counter(cluster_labels)\n",
    "    # Rimuovo il rumore (-1) se presente\n",
    "    label_counts.pop(-1, None)\n",
    "    \n",
    "    if not label_counts:\n",
    "        print(\"[WARN] DBSCAN non ha trovato alcun cluster valido (solo rumore).\")\n",
    "        return []\n",
    "    \n",
    "    # Estraiamo i c cluster più frequenti\n",
    "    most_common = label_counts.most_common(c)  # es. [(label1, count1), (label2, count2), ...]\n",
    "    top = [int(lbl) for lbl, _ in most_common]\n",
    "    \n",
    "    # Se DBSCAN ha trovato meno di c cluster, most_common contiene già tutti\n",
    "    if len(top) < c:\n",
    "        print(f\"[WARN] DBSCAN ha trovato solo {len(top)} cluster (meno di {c}).\")\n",
    "    return top\n",
    "\n",
    "\n",
    "def plot_mds_clusters(features, cluster_labels, top_clusters, metric='euclidean'):\n",
    "    \"\"\"\n",
    "    features: array (N, d) delle tue feature originali (senza aver fatto PCA).\n",
    "    cluster_labels: array (N,) con i risultati DBSCAN.\n",
    "    top_clusters: lista di interi pari ai cluster \"significativi\" (col più grandi).\n",
    "    metric: la distanza da usare per MDS (default 'euclidean').\n",
    "    Mostra un grafico 2D (scatter) con i punti appartenenti ai top_clusters colorati diversamente,\n",
    "    tutti gli altri (cluster minori o -1) in grigio chiaro.\n",
    "    \"\"\"\n",
    "    # 1) Normalizzo / scalizzo le features originali\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # 2) Calcolo la matrice di distanze (facoltativo) e poi MDS → Y (N×2)\n",
    "    #    Qui usiamo MDS “direttamente” su features_scaled, che di default assume euclidea.\n",
    "    mds = MDS(n_components=2, random_state=42, dissimilarity='euclidean')\n",
    "    Y = mds.fit_transform(features_scaled)\n",
    "\n",
    "    # 3) Plotto i punti\n",
    "    import matplotlib\n",
    "    cmap= matplotlib.colormaps['tab10']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(Y)):\n",
    "        lbl = cluster_labels[i]\n",
    "        if lbl in top_clusters:\n",
    "            color_idx = top_clusters.index(lbl)\n",
    "            plt.scatter(Y[i,0], Y[i,1], color=cmap(color_idx), s=30, edgecolor='k', linewidth=0.2)\n",
    "        else:\n",
    "            # punti rumore o cluster “non top”\n",
    "            plt.scatter(Y[i,0], Y[i,1], color='lightgray', s=8)\n",
    "\n",
    "    plt.title(f\"MDS 2D – Top {len(top_clusters)} cluster\")\n",
    "    plt.xlabel(\"MDS 1\")\n",
    "    plt.ylabel(\"MDS 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_cluster_thumbnails(images, cluster_labels, top_clusters, thumb_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    images: lista (o array) di percorsi file (lunghezza N), \n",
    "            ossia a images[i] corrisponde features[i].\n",
    "    cluster_labels: array (N,) di cluster per ogni immagine.\n",
    "    top_clusters: lista dei c cluster (int) che vogliamo visualizzare.\n",
    "    thumb_size: dimensione (w,h) di ogni miniatura.\n",
    "    Per ogni cluster ∈ top_clusters stampa a video (o fa plt.show) \n",
    "    una griglia di miniature (fino a ~16‐25 alla volta).\n",
    "    \"\"\"\n",
    "    for cluster_id in top_clusters:\n",
    "        # Indici di tutte le immagini che appartengono a questo cluster\n",
    "        idxs = [i for i, cl in enumerate(cluster_labels) if cl == cluster_id]\n",
    "        print(f\"[INFO] Cluster {cluster_id}: {len(idxs)} immagini trovate\")\n",
    "\n",
    "        # Se vogliamo limitare a N miniatura per cluster (tipo 16):\n",
    "        max_display = min(len(idxs), 16)\n",
    "        n = int(np.ceil(np.sqrt(max_display)))  # facciamo una griglia n×n\n",
    "        plt.figure(figsize=(n, n))\n",
    "\n",
    "        for j, i_img in enumerate(idxs[:max_display]):\n",
    "            img = Image.open(images[i_img]).convert('RGB')\n",
    "            img_thumb = img.resize(thumb_size, Image.LANCZOS)\n",
    "\n",
    "\n",
    "            ax = plt.subplot(n, n, j+1)\n",
    "            plt.imshow(img_thumb)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Cluster {cluster_id} – {len(idxs)} immagini (mostrate: {max_display})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ddc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# BLOCCHETTO PER TASK 8 (RIUSO FUNZIONI)\n",
    "# ================================\n",
    "\n",
    "# 3) PARAMETRI (modificabili a piacere)\n",
    "eps = 2.0            # valore DBSCAN di esempio\n",
    "min_samples = 3      # valore DBSCAN di esempio\n",
    "n_components = 50    # quante dimensioni tenere con PCA PRIMA di DBSCAN\n",
    "c = 3                # quanti cluster “significativi” voglio prendere per ciascuna label\n",
    "\n",
    "# 4) Creare una cartella di output (facoltativo)\n",
    "output_base = \"./results_task8\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# 3) Costruisco l’elenco dei full path per tutte le immagini\n",
    "base_folder = \"Part1\"  # o path assoluto \"/Users/.../Parte1\"\n",
    "images_full = [ os.path.join(base_folder, lbl, fname)\n",
    "                for fname, lbl in zip(flname_part1, lbls_part1) ]\n",
    "\n",
    "# 5) SCORRO OGNI LABEL DI Parte1 E APPLICO DBSCAN+PCA\n",
    "unique_labels = np.unique(lbls_part1)  # es. [\"Glioma\",\"Meningioma\",\"Pituitary\"]\n",
    "\n",
    "for lbl in unique_labels:\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"[INFO] Elaboro label: {lbl}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # 5.1) Estraggo le righe di feat_matrix_part1 / flname_part1 corrispondenti\n",
    "    mask_lbl = (lbls_part1 == lbl)\n",
    "    features_label = feat_matrix_part1[mask_lbl]   # shape = (n_i, d)\n",
    "    images_label   = np.array(images_full)[mask_lbl]\n",
    "\n",
    "    # 5.2) Chiamo la tua funzione che fa PCA + DBSCAN\n",
    "    cluster_labels = apply_dbscan_with_pca(\n",
    "        features_label,\n",
    "        eps=eps,\n",
    "        min_samples=min_samples,\n",
    "        n_components=n_components\n",
    "    )\n",
    "    print(f\"[INFO] Cluster-labels trovati: {np.unique(cluster_labels)}\")\n",
    "\n",
    "    # 5.3) Trovo i c cluster più grandi\n",
    "    top_clusters = top_c_clusters(cluster_labels, c)\n",
    "    print(f\"[INFO] Top {c} cluster (per dimensione): {top_clusters}\")\n",
    "\n",
    "    # 5.4) Creo sotto-cartella di output per questa label\n",
    "    out_dir_lbl = os.path.join(output_base, f\"label_{lbl}\")\n",
    "    os.makedirs(out_dir_lbl, exist_ok=True)\n",
    "\n",
    "    # 5.5) MDS‐2D + scatter plot del clustering\n",
    "    print(f\"[INFO] Disegno MDS 2D per i cluster di '{lbl}' …\")\n",
    "    # (ATTENZIONE: plot_mds_clusters in genere “fa plt.show()” a video.\n",
    "    #  Se vuoi salvare l’immagine invece di far vedere a notebook, \n",
    "    #  devi modificare leggermente quella funzione per usare plt.savefig())\n",
    "    plot_mds_clusters(\n",
    "        features_label,\n",
    "        cluster_labels,\n",
    "        top_clusters,\n",
    "        metric='euclidean'\n",
    "    )\n",
    "    # Se invece vuoi **salvare** l’immagine in PNG anziché fare “show()”:\n",
    "    #    plt.savefig(os.path.join(out_dir_lbl, f\"{lbl}_MDS_clusters.png\"))\n",
    "    #    plt.close()\n",
    "\n",
    "    # 5.6) Creo le miniature di ogni cluster “significativo”\n",
    "    print(f\"[INFO] Genero miniature per ciascun cluster di '{lbl}' …\")\n",
    "    show_cluster_thumbnails(\n",
    "        images_label,      # array di stringhe di percorsi\n",
    "        cluster_labels,    # array di int di lunghezza n_i\n",
    "        top_clusters,      # la lista dei c indici di cluster\n",
    "        thumb_size=(64, 64)\n",
    "    )\n",
    "    # Anche qui, di default quella funzione fa plt.show() per ogni cluster.\n",
    "    # Se vuoi salvare le figure in file, modifica show_cluster_thumbnails in\n",
    "    # modo che setti un outpath e faccia plt.savefig().\n",
    "\n",
    "print(\"\\n[FINITO] Task 8 completato per tutte le label di Parte1.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667ee99",
   "metadata": {},
   "source": [
    "Task 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9edaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imposta il valore di m per l'm-NN\n",
    "m = 5  # Modifica questo valore in base alle tue necessità\n",
    "\n",
    "# Addestramento m-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=m)\n",
    "knn_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Addestramento Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Predizioni su Part2\n",
    "pred_knn = knn_model.predict(feat_matrix_part2)\n",
    "pred_dt = dt_model.predict(feat_matrix_part2)\n",
    "\n",
    "# Valutazione m-NN\n",
    "print(\"Risultati m-NN:\")\n",
    "print(classification_report(lbls_part2, pred_knn))\n",
    "print(\"Accuratezza complessiva m-NN:\", accuracy_score(lbls_part2, pred_knn))\n",
    "\n",
    "# Valutazione Decision Tree\n",
    "print(\"Risultati Decision Tree:\")\n",
    "print(classification_report(lbls_part2, pred_dt))\n",
    "print(\"Accuratezza complessiva Decision Tree:\", accuracy_score(lbls_part2, pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae27b9",
   "metadata": {},
   "source": [
    "Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b424686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH_EuclideanQuantized:\n",
    "    \"\"\"\n",
    "    LSH per distanza Euclidea con quantizzazione (p-stable).\n",
    "    num_layers = L, num_hashes = h, dim = D, r = bucket width.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_hashes: int, dim: int, r: float):\n",
    "        self.L = num_layers\n",
    "        self.h = num_hashes\n",
    "        self.d = dim\n",
    "        self.r = r\n",
    "\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.L)]\n",
    "        self.a_vectors = [\n",
    "            [np.random.randn(self.d) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.b_offsets = [\n",
    "            [np.random.uniform(0, self.r) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.data_vectors = None\n",
    "\n",
    "    def _compute_hash_tuple(self, vec: np.ndarray, layer_idx: int) -> tuple:\n",
    "        bits = []\n",
    "        for j in range(self.h):\n",
    "            a_j = self.a_vectors[layer_idx][j]\n",
    "            b_j = self.b_offsets[layer_idx][j]\n",
    "            proj = float(np.dot(a_j, vec) + b_j)\n",
    "            h_val = int(np.floor(proj / self.r))\n",
    "            bits.append(h_val)\n",
    "        return tuple(bits)\n",
    "\n",
    "    def index(self, vectors: np.ndarray):\n",
    "        self.data_vectors = vectors\n",
    "        N, D = vectors.shape\n",
    "        assert D == self.d, f\"Dimensione vettore ({D}) ≠ D di LSH ({self.d}).\"\n",
    "        for idx in range(N):\n",
    "            v = vectors[idx]\n",
    "            for l in range(self.L):\n",
    "                key = self._compute_hash_tuple(v, l)\n",
    "                self.hash_tables[l][key].append(idx)\n",
    "\n",
    "    def query(self, q_vec: np.ndarray, top_t: int = 5):\n",
    "        assert q_vec.shape[0] == self.d, \"Dimensione query ≠ D.\"\n",
    "        candidati = set()\n",
    "        total_checked = 0\n",
    "        for l in range(self.L):\n",
    "            h_tuple = self._compute_hash_tuple(q_vec, l)\n",
    "            bucket = self.hash_tables[l].get(h_tuple, [])\n",
    "            total_checked += len(bucket)\n",
    "            candidati.update(bucket)\n",
    "\n",
    "        risultati = []\n",
    "        for idx in candidati:\n",
    "            v_i = self.data_vectors[idx]\n",
    "            dist = np.linalg.norm(v_i - q_vec)\n",
    "            risultati.append((idx, dist))\n",
    "        risultati.sort(key=lambda x: x[1])\n",
    "        top_results = risultati[:top_t]\n",
    "        return top_results, len(candidati), total_checked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Center + L2 normalize su Part1\n",
    "mean_vec = np.mean(feat_matrix_part1, axis=0)\n",
    "feat_centered = feat_matrix_part1 - mean_vec\n",
    "feat_normed = normalize(feat_centered, norm='l2', axis=1)\n",
    "\n",
    "# 2) Parametri LSH con quantizzazione\n",
    "D = feat_normed.shape[1]      # ad esempio 900\n",
    "L = 5                         # numero di layer (esempio)\n",
    "h = 5                        # numero di hash per layer (esempio)\n",
    "r = 5.0                       # bucket width, da sperimentare\n",
    "\n",
    "# 3) Creo l'indice\n",
    "lsh_quant = LSH_EuclideanQuantized(num_layers=L, num_hashes=h, dim=D, r=r)\n",
    "lsh_quant.index(feat_normed)\n",
    "\n",
    "print(f\"[INFO] Indice LSH-Quant creato. D={D}, L={L}, h={h}, r={r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_lsh_quant(base_folder: str, img_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a img_path (di Part2) usando lsh_quant costruito su Part1.\n",
    "    \"\"\"\n",
    "    # 1) Estrai feature raw (900-dim)\n",
    "    raw_q = np.array(extract_color_moments(img_path), dtype=np.float32)\n",
    "\n",
    "    # 2) Center + normalize (stesso mean_vec usato su Part1)\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # 3) Chiamata LSH\n",
    "    top_results, unique_count, total_checked = lsh_quant.query(q_normed, top_t=k)\n",
    "\n",
    "    # 4) Stampo i risultati testuali\n",
    "    print(f\"\\n[LSH-Quant] Top {k} simili a: {img_path}\")\n",
    "    for rank, (idx, dist) in enumerate(top_results, start=1):\n",
    "        label = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        print(f\"  {rank}. {fname} | Classe: {label} | Distanza Euclidea: {dist:.2f}\")\n",
    "    print(f\"[LSH-Quant] Immagini uniche considerate: {unique_count}\")\n",
    "    print(f\"[LSH-Quant] Immagini totali controllate: {total_checked}\")\n",
    "\n",
    "    # 5) Visualizzazione (query + k risultati)\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(4*(k+1), 4))\n",
    "    img_q = cv2.imread(img_path)\n",
    "    img_q = cv2.cvtColor(img_q, cv2.COLOR_BGR2RGB)\n",
    "    axs[0].imshow(img_q)\n",
    "    axs[0].set_title(\"Query (LSH-Quant)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, (idx, dist) in enumerate(top_results, start=1):\n",
    "        lab = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        full_path = os.path.join(base_folder, lab, fname)\n",
    "        img_match = cv2.imread(full_path)\n",
    "        img_match = cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img_match)\n",
    "        axs[i].set_title(f\"Rank {i}\\nd={dist:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ce3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizzo su un'immagine di Part2 ---\n",
    "query_path = \"Part2/brain_menin/brain_menin_1003.jpg\"\n",
    "\n",
    "print(query_path);\n",
    "k = 5                         # numero di immagini simili da visualizzare\n",
    "\n",
    "# Eseguo la ricerca LSH\n",
    "find_k_similar_lsh_quant(\"Part1\", query_path, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740a0ea",
   "metadata": {},
   "source": [
    "Part2/brain_menin/brain_menin_1003.jpg\n",
    "\n",
    "[LSH] Top 5 immagini simili a: Part2/brain_menin/brain_menin_1003.jpg\n",
    "  1. brain_glioma_0596.jpg | Classe: brain_glioma | Distanza: 280.09\n",
    "  2. brain_tumor_0280.jpg | Classe: brain_tumor | Distanza: 371.85\n",
    "  3. brain_tumor_0769.jpg | Classe: brain_tumor | Distanza: 384.50\n",
    "  4. brain_menin_0470.jpg | Classe: brain_menin | Distanza: 386.37\n",
    "  5. brain_menin_0554.jpg | Classe: brain_menin | Distanza: 389.00\n",
    "[LSH] Immagini uniche considerate: 1337\n",
    "[LSH] Immagini totali controllate: 1625\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
