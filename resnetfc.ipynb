{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0f5c32",
   "metadata": {},
   "source": [
    "    ResNet-FC-1000: Ridimensiona l'immagine a 224x224; collega un \"hook\" all'output del livello \"fc\" dell'architettura pre-addestrata ResNet per ottenere un tensore di dimensione 1000.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b82eb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "# Standard library\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Scientific/numeric\n",
    "import numpy as np\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch and torchvision\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "# Machine Learning (scikit-learn)\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.metrics.pairwise import (\n",
    "    pairwise_distances,\n",
    "    cosine_distances,\n",
    "    euclidean_distances, \n",
    "    manhattan_distances\n",
    ")\n",
    "from scipy.spatial.distance import (\n",
    "    correlation, \n",
    "    mahalanobis\n",
    ") \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Advanced dimensionality reduction\n",
    "import umap\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468c50e",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6784274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_brain(img):\n",
    "    \"\"\"Ritaglia l'area informativa (cervello) da un'immagine.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "    coords = cv2.findNonZero(thresh)\n",
    "    if coords is not None:\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        return img[y:y+h, x:x+w]\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    return img  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a3ee13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se CUDA disponibile, usa la GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carica ResNet50 pre-addestrata e in modalità eval\n",
    "weights = ResNet50_Weights.IMAGENET1K_V1  # o DEFAULT per i pesi più aggiornati\n",
    "model = models.resnet50(weights=weights)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Preprocessing standard per ResNet\n",
    "preprocess = weights.transforms()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8e763",
   "metadata": {},
   "source": [
    "# Task 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6a398",
   "metadata": {},
   "source": [
    "Implementa un programma che estrae e memorizza i descrittori di feature per tutte le immagini nel set di dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "eda72568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fc_features(image_path, model, preprocess, device):\n",
    "    \"\"\"\n",
    "    Estrae:\n",
    "    - feature del layer fully connected (fc) di ResNet50\n",
    "    - immagine ritagliata (cropped_img)\n",
    "    - tensore preprocessato (img_tensor)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERRORE] Immagine non trovata o non valida: {image_path}\")\n",
    "        return None, None, None\n",
    "\n",
    "    cropped_img = crop_to_brain(img)\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
    "    img_tensor = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    fc_output = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        fc_output.append(output)\n",
    "\n",
    "    hook = model.fc.register_forward_hook(hook_fn)\n",
    "    with torch.no_grad():\n",
    "        model(img_tensor)\n",
    "    hook.remove()\n",
    "\n",
    "    if fc_output:\n",
    "        fc_vec = fc_output[0].squeeze(0).cpu().numpy()\n",
    "        return fc_vec, cropped_img, img_tensor\n",
    "    else:\n",
    "        print(f\"[ERRORE] Nessun output FC per {image_path}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa9a99",
   "metadata": {},
   "source": [
    "Elaborazione Batch e Salvataggio Feature in .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d43f4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_features(base_folder, subfolders, output_file):\n",
    "    \"\"\"\n",
    "    Estrae le feature FC da immagini in più cartelle e salva in un file .npz.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_filenames = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label in subfolders:\n",
    "        folder_path = os.path.join(base_folder, label)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"[ATTENZIONE] Cartella non trovata: {folder_path}\")\n",
    "            continue\n",
    "        print(f\"[INFO] Elaboro cartella: {label}\")\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                features, _, _ = extract_fc_features(img_path, model, preprocess, device)\n",
    "                if features is not None:\n",
    "                    all_features.append(features)\n",
    "                    all_filenames.append(filename)\n",
    "                    all_labels.append(label)\n",
    "                else:\n",
    "                    print(f\"[ERRORE] Feature non estratte da {img_path}\")\n",
    "\n",
    "    # Salva in file .npz\n",
    "    np.savez(output_file,\n",
    "             features=np.array(all_features),\n",
    "             filenames=np.array(all_filenames),\n",
    "             labels=np.array(all_labels))\n",
    "    \n",
    "    print(f\"[SALVATO] Features salvate in {output_file}\")\n",
    "    print(f\"[FINE] Totale immagini processate: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri cartelle e output\n",
    "subfolders = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]\n",
    "\n",
    "# Estrazione e salvataggio\n",
    "process_and_save_features(\"Part1\", subfolders, \"resnetfc_part1\")\n",
    "process_and_save_features(\"Part2\", subfolders, \"resnetfc_part2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c57009b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione di creazione dei file\n",
    "def load_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    return data['features'], data['labels'], data.get('filenames', [f\"img_{i}\" for i in range(len(data['features']))])\n",
    "\n",
    "\n",
    "feat_matrix_part1, lbls_part1, flname_part1 = load_features(\n",
    "    \"resnetfc_part1.npz\")\n",
    "feat_matrix_part2, lbls_part2, flname_part2 = load_features(\n",
    "    \"resnetfc_part2.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce2ad1",
   "metadata": {},
   "source": [
    "### Grad-CAM (Gradient-weighted Class Activation Mapping)\n",
    "\n",
    "#### Cos'è il Grad-CAM?\n",
    "\n",
    "Grad-CAM è una tecnica di visualizzazione per le **reti neurali convoluzionali (CNN)** che permette di capire **quali regioni dell'immagine** hanno influenzato maggiormente la predizione del modello.\n",
    "\n",
    "È particolarmente utile per:\n",
    "- applicazioni mediche: localizzazione di tumori o anomalie in immagini diagnostiche.\n",
    "\n",
    "---\n",
    "\n",
    "#### Come funziona\n",
    "\n",
    "Grad-CAM utilizza:\n",
    "\n",
    "1. Le **attivazioni** di un layer convoluzionale profondo (es. `layer4` in ResNet).\n",
    "2. I **gradienti dei logit** (output prima del softmax) rispetto a quelle attivazioni.\n",
    "\n",
    "##### Passaggi principali:\n",
    "\n",
    "1. **Esegui forward pass** per ottenere le predizioni del modello.\n",
    "2. **Esegui backward pass** sulla classe target per ottenere i gradienti rispetto al layer scelto.\n",
    "3. Calcola i **pesi (αᵢ)** facendo la media spaziale dei gradienti per ogni canale.\n",
    "4. Combina i pesi con le attivazioni convoluzionali per ottenere la **mappa Grad-CAM**:\n",
    "   \\[\n",
    "   \\text{Grad-CAM}(x, y) = \\text{ReLU} \\left( \\sum_i \\alpha_i \\cdot A_i(x, y) \\right)\n",
    "   \\]\n",
    "5. Applica una **colormap** e sovrapponila all'immagine originale.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretazione della Heatmap\n",
    "\n",
    "**Rosso / Giallo**: Aree di **alta importanza** per la decisione del modello.\n",
    "**Blu / Nero**: Aree **poco o per nulla considerate** dal modello.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f2ede931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam_with_legend(image_path, model, preprocess, device, target_layer=\"layer4\"):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"[ERRORE] Immagine non trovata: {image_path}\")\n",
    "        return\n",
    "\n",
    "    img = crop_to_brain(img)\n",
    "    orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(orig_img)\n",
    "    input_tensor = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    features, gradients = [], []\n",
    "\n",
    "    def forward_hook(m, i, o): features.append(o)\n",
    "    def backward_hook(m, gi, go): gradients.append(go[0])\n",
    "\n",
    "    layer = dict([*model.named_modules()])[target_layer]\n",
    "    fh = layer.register_forward_hook(forward_hook)\n",
    "    bh = layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(input_tensor)\n",
    "    top_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    output[0, top_idx].backward()\n",
    "\n",
    "    grads = gradients[0]\n",
    "    fmap = features[0]\n",
    "    weights_ = grads.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = (weights_ * fmap).sum(dim=1).squeeze(0)\n",
    "    cam = torch.relu(cam)\n",
    "    cam -= cam.min()\n",
    "    cam /= cam.max()\n",
    "    cam_np = cam.detach().cpu().numpy()\n",
    "\n",
    "    heatmap = cv2.resize(cam_np, (orig_img.shape[1], orig_img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed = cv2.addWeighted(orig_img, 0.5, heatmap_color, 0.5, 0)\n",
    "\n",
    "    fh.remove(); bh.remove()\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax[0].imshow(orig_img)\n",
    "    ax[0].set_title(\"Immagine ritagliata\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(superimposed)\n",
    "    ax[1].set_title(f\"Grad‑CAM – classe idx {top_idx}\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    # Legenda\n",
    "    cmap = plt.cm.jet\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax[1], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Importanza (attivazione Grad‑CAM)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2db27f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam_with_legend1(image_path, model, preprocess, device, target_layer=\"layer4\"):\n",
    "    # Usa extract_fc_features_from_image per preprocessing e feature extraction\n",
    "    fc_vec, cropped_img, img_tensor = extract_fc_features(image_path, model, preprocess, device)\n",
    "\n",
    "    if fc_vec is None or cropped_img is None or img_tensor is None:\n",
    "        print(\"[ERRORE] Estrazione FC o preprocess fallita.\")\n",
    "        return\n",
    "\n",
    "    # Converti immagine in RGB per visualizzazione\n",
    "    orig_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    features, gradients = [], []\n",
    "\n",
    "    def forward_hook(m, i, o): features.append(o)\n",
    "    def backward_hook(m, gi, go): gradients.append(go[0])\n",
    "\n",
    "    # Hook su layer target\n",
    "    layer = dict([*model.named_modules()])[target_layer]\n",
    "    fh = layer.register_forward_hook(forward_hook)\n",
    "    bh = layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    # Forward e backward\n",
    "    output = model(img_tensor)\n",
    "    top_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    output[0, top_idx].backward()\n",
    "\n",
    "    grads = gradients[0]\n",
    "    fmap = features[0]\n",
    "    weights_ = grads.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = (weights_ * fmap).sum(dim=1).squeeze(0)\n",
    "    cam = torch.relu(cam)\n",
    "    cam -= cam.min()\n",
    "    cam /= cam.max()\n",
    "    cam_np = cam.detach().cpu().numpy()\n",
    "\n",
    "    heatmap = cv2.resize(cam_np, (orig_img.shape[1], orig_img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed = cv2.addWeighted(orig_img, 0.5, heatmap_color, 0.5, 0)\n",
    "\n",
    "    fh.remove(); bh.remove()\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax[0].imshow(orig_img)\n",
    "    ax[0].set_title(\"Immagine ritagliata\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(superimposed)\n",
    "    ax[1].set_title(f\"Grad‑CAM – classe idx {top_idx}\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    # Legenda\n",
    "    cmap = plt.cm.jet\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax[1], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Importanza (attivazione Grad‑CAM)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gradcam_with_legend1(\n",
    "    \"Part1/brain_glioma/brain_glioma_0017.jpg\",\n",
    "    model, preprocess, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3b6da18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer4_from_fc(image_path, model, preprocess, device, n_maps=6):\n",
    "    fc_vec, cropped_img, img_tensor = extract_fc_features(image_path, model, preprocess, device)\n",
    "    \n",
    "\n",
    "    if fc_vec is None or cropped_img is None or img_tensor is None:\n",
    "        return\n",
    "\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output.squeeze(0).detach().cpu())  # [C, H, W]\n",
    "\n",
    "    handle = model.layer4.register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(img_tensor)\n",
    "    handle.remove()\n",
    "\n",
    "    fmap = activations[0]\n",
    "    n_maps = min(n_maps, fmap.shape[0])\n",
    "\n",
    "    fig, axs = plt.subplots(1, n_maps, figsize=(4 * n_maps, 4))\n",
    "    for i in range(n_maps):\n",
    "        axs[i].imshow(fmap[i], cmap='viridis')\n",
    "        axs[i].set_title(f\"Map {i}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.suptitle(\"Mappe di attivazione – layer4\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf90302",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_layer4_from_fc(\n",
    "    \"Part1/brain_glioma/brain_glioma_0017.jpg\",\n",
    "    model, preprocess, device,\n",
    "    n_maps=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6584b",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94516259",
   "metadata": {},
   "source": [
    " Implementa un programma che, dato il nome di un file immagine e un valore \"k\", restituisce e visualizza le k immagini più simili in base a ciascun modello visivo - selezionerai l'appropriata misura di distanza/similarità per ciascun modello di feature.  Per ogni corrispondenza, elenca anche il corrispondente punteggio di distanza/similarità. \n",
    "\n",
    "  *Retrieval: Immagini più Simili (distanza coseno)*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cc7d97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_cosine(img_path, k):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili rispetto a una query, usando la distanza coseno.\n",
    "    \"\"\"\n",
    "    query_feature, _, _ = extract_fc_features(img_path, model, preprocess, device) \n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Impossibile estrarre feature dalla query.\")\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "    distances = cosine_distances(feat_matrix_part1, query_feature).flatten()\n",
    "\n",
    "    top_k_idx = np.argsort(distances)[:k]\n",
    "    top_k_scores = distances[top_k_idx]\n",
    "\n",
    "    print(f\"\\nTop {k} immagini più simili a: {img_path}\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        print(f\"{rank+1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Distanza: {top_k_scores[rank]:.4f}\")\n",
    "\n",
    "    # Visualizzazione\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(15, 5))\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, idx in enumerate(top_k_idx):\n",
    "        match_img = cv2.imread(os.path.join(\"Part1\", lbls_part1[idx], flname_part1[idx]))\n",
    "        axs[i+1].imshow(cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB))\n",
    "        axs[i+1].set_title(f\"Rank {i+1}\\nD={top_k_scores[i]:.4f}\")\n",
    "        axs[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test su immagine di query\n",
    "query_img = \"Part1/brain_glioma/brain_glioma_0017.jpg\"\n",
    "find_k_similar_cosine(query_img, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b1dbdb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_with_metric(img_path, k, metric=\"cosine\"):\n",
    "    query_feature, _, _ = extract_fc_features(img_path, model, preprocess, device)\n",
    "    if query_feature is None:\n",
    "        print(\"[ERRORE] Estrazione feature fallita.\")\n",
    "        return\n",
    "\n",
    "    query_feature = np.array(query_feature).reshape(1, -1)\n",
    "\n",
    "    if metric == \"cosine\":\n",
    "        distances = cosine_distances(feat_matrix_part1, query_feature).flatten()\n",
    "    elif metric == \"euclidean\":\n",
    "        distances = euclidean_distances(feat_matrix_part1, query_feature).flatten()\n",
    "    elif metric == \"manhattan\":\n",
    "        distances = manhattan_distances(feat_matrix_part1, query_feature).flatten()\n",
    "    elif metric == \"correlation\":\n",
    "        distances = np.array([correlation(query_feature[0], v) for v in feat_matrix_part1])\n",
    "    elif metric == \"mahalanobis\":\n",
    "        VI = np.linalg.inv(np.cov(feat_matrix_part1.T))\n",
    "        distances = np.array([mahalanobis(query_feature[0], v, VI) for v in feat_matrix_part1])\n",
    "    else:\n",
    "        raise ValueError(f\"[ERRORE] Metrica '{metric}' non supportata.\")\n",
    "\n",
    "    # top-k più simili\n",
    "    top_k_idx = np.argsort(distances)[:k]\n",
    "    top_k_scores = distances[top_k_idx]\n",
    "\n",
    "    # stampa risultati\n",
    "    print(f\"\\nTop {k} immagini più simili a: {img_path} usando metrica '{metric}'\")\n",
    "    for rank, idx in enumerate(top_k_idx):\n",
    "        print(f\"{rank+1}. {flname_part1[idx]} | Classe: {lbls_part1[idx]} | Distanza: {top_k_scores[rank]:.4f}\")\n",
    "\n",
    "    # visualizzazione\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(16, 4))\n",
    "    axs[0].imshow(cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB))\n",
    "    axs[0].set_title(\"Query\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, idx in enumerate(top_k_idx):\n",
    "        match_img = cv2.imread(os.path.join(\"Part1\", lbls_part1[idx], flname_part1[idx]))\n",
    "        axs[i+1].imshow(cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB))\n",
    "        axs[i+1].set_title(f\"Rank {i+1}\\nD={top_k_scores[i]:.4f}\")\n",
    "        axs[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "        # Istogramma delle distanze\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(distances, bins=30, kde=True)\n",
    "    plt.title(f\"Distribuzione delle distanze - {metric}\")\n",
    "    plt.xlabel(\"Distanza\")\n",
    "    plt.ylabel(\"Frequenza\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585af0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_k_similar_with_metric(\"Part1/brain_glioma/brain_glioma_0017.jpg\", k=5, metric=\"cosine\")\n",
    "find_k_similar_with_metric(\"Part1/brain_glioma/brain_glioma_0017.jpg\", k=5, metric=\"euclidean\")\n",
    "find_k_similar_with_metric(\"Part1/brain_glioma/brain_glioma_0017.jpg\", k=5, metric=\"mahalanobis\")\n",
    "find_k_similar_with_metric(\"Part1/brain_glioma/brain_glioma_0017.jpg\", k=5, metric=\"correlation\")\n",
    "find_k_similar_with_metric(\"Part1/brain_glioma/brain_glioma_0017.jpg\", k=5, metric=\"manhattan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11916d0b",
   "metadata": {},
   "source": [
    "# Task 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bef70",
   "metadata": {},
   "source": [
    " Implementa un programma che, dati (a) un file immagine di query della parte 2, (b) uno spazio di feature selezionato dall'utente e (c) un numero intero positivo k (k<=2), identifica ed elenca le k etichette di corrispondenza più probabili, insieme ai loro punteggi, nello spazio di feature selezionato.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f7fc4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task4_predict_labels_resnetfc(query_img_path, k, extractor_fn, features, labels, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Predict top-k labels for query image using ResNet FC features.\n",
    "    extractor_fn: funzione che prende image_path e restituisce feature numpy array\n",
    "    features: matrice numpy (N, d)\n",
    "    labels: array di stringhe (N,)\n",
    "    metric: \"euclidean\" o \"cosine\"\n",
    "    \"\"\"\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "    print(f\"\\n======== PREDIZIONE PER: {os.path.basename(query_img_path)} ========\")\n",
    "\n",
    "    def compute_metric(query_feat, target_feats, metric):\n",
    "        query_feat = query_feat.reshape(1, -1)\n",
    "        if metric == \"euclidean\":\n",
    "            # distanza euclidea tra query_feat e tutti target_feats\n",
    "            return np.linalg.norm(target_feats - query_feat, axis=1)\n",
    "        elif metric == \"cosine\":\n",
    "            # similarità coseno tra query_feat e tutti target_feats\n",
    "            # sim = (A·B) / (|A||B|), distanza = 1 - sim\n",
    "            dot_prod = np.dot(target_feats, query_feat.T).flatten()\n",
    "            norm_feats = np.linalg.norm(target_feats, axis=1)\n",
    "            norm_query = np.linalg.norm(query_feat)\n",
    "            cosine_sim = dot_prod / (norm_feats * norm_query + 1e-10)\n",
    "            cosine_dist = 1 - cosine_sim  # distanza coseno\n",
    "            return cosine_dist\n",
    "        else:\n",
    "            raise ValueError(\"Metric must be 'euclidean' or 'cosine'\")\n",
    "\n",
    "    def predict_top_k_labels_distance_mean(query_img_path, k, features, labels, metric):\n",
    "        query_feat = extractor_fn(query_img_path)\n",
    "        if query_feat is None:\n",
    "            print(\"[ERRORE] Feature non estratte.\")\n",
    "            return\n",
    "        unique_labels = np.unique(labels)\n",
    "        scores = []\n",
    "        for label in unique_labels:\n",
    "            class_feats = features[labels == label]\n",
    "            dists = compute_metric(query_feat, class_feats, metric)\n",
    "            scores.append(dists.mean())\n",
    "        if metric == \"euclidean\":\n",
    "            top_k = np.argsort(scores)[:k]\n",
    "        else:  # cosine distanza: più piccola è meglio\n",
    "            top_k = np.argsort(scores)[:k]\n",
    "        print(f\"\\n[STRATEGIA: distanza media - metrica: {metric}]\")\n",
    "        for idx in top_k:\n",
    "            print(f\"Classe: {unique_labels[idx]} | Score medio: {scores[idx]:.4f}\")\n",
    "\n",
    "    def predict_top_k_labels_prototype(query_img_path, k, features, labels, metric):\n",
    "        query_feat = extractor_fn(query_img_path)\n",
    "        if query_feat is None:\n",
    "            print(\"[ERRORE] Feature non estratte.\")\n",
    "            return\n",
    "        unique_labels = np.unique(labels)\n",
    "        prototypes = []\n",
    "        for label in unique_labels:\n",
    "            class_feats = features[labels == label]\n",
    "            prototypes.append(class_feats.mean(axis=0))\n",
    "        prototypes = np.vstack(prototypes)\n",
    "        scores = compute_metric(query_feat, prototypes, metric)\n",
    "        if metric == \"euclidean\":\n",
    "            top_k = np.argsort(scores)[:k]\n",
    "        else:\n",
    "            top_k = np.argsort(scores)[:k]\n",
    "        print(f\"\\n[STRATEGIA: prototipo di classe - metrica: {metric}]\")\n",
    "        for idx in top_k:\n",
    "            print(f\"Classe: {unique_labels[idx]} | Score: {scores[idx]:.4f}\")\n",
    "\n",
    "    predict_top_k_labels_distance_mean(query_img_path, k, features, labels, metric)\n",
    "    predict_top_k_labels_prototype(query_img_path, k, features, labels, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d388b1a",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_menin/brain_menin_1202.jpg\"\n",
    "\n",
    "# --- KNN con distanza Euclidea ---\n",
    "task4_predict_labels_resnetfc(\n",
    "    query_img,\n",
    "    k=2,\n",
    "    extractor_fn=lambda img: extract_fc_features(img, model, preprocess, device)[0],  # usa solo il primo valore\n",
    "    features=feat_matrix_part1,\n",
    "    labels=lbls_part1,\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "# --- KNN con distanza Coseno ---\n",
    "task4_predict_labels_resnetfc(\n",
    "    query_img,\n",
    "    k=2,\n",
    "    extractor_fn=lambda img: extract_fc_features(img, model, preprocess, device)[0],  # usa solo il primo valore\n",
    "    features=feat_matrix_part1,\n",
    "    labels=lbls_part1,\n",
    "    metric=\"cosine\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cfea21",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e26d6",
   "metadata": {},
   "source": [
    "Implementa un programma che (a) dato uno dei modelli di feature, (b) un valore k specificato dall'utente, (c) una delle tre tecniche di riduzione della dimensionalità (SVD, LDA, k-means) scelte dall'utente, riporta le prime k semantiche latenti estratte nello spazio di feature selezionato.    \n",
    "\n",
    "-Memorizza le semantiche latenti in un file di output adeguatamente nominato.    \n",
    "\n",
    "-Elenca le coppie imageID-peso, ordinate in ordine decrescente di pesi.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "83a11ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task5_latent_semantics_resnetfc(feature_model_path, technique, k):\n",
    "    technique = technique.lower()\n",
    "\n",
    "    if technique == \"svd\":\n",
    "        model = TruncatedSVD(n_components=k, random_state=42)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1)\n",
    "        components = model.components_\n",
    "        method = \"svd\"\n",
    "\n",
    "    elif technique == \"lda\":\n",
    "        unique_labels = np.unique(lbls_part1)\n",
    "        max_k = len(unique_labels) - 1\n",
    "        if k > max_k:\n",
    "            print(f\"[ATTENZIONE] LDA supporta al massimo {max_k} componenti con {len(unique_labels)} classi.\")\n",
    "            k = max_k\n",
    "        model = LDA(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1, lbls_part1)\n",
    "        components = model.scalings_.T[:k]\n",
    "        method = \"lda\"\n",
    "\n",
    "    elif technique == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        model.fit(feat_matrix_part1)\n",
    "        components = model.cluster_centers_\n",
    "        X_transformed = model.transform(feat_matrix_part1)\n",
    "        method = \"kmeans\"\n",
    "    else:\n",
    "        print(\"[ERRORE] Tecnica non supportata. Usa: 'svd', 'lda', 'kmeans'\")\n",
    "        return\n",
    "\n",
    "    if technique in [\"svd\", \"lda\"]:\n",
    "        plot_latent_space_2d(X_transformed, lbls_part1, technique, k)\n",
    "    elif technique == \"kmeans\":\n",
    "        plot_kmeans_clusters_2d(feat_matrix_part1, lbls_part1, k)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = f\"latent_semantics_{method}_{base_name}_k{k}.txt\"\n",
    "\n",
    "# Creazione output\n",
    "    output_dir = os.path.join(\"task5_output\", \"latent_semantics_resnetfc\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = os.path.join(output_dir, f\"tutti_i_pesi_{method}_{base_name}_k{k}.txt\")\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        # intestazione opzionale\n",
    "        header = \"ImageID | Class\" + \"\".join([f\" | Peso{i+1}\" for i in range(k)]) + \"\\n\"\n",
    "        f.write(header)\n",
    "        \n",
    "        # scegli la matrice trasformata giusta in base alla tecnica\n",
    "        if technique in [\"svd\", \"lda\"]:\n",
    "            latent_matrix = X_transformed  # shape: (n_immagini, k)\n",
    "        else:  # kmeans: X_transformed è la distanza da ciascun centroide\n",
    "            latent_matrix = -X_transformed  # invertiamo il segno per coerenza\n",
    "\n",
    "        for idx, img in enumerate(flname_part1):\n",
    "            weights = latent_matrix[idx]\n",
    "            line = f\"{img} | {lbls_part1[idx]}\" + \"\".join([f\" | {w:.4f}\" for w in weights]) + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "\n",
    "def plot_latent_space_2d(X_transformed, labels, technique, k):\n",
    "    if X_transformed.shape[1] < 2:\n",
    "        print(\"[INFO] Meno di 2 componenti: impossibile visualizzare in 2D.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=labels, palette=\"Set2\", s=80)\n",
    "    plt.title(f\"{technique.upper()} - Proiezione sulle prime 2 componenti latenti (k={k})\")\n",
    "    plt.xlabel(\"Componente 1\")\n",
    "    plt.ylabel(\"Componente 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_kmeans_clusters_2d(feature_matrix, labels, n_clusters):\n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd.fit_transform(feature_matrix)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_labels, palette='tab10', s=80, style=labels)\n",
    "    plt.title(f\"KMeans Clustering (k={n_clusters}) con proiezione SVD 2D\")\n",
    "    plt.xlabel(\"Componente Latente 1 (da SVD)\")\n",
    "    plt.ylabel(\"Componente Latente 2 (da SVD)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "task5_latent_semantics_resnetfc(\"resnetfc_part1.npz\", technique=\"svd\", k=5)\n",
    "task5_latent_semantics_resnetfc(\"resnetfc_part1.npz\", technique=\"lda\", k=2)\n",
    "task5_latent_semantics_resnetfc(\"resnetfc_part1.npz\", technique=\"kmeans\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872f36c",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbd2d0",
   "metadata": {},
   "source": [
    "\n",
    ">Task 6a:\n",
    "\n",
    " Implementa un programma che calcola e stampa la \"inherent dimensionality\" associata alle immagini della parte 1.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4073e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_intrinsic_dimensionality(feature_matrix, threshold, plot=True):\n",
    "    max_components = min(feature_matrix.shape)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(feature_matrix)\n",
    "\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "    intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "\n",
    "    #print(f\"[INFO] Spiegazione varianza per ogni componente PCA:\\n{explained}\")\n",
    "    #print(f\"[INFO] Varianza cumulativa:\\n{cumulative}\")\n",
    "    #print(f\"[INFO] Soglia impostata: {threshold}\")\n",
    "    \n",
    "\n",
    "    if threshold >= 1.0:\n",
    "        intrinsic_dim = len(cumulative)\n",
    "    else:\n",
    "        intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "    print(f\"[INFO] Feature matrix shape: {feature_matrix.shape}\")\n",
    "    print(f\"[INFO] Dimensione intrinseca stimata: {intrinsic_dim}\")\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cumulative, marker='o', label=\"Varianza cumulativa\")\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Soglia {threshold*100:.0f}%\")\n",
    "        plt.axvline(x=intrinsic_dim, color='g', linestyle='--', label=f\"k suggerito: {intrinsic_dim}\")\n",
    "        plt.xlabel(\"Numero componenti\")\n",
    "        plt.ylabel(\"Varianza cumulativa\")\n",
    "        plt.title(\"Scelta ottimale di k (PCA/SVD)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"[INFO] k ottimale suggerito (soglia {threshold*100:.0f}%): {intrinsic_dim}\")\n",
    "    return intrinsic_dim, cumulative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6dca7",
   "metadata": {},
   "source": [
    "\n",
    "> Task 6b: \n",
    "\n",
    "Implementa un programma che calcola e stampa la \"dimensionalità intrinseca\" (numero di dim indipendenti minime necassari per rappresentare set) associata a ciascuna etichetta univoca delle immagini della parte 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9585c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality_per_label(feature_matrix, labels, threshold):\n",
    "    label_dim_map = {}\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"[INFO] Etichette uniche trovate: {len(unique_labels)}\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        label_features = feature_matrix[indices]\n",
    "\n",
    "        if len(indices) < 2:\n",
    "            print(f\"[AVVISO] Label '{label}' ha meno di 2 campioni — ignorata.\")\n",
    "            continue\n",
    "\n",
    "        k, _ = estimate_intrinsic_dimensionality(label_features, threshold=threshold, plot=False)\n",
    "        label_dim_map[label] = k\n",
    "        print(f\" Label '{label}' : k = {k}\")\n",
    "\n",
    "    return label_dim_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c38a2a",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428759c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimate_intrinsic_dimensionality(feat_matrix_part1, threshold=1.00, plot=True)\n",
    "\n",
    "\n",
    "print(\"\\n Task 6b – Dimensionalità per etichetta:\\n\")\n",
    "label_dimensionalities = estimate_dimensionality_per_label(feat_matrix_part1, lbls_part1, threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c608a",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6928239",
   "metadata": {},
   "source": [
    "Implementa un programma che,per ciascuna etichetta univoca l, calcola le corrispondenti k semantiche latenti (a tua scelta) associate alle immagini della parte 1, e\n",
    "per le immagini della parte 2, prevede le etichette più probabili utilizzando distanze/similarità calcolate sotto le semantiche latenti specifiche dell'etichetta.    \n",
    "Il sistema dovrebbe anche fornire valori di precision, recall, and F1-score per etichetta, nonché un valore di accuratezza complessiva.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "63c39581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_semantics_per_class(X, y, k=10):\n",
    "    class_models = {}\n",
    "    class_means = {}\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        X_class = X[y == label]  # Prende solo le istanze della classe corrente\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_class)  # Normalizza i dati della classe\n",
    "\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        latent = svd.fit_transform(X_scaled)  # Riduzione dimensionale con SVD\n",
    "\n",
    "        # Salva modello SVD e scaler per la classe\n",
    "        class_models[label] = {\n",
    "            'svd': svd,\n",
    "            'scaler': scaler,\n",
    "            'latent_vectors': latent\n",
    "        }\n",
    "        # Calcola la media dei vettori latenti della classe\n",
    "        class_means[label] = np.mean(latent, axis=0)\n",
    "    return class_models, class_means\n",
    "\n",
    "def predict_label(X_test, class_models, class_means):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        best_label = None\n",
    "        min_dist = float('inf')\n",
    "        for label, model in class_models.items():\n",
    "            x_scaled = model['scaler'].transform(x.reshape(1, -1))  # Normalizza x\n",
    "            x_latent = model['svd'].transform(x_scaled)  # Trasforma in spazio latente\n",
    "            dist = np.linalg.norm(x_latent - class_means[label])  # Distanza dal centroide\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_label = label\n",
    "        y_pred.append(best_label)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    labels = np.unique(y_true)\n",
    "    print(\"Per-class metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\n",
    "            f\"Class {label}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f}\")\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    print(\"[VALUTAZIONE] Report di classificazione:\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bb0d64",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c23056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento sui dati di Part1\n",
    "class_models, class_means = compute_latent_semantics_per_class(\n",
    "    feat_matrix_part1, lbls_part1, k=10)\n",
    "\n",
    "# Predizione su Part2\n",
    "predicted_labels = predict_label(feat_matrix_part2, class_models, class_means)\n",
    "\n",
    "# Valutazione\n",
    "evaluate(lbls_part2, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606d712",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57a583",
   "metadata": {},
   "source": [
    "Implementa un programma che, per ciascuna etichetta univoca l, calcola i corrispondenti c cluster più significativi associati alle immagini della parte 1 (utilizzando l'algoritmo DBScan);\n",
    "i cluster risultanti devono essere visualizzati sia\n",
    "come nuvole di punti colorate in modo diverso in uno spazio MDS a 2 dimensioni, sia\n",
    "come gruppi di miniature di immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9ceba398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applichiamo tecniche di dimensionality reduction\n",
    "def reduce_features(features, method, n_components, random_state=42):\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=n_components)\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Metodo di riduzione '{method}' non supportato.\")\n",
    "\n",
    "    return reducer.fit_transform(features)\n",
    "\n",
    "#Applichiamo una tecnica di riduzione mediante reduce_feature alle features di partenza ottenendo la lista di Feature Latenti\n",
    "#Applichiamo su insieme di feature latenti StandardScaler per cercare di ottenere migliori cluster tramite DBSCAN\n",
    "\n",
    "def apply_dbscan_with_pca(features, eps, min_samples, n_components, method):\n",
    "    print(f\"Applicazione di {method} -> Riduzione a {n_components} componenti\")\n",
    "    reduced_features = reduce_features(features, method, n_components=n_components)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    reduced_scaled = scaler.fit_transform(reduced_features)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(reduced_scaled)\n",
    "    return labels\n",
    "\n",
    "#Calcola i 'c' cluster di maggior cardinalità\n",
    "def top_c_clusters(cluster_labels, c):\n",
    "    label_counts = Counter(cluster_labels)\n",
    "    label_counts.pop(-1, None) # rimozione cluster catalogato come rumore (-1)\n",
    "    if not label_counts:\n",
    "        print(\"[WARN] DBSCAN non ha trovato alcun cluster valido (solo rumore).\")\n",
    "        return []\n",
    "    \n",
    "    # Estraiamo i 'c' cluster più frequenti\n",
    "    most_common = label_counts.most_common(c)\n",
    "    top = [int(lbl) for lbl, _ in most_common]\n",
    "    \n",
    "    if len(top) < c:\n",
    "        print(f\"[WARN] DBSCAN ha trovato solo {len(top)} cluster (meno di {c}).\")\n",
    "    return top\n",
    "\n",
    "\n",
    "#Applichiamo al risultato di DBSCAN l'algoritmo di MDS\n",
    "def plot_mds_clusters(features, cluster_labels, top_clusters, metric):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    #Generazione di una nuova matrice basata sul parametro metric (es. 'cosine')\n",
    "    D = pairwise_distances(features_scaled, metric=metric)\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42, n_init=4) \n",
    "    Y = mds.fit_transform(D)\n",
    "\n",
    "    cmap= matplotlib.colormaps['tab10']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(Y)):\n",
    "        lbl = cluster_labels[i]\n",
    "        if lbl in top_clusters:\n",
    "            color_idx = top_clusters.index(lbl)\n",
    "            plt.scatter(Y[i,0], Y[i,1], color=cmap(color_idx), s=30, edgecolor='k', linewidth=0.2)\n",
    "        else:\n",
    "            # punti rumore o cluster “non top”\n",
    "            plt.scatter(Y[i,0], Y[i,1], color='lightgray', s=8)\n",
    "    \n",
    "    plt.title(f\"MDS 2D - Top {len(top_clusters)} cluster\")\n",
    "    plt.xlabel(\"MDS 1\")\n",
    "    plt.ylabel(\"MDS 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Genera blocco di immagini trovate per cluster\n",
    "def show_cluster_thumbnails(images, cluster_labels, top_clusters, thumb_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    images: lista (o array) di percorsi file (lunghezza N), \n",
    "            ossia a images[i] corrisponde features[i].\n",
    "    cluster_labels: array (N,) di cluster per ogni immagine.\n",
    "    top_clusters: lista dei c cluster (int) che vogliamo visualizzare.\n",
    "    thumb_size: dimensione (w,h) di ogni miniatura.\n",
    "    Per ogni cluster ∈ top_clusters stampa a video (o fa plt.show) \n",
    "    una griglia di miniature (fino a ~16-25 alla volta).\n",
    "    \"\"\"\n",
    "    for cluster_id in top_clusters:\n",
    "        # Indici di tutte le immagini che appartengono a questo cluster\n",
    "        idxs = [i for i, cl in enumerate(cluster_labels) if cl == cluster_id]\n",
    "        print(f\"[INFO] Cluster {cluster_id}: {len(idxs)} immagini trovate\")\n",
    "\n",
    "        # Se vogliamo limitare a N miniatura per cluster (tipo 16):\n",
    "        max_display = min(len(idxs), 16)\n",
    "        n = int(np.ceil(np.sqrt(max_display)))  # facciamo una griglia n×n\n",
    "        plt.figure(figsize=(n, n))\n",
    "\n",
    "        for j, i_img in enumerate(idxs[:max_display]):\n",
    "            img = Image.open(images[i_img]).convert('RGB')\n",
    "            img_thumb = img.resize(thumb_size, Image.LANCZOS)\n",
    "            \n",
    "            ax = plt.subplot(n, n, j+1)\n",
    "            plt.imshow(img_thumb)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Cluster {cluster_id} – {len(idxs)} immagini (mostrate: {max_display})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ce6f6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_scan_detection(eps, min_samples, n_components, c, method):\n",
    "    \n",
    "    base_folder = \"Part1\" \n",
    "    images_full = [os.path.join(base_folder, lbl, fname) for fname, lbl in zip(flname_part1, lbls_part1)]\n",
    "\n",
    "    # Scorre ogni label di Parte1 ed applico DBSCAN+PCA\n",
    "    unique_labels = np.unique(lbls_part1)  # es. [\"Glioma\",\"Meningioma\",\"Pituitary\"]\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"[INFO] Elaboro label: {lbl}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        #Estrae le righe di feat_matrix_part1 / flname_part1 corrispondenti\n",
    "        mask_lbl = (lbls_part1 == lbl)\n",
    "        features_label = feat_matrix_part1[mask_lbl]   # shape = (n_i, d)\n",
    "        images_label = np.array(images_full)[mask_lbl]\n",
    "\n",
    "        #Chiama la tua funzione PCA + DBSCAN\n",
    "        cluster_labels = apply_dbscan_with_pca(\n",
    "            features_label,\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            n_components=n_components,\n",
    "            method = method\n",
    "        )\n",
    "        print(f\"[INFO] Cluster-labels trovati: {np.unique(cluster_labels)}\")\n",
    "\n",
    "        # Trova i c cluster più grandi\n",
    "        top_clusters = top_c_clusters(cluster_labels, c)\n",
    "        print(f\"[INFO] Top {c} cluster (per dimensione): {top_clusters}\")\n",
    "\n",
    "        # MDS‐2D + scatter plot del clustering\n",
    "        print(f\"[INFO] Disegno MDS 2D per i cluster di '{lbl}' …\")\n",
    "\n",
    "        plot_mds_clusters(\n",
    "            features_label,\n",
    "            cluster_labels,\n",
    "            top_clusters,\n",
    "            metric='cosine'\n",
    "        )\n",
    "\n",
    "        # Creo le miniature di ogni cluster “significativo”\n",
    "        print(f\"[INFO] Genero miniature per ciascun cluster di '{lbl}' …\")\n",
    "        show_cluster_thumbnails(\n",
    "            images_label,      # array di stringhe di percorsi\n",
    "            cluster_labels,    # array di int di lunghezza n_i\n",
    "            top_clusters,      # la lista dei c indici di cluster\n",
    "            thumb_size=(64, 64)\n",
    "        )\n",
    "\n",
    "    print(\"\\n[FINITO] Task 8 completato per tutte le label di Parte1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba23bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 5.0            # valore DBSCAN di esempio\n",
    "min_samples = 3      # valore DBSCAN di esempio\n",
    "n_components = 50    # quante dimensioni tenere con PCA PRIMA di DBSCAN\n",
    "c = 3                # quanti cluster “significativi” voglio prendere per ciascuna label\n",
    "method = 'pca'      # umap or pca\n",
    "db_scan_detection(eps, min_samples, n_components, c, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def0437",
   "metadata": {},
   "source": [
    "# Task 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266c9b0",
   "metadata": {},
   "source": [
    "Implementa un programma che, dati le immagini della parte 1:\n",
    "\n",
    "*crea un classificatore m-NN (per una m specificata dall'utente),\n",
    "\n",
    "*crea un classificatore ad albero decisionale,\n",
    "Per questo task, puoi utilizzare lo spazio delle feature di tua scelta.    \n",
    "\n",
    "Per le immagini della parte 2, prevede le etichette più probabili utilizzando il classificatore selezionato dall'utente.\n",
    "Il sistema dovrebbe anche fornire valori di precisione, richiamo e punteggio F1 per etichetta, nonché un valore di accuratezza complessiva.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bdcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Imposta il valore di m per l'm-NN\n",
    "m = 5  # Modifica questo valore in base alle tue necessità\n",
    "\n",
    "# Addestramento m-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=m)\n",
    "knn_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Addestramento Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Predizioni su Part2\n",
    "pred_knn = knn_model.predict(feat_matrix_part2)\n",
    "pred_dt = dt_model.predict(feat_matrix_part2)\n",
    "\n",
    "# Valutazione m-NN\n",
    "print(\"Risultati m-NN:\")\n",
    "print(classification_report(lbls_part2, pred_knn))\n",
    "print(\"Accuratezza complessiva m-NN:\", accuracy_score(lbls_part2, pred_knn))\n",
    "\n",
    "# Valutazione Decision Tree\n",
    "print(\"Risultati Decision Tree:\")\n",
    "print(classification_report(lbls_part2, pred_dt))\n",
    "print(\"Accuratezza complessiva Decision Tree:\", accuracy_score(lbls_part2, pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e18743",
   "metadata": {},
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed04aa",
   "metadata": {},
   "source": [
    "> 10a:\n",
    "\n",
    " Implementa uno strumento di Locality Sensitive Hashing (LSH) (per la distanza euclidea) che prende come input (a) il numero di livelli, L, (b) il numero di hash per livello, h, e (c) un insieme di vettori come input e crea una struttura di indice in memoria contenente l'insieme di vettori dato. \n",
    "\n",
    "Vedi:\n",
    "\"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\" (di Alexandr Andoni e Piotr Indyk). Communications of the ACM, vol. 51, no. 1, 2008, pp. 117-122.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ce167b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe LSH con quantizzazione\n",
    "\n",
    "class LSH_EuclideanQuantized:\n",
    "    \"\"\"\n",
    "    LSH per distanza Euclidea (p-stable) con bucket width r.\n",
    "    Ogni hash h_j(v) = floor((a_j · v + b_j) / r).\n",
    "\n",
    "    Parametri:\n",
    "      - num_layers   = L = numero di tavole hash\n",
    "      - num_hashes   = h = numero di functions concatenati in ciascuna tavola\n",
    "      - dim          = D = dimensione dei vettori di input\n",
    "      - r            = bucket width (parte intera di quantizzazione)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_hashes: int, dim: int, r: float):\n",
    "        self.L = num_layers\n",
    "        self.h = num_hashes\n",
    "        self.d = dim\n",
    "        self.r = r\n",
    "\n",
    "        # Prepara L tavole hash: ciascuna è un dict (chiave tuple di h interi -> lista di indici)\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.L)]\n",
    "\n",
    "        # Per ogni layer l=0..L-1, e per ogni j=0..h-1, genero:\n",
    "        #   - a_lj  vettore gaussiano di dimensione D\n",
    "        #   - b_lj  offset (uniforme in [0, r) )\n",
    "        self.a_vectors = [\n",
    "            [np.random.randn(self.d) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.b_offsets = [\n",
    "            [np.random.uniform(0, self.r) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "\n",
    "        # Memorizzerò i vettori originali di Part1 in questo array, shape=(N, D)\n",
    "        self.data_vectors = None\n",
    "\n",
    "    def _compute_hash_tuple(self, vec: np.ndarray, layer_idx: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Calcola l'hash (h interi) per il layer layer_idx su un vettore vec:\n",
    "          h_j = floor((a_vectors[layer_idx][j] · vec + b_offsets[layer_idx][j]) / r)\n",
    "        Ritorna una tupla di h interi.\n",
    "        \"\"\"\n",
    "        keys = []\n",
    "        a_vs = self.a_vectors[layer_idx]\n",
    "        b_os = self.b_offsets[layer_idx]\n",
    "        for j in range(self.h):\n",
    "            a_j = a_vs[j]         # vettore dimensione D\n",
    "            b_j = b_os[j]         # float in [0, r)\n",
    "            proj = float(np.dot(a_j, vec) + b_j)\n",
    "            h_val = int(np.floor(proj / self.r))\n",
    "            keys.append(h_val)\n",
    "        return tuple(keys)\n",
    "\n",
    "    def index(self, vectors: np.ndarray):\n",
    "        \"\"\"\n",
    "        Costruisci l'indice LSH su un insieme di vettori di Part1:\n",
    "          vectors: numpy array shape = (N, D)\n",
    "        Al termine di questa chiamata:\n",
    "          - self.data_vectors = vectors\n",
    "          - self.hash_tables[l][hash_tuple] conterrà la lista di indici i per cui\n",
    "            hash_tuple = _compute_hash_tuple(vectors[i], l).\n",
    "        \"\"\"\n",
    "        self.data_vectors = vectors\n",
    "        N, D = vectors.shape\n",
    "        assert D == self.d, f\"Errore: dimensione vettore ({D}) ≠ atteso ({self.d}).\"\n",
    "\n",
    "        # Inserisco ogni vettore in ciascuna tavola hash\n",
    "        for idx in range(N):\n",
    "            v = vectors[idx]\n",
    "            for l in range(self.L):\n",
    "                key = self._compute_hash_tuple(v, l)\n",
    "                self.hash_tables[l][key].append(idx)\n",
    "\n",
    "    def query(self, q_vec: np.ndarray, top_t: int = 5):\n",
    "        \"\"\"\n",
    "        Esegui una query LSH per cercare i top_t vettori più vicini a q_vec.\n",
    "        Restituisce:\n",
    "          - top_results: lista di tuple (indice, distanza) ord. per dist. crescente\n",
    "          - unique_count: numero di indici distinti considerati (cardinalità dei candidati)\n",
    "          - total_checked: somma della lunghezza di tutti i bucket esaminati\n",
    "        \"\"\"\n",
    "        assert q_vec.shape[0] == self.d, \"Errore: dimensione query ≠ D.\"\n",
    "        candidati = set()\n",
    "        total_checked = 0\n",
    "\n",
    "        # Per ciascun layer, ottengo la chiave polidimensionale e i suoi bucket\n",
    "        for l in range(self.L):\n",
    "            h_tuple = self._compute_hash_tuple(q_vec, l)\n",
    "            bucket = self.hash_tables[l].get(h_tuple, [])\n",
    "            total_checked += len(bucket)\n",
    "            candidati.update(bucket)\n",
    "\n",
    "        # Ora calcolo la distanza euclidea esatta tra q_vec e ciascun candidato\n",
    "        risultati = []\n",
    "        for idx in candidati:\n",
    "            v_i = self.data_vectors[idx]\n",
    "            dist = np.linalg.norm(v_i - q_vec)\n",
    "            risultati.append((idx, dist))\n",
    "\n",
    "        # Ordino e prendo i primi top_t\n",
    "        risultati.sort(key=lambda x: x[1])\n",
    "        top_results = risultati[:top_t]\n",
    "        return top_results, len(candidati), total_checked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99704571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costruzione LSH_EuclideanQuantized su Part1\n",
    "\n",
    "# 1) (Opzionale ma consigliato) centra e normalizza i vettori di Part1\n",
    "#    Questo passaggio riduce l'effetto di scale diverse e spesso migliora la qualità LSH\n",
    "mean_vec = np.mean(feat_matrix_part1, axis=0)\n",
    "print(\"mean_vec.shape =\", mean_vec.shape)\n",
    "feat_centered = feat_matrix_part1 - mean_vec\n",
    "feat_normed = normalize(feat_centered, norm='l2', axis=1)\n",
    "\n",
    "# 2) Parametri LSH\n",
    "D = feat_normed.shape[1]      # di solito 900\n",
    "L = 10                         # numero di tavole hash (scegli in base a esperimenti)\n",
    "h = 7                        # numero di funzioni concatenati in ciascuna tavola\n",
    "r = 1                      # parametro di larghezza (esempio: 0.5); puoi sperimentare\n",
    "\n",
    "# 3) Creo l'oggetto e indicizzo\n",
    "lsh_quant = LSH_EuclideanQuantized(num_layers=L, num_hashes=h, dim=D, r=r)\n",
    "lsh_quant.index(feat_normed)\n",
    "\n",
    "print(f\"[INFO] LSH quantizzato costruito: D={D}, L={L}, h={h}, r={r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb6903",
   "metadata": {},
   "source": [
    "> 10b:\n",
    "\n",
    " Implementa un algoritmo di ricerca di immagini simili utilizzando questa struttura di indice che memorizza le immagini della parte 1 e un modello visivo di tua scelta (il modello visivo combinato deve avere almeno 256 dimensioni): per una data immagine di query e un numero intero t, \n",
    " \n",
    "*visualizza le t immagini più simili,\n",
    "*fornisce il numero di immagini univoche e il numero complessivo di immagini considerate durante il processo.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a2c91f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_similar_lsh_quant(base_folder: str, img_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Trova le k immagini più simili a img_path (di Part2) usando lsh_quant costruito su Part1.\n",
    "    \"\"\"\n",
    "    # 1) Estrai feature raw (900-dim)\n",
    "    raw_q, _, _ = extract_fc_features(img_path, model, preprocess, device)\n",
    "    raw_q = np.array(raw_q, dtype=np.float32)\n",
    "\n",
    "    # 2) Center + normalize (stesso mean_vec usato su Part1)\n",
    "    print(\"raw_q.shape =\", raw_q.shape)        # ad esempio (1024,)\n",
    "    print(\"mean_vec.shape =\", mean_vec.shape)  # ad esempio (900,) o (2048,) o altro\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # 3) Chiamata LSH\n",
    "    top_results, unique_count, total_checked = lsh_quant.query(q_normed, top_t=k)\n",
    "\n",
    "    # 4) Stampo i risultati testuali\n",
    "    print(f\"\\n[LSH-Quant] Top {k} simili a: {img_path}\")\n",
    "    for rank, (idx, dist) in enumerate(top_results, start=1):\n",
    "        label = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        print(f\"  {rank}. {fname} | Classe: {label} | Distanza Euclidea: {dist:.2f}\")\n",
    "    print(f\"[LSH-Quant] Immagini uniche considerate: {unique_count}\")\n",
    "    print(f\"[LSH-Quant] Immagini totali controllate: {total_checked}\")\n",
    "\n",
    "    # 5) Visualizzazione (query + k risultati)\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(4*(k+1), 4))\n",
    "    img_q = cv2.imread(img_path)\n",
    "    img_q = cv2.cvtColor(img_q, cv2.COLOR_BGR2RGB)\n",
    "    axs[0].imshow(img_q)\n",
    "    axs[0].set_title(\"Query (LSH-Quant)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, (idx, dist) in enumerate(top_results, start=1):\n",
    "        lab = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        full_path = os.path.join(base_folder, lab, fname)\n",
    "        img_match = cv2.imread(full_path)\n",
    "        img_match = cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img_match)\n",
    "        axs[i].set_title(f\"Rank {i}\\nd={dist:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3730b46",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d442ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di utilizzo su un'immagine di Part2 ---\n",
    "query_path = \"Part2/brain_glioma/brain_glioma_1410.jpg\"\n",
    "\n",
    "k = 3                         # numero di immagini simili da visualizzare\n",
    "\n",
    "# Eseguo la ricerca LSH\n",
    "find_k_similar_lsh_quant(\"Part1\", query_path, k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
