{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b893d58e",
   "metadata": {},
   "source": [
    "### Feature Model: ResNet50 - Layer3 - 1024\n",
    "\n",
    "In questo progetto viene utilizzata una rete neurale pre-addestrata **ResNet50** per estrarre caratteristiche (features) significative dalle immagini. In particolare, si fa uso di un **layer intermedio** della rete, denominato `layer3`.\n",
    "\n",
    "1. **Ridimensionamento dell'immagine**\n",
    "   - Ogni immagine viene **ridimensionata a 224×224 pixel**, come richiesto dalla struttura di ResNet50.\n",
    "\n",
    "2. **Hook sul \"layer3\"**\n",
    "   - Si applica un **hook** sull'output del livello `layer3` della ResNet50. Questo layer produce un tensore tridimensionale di forma:\n",
    "     - 1024 modalità (feature maps)\n",
    "     - ciascun canale ha dimensioni spaziali 14×14\n",
    "     - Queste feature rappresentano caratteristiche visive di medio livello, come texture, bordi complessi e forme astratte.\n",
    "\n",
    "3. **Global Average Pooling**\n",
    "   -Per convertire il tensore [1024, 14, 14] in un vettore compatto, si applica un **Global Average Pooling**, ovvero: [1024, 14, 14]\n",
    "   - si calcola la **media** dei valori all'interno di ciascun canale (mappa 14×14)\n",
    "   - si ottiene così un vettore di **1024 dimensioni**, uno per ogni canale: Feature vector ∈ ℝ¹⁰²⁴\n",
    "\n",
    "**Perché questo approccio?**\n",
    "- Il layer `layer3` è sufficientemente profondo da catturare informazioni semantiche utili, ma non troppo da perdere generalità.\n",
    "- Il vettore da 1024 dimensioni è un buon compromesso tra **ricchezza informativa** e **dimensione gestibile**.\n",
    "- Queste feature sono adatte per task come:\n",
    "  - Estrazione di concetti latenti (es. SVD, PCA)\n",
    "  - Clustering (es. KMeans)\n",
    "  - Similarità e image retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac94653",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.stats import skew\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, pairwise_distances\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except ImportError:\n",
    "    print(\"[WARNING] UMAP non disponibile\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e777b2c",
   "metadata": {},
   "source": [
    "### Setup del Modello ResNet50\n",
    "\n",
    "In questo blocco inizializziamo il modello ResNet50 pre-addestrato su ImageNet. Il modello viene caricato in modalità valutazione (`eval`) e spostato su GPU se disponibile, altrimenti su CPU. Inoltre, viene definito il preprocessing standard usato per l'inferenza, basato sui parametri di normalizzazione ImageNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se CUDA disponibile, usa la GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carica ResNet50 pre-addestrata e in modalità eval\n",
    "weights = ResNet50_Weights.IMAGENET1K_V1  # o DEFAULT per i pesi più aggiornati\n",
    "model = models.resnet50(weights=weights)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Preprocessing standard per ResNet\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046eb31e",
   "metadata": {},
   "source": [
    "### Preprocessing delle Immagini\n",
    "\n",
    "Questa funzione carica un'immagine da un percorso specifico e applica le trasformazioni richieste per essere compatibile con ResNet50. In particolare, le immagini vengono ridimensionate, ritagliate al centro e normalizzate con le statistiche di ImageNet. Il risultato è un tensore PyTorch pronto per l’inferenza.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per caricare l'immagine e fare la pre-elaborazione\n",
    "def preprocess_image(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883614c",
   "metadata": {},
   "source": [
    "### Estrazione delle Feature da un’Immagine\n",
    "\n",
    "Questa funzione combina il preprocessing e l’estrazione delle feature. Una volta preprocessata, l’immagine viene passata attraverso il modello, e viene calcolata la media delle attivazioni spaziali su ogni canale. Questo produce un vettore di feature compatto, che rappresenta l'immagine in uno spazio semantico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68078302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per estrarre la media su ciascuna mappa di attivazione\n",
    "def extract_and_process_image(img_path, model):\n",
    "    img_tensor = preprocess_image(img_path)\n",
    "    features = extract_resnet_features(img_tensor, model)\n",
    "    features_vector = features.detach().mean(dim=[1, 2]).cpu().numpy()\n",
    "    return features_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a82fd4",
   "metadata": {},
   "source": [
    "### Caricamento delle Feature Estratte\n",
    "\n",
    "Questa funzione carica da disco i vettori di feature, le etichette e i nomi dei file associati, precedentemente salvati in un file `.npz`. Vengono caricati due insiemi di dati, corrispondenti a Part1 e Part2. Questi saranno utilizzati per analisi comparative nei task successivi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741cedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(npz_path):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    return data['features'], data['labels'], data.get('filenames', [f\"img_{i}\" for i in range(len(data['features']))])\n",
    "\n",
    "feat_matrix_part2, lbls_part2, flname_part2 = load_features(\"resnet_features_part2.npz\")\n",
    "feat_matrix_part1, lbls_part1, flname_part1 = load_features(\"resnet_features_part1.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc538df",
   "metadata": {},
   "source": [
    "## Task 1-2\n",
    "Implementa un programma che estrae e memorizza i descrittori di feature per tutte le immagini nel set di dati  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dce16f",
   "metadata": {},
   "source": [
    "#### Estrazione delle Feature dal Livello `layer3` della ResNet50\n",
    "Questa funzione estrae le feature intermedie da un'immagine passata al modello, utilizzando un hook di PyTorch sul primo modulo del blocco `layer3` di ResNet50. \n",
    "\n",
    "#### Obiettivo\n",
    "Intercettare l'output del **primo blocco residuo** di `layer3` (ossia `layer3[0]`) per ottenere un tensore tridimensionale con forma `[1024, 14, 14]`, rappresentante le feature visive di medio livello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebafc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per estrarre le caratteristiche dal livello \"layer3\" della ResNet\n",
    "def extract_resnet_features(img_tensor, model):\n",
    "    def hook_fn(module, input, output):\n",
    "        hook_fn.features = output\n",
    "\n",
    "    hook = model.layer3[0].register_forward_hook(hook_fn)\n",
    "    model(img_tensor)\n",
    "    hook.remove()\n",
    "\n",
    "    features = hook_fn.features.squeeze()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c286f",
   "metadata": {},
   "source": [
    "### Estrazione e Salvataggio delle Feature da Cartelle di Immagini\n",
    "\n",
    "Questa funzione processa immagini distribuite in diverse sottocartelle all’interno di una directory base. Per ogni immagine valida, estrae il vettore di feature utilizzando la pipeline definita in `extract_and_process_image`. \n",
    "\n",
    "I vettori di feature, insieme ai nomi dei file e alle etichette (nomi delle sottocartelle), vengono salvati in un file `.npz` per un utilizzo efficiente nei task successivi.\n",
    "\n",
    "La funzione fornisce messaggi di log per monitorare il progresso e segnala eventuali errori o cartelle mancanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_features(base_folder, subfolders, output_file):\n",
    "    all_features = []\n",
    "    all_filenames = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label in subfolders:\n",
    "        folder_path = os.path.join(base_folder, label)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"[ATTENZIONE] Cartella non trovata: {folder_path}\")\n",
    "            continue\n",
    "        print(f\"[INFO] Elaboro cartella: {label}\")\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):\n",
    "                img_path = os.path.join(folder_path, filename)\n",
    "                features = extract_and_process_image(img_path, model)\n",
    "                if features is not None:\n",
    "                    all_features.append(features)\n",
    "                    all_filenames.append(filename)\n",
    "                    all_labels.append(label)\n",
    "                else:\n",
    "                    print(f\"[ERRORE] Feature non estratte da {img_path}\")\n",
    "\n",
    "    # Salva in file .npz\n",
    "    np.savez(output_file,\n",
    "             features=np.array(all_features),\n",
    "             filenames=np.array(all_filenames),\n",
    "             labels=np.array(all_labels))\n",
    "    \n",
    "    print(f\"[SALVATO] Features salvate in {output_file}\")\n",
    "    print(f\"[FINE] Totale immagini processate: {len(all_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697da27",
   "metadata": {},
   "source": [
    "Memorizzazione "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbf7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri cartelle e output\n",
    "subfolders = [\"brain_glioma\", \"brain_menin\", \"brain_tumor\"]\n",
    "\n",
    "# Estrazione e salvataggio\n",
    "process_and_save_features(\"Part1\", subfolders, \"resnet_features_part1\")\n",
    "process_and_save_features(\"Part2\", subfolders, \"resnet_features_part2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a3725",
   "metadata": {},
   "source": [
    "# Task 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b76b2",
   "metadata": {},
   "source": [
    " Implementa un programma che, dato il nome di un file immagine e un valore \"k\", restituisce e visualizza le k immagini più simili in base a ciascun modello visivo - selezionerai l'appropriata misura di distanza/similarità per ciascun modello di feature.  Per ogni corrispondenza, elenca anche il corrispondente punteggio di distanza/similarità. \n",
    "\n",
    "  *Retrieval: Immagini più Simili (distanza coseno & eclidea)* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ffb2f4",
   "metadata": {},
   "source": [
    "### Metodo\n",
    "\n",
    "1. **Estrazione delle feature**\n",
    "   - Le feature dell’immagine di query vengono estratte usando il modello `ResNet50-layer3`, già descritto nel Task precedente.\n",
    "   - Anche le feature del dataset di immagini (già estratte in precedenza) vengono caricate.\n",
    "\n",
    "2. **Scelta della metrica**\n",
    "   - Poiché lo spazio delle feature è vettoriale, si utilizza una **metrica di distanza** adatta:\n",
    "     - In questo caso, si usa la **distanza euclidea** oppure la **cosine similarity** per confrontare i vettori.\n",
    "\n",
    "3. **Calcolo delle similarità**\n",
    "   - Per ogni immagine nel dataset, si calcola la distanza (o similarità) rispetto alla query.\n",
    "   - Si ordinano le immagini in base alla distanza crescente (o similarità decrescente).\n",
    "   - Si selezionano le top-`k`.\n",
    "\n",
    "4. **Visualizzazione**\n",
    "   - Si mostra l’immagine di query seguita dalle `k` immagini più simili.\n",
    "   - Sotto ogni immagine simile viene indicato il relativo **punteggio di similarità/distanza**.\n",
    "\n",
    "---\n",
    "\n",
    "### Output previsto\n",
    "\n",
    "- Una riga di immagini:\n",
    "  - La prima è la **query**\n",
    "  - Le successive sono le **k immagini più simili**\n",
    "- Sotto ogni immagine simile compare la distanza/similarità\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451d72f",
   "metadata": {},
   "source": [
    "### Funzione di Retrieval: Trovare le \\(k\\) Immagini Più Simili\n",
    "\n",
    "Questa funzione implementa il sistema di retrieval delle immagini più simili rispetto a una immagine di query, utilizzando due diverse metriche di confronto:\n",
    "\n",
    "- **Distanza Euclidea**: misura la distanza geometrica tra il vettore di feature della query e quelli del dataset. Immagini con valori di distanza minori sono considerate più simili.\n",
    "- **Similarità Coseno**: misura l’angolo tra i vettori, indicando quanto sono orientati nella stessa direzione. Valori più alti indicano una maggiore similarità.\n",
    "\n",
    "La funzione estrae la feature dell’immagine query tramite il modello ResNet, calcola la distanza o similarità con tutte le feature del dataset, ordina i risultati e visualizza le prime \\(k\\) immagini più simili, con i rispettivi punteggi.\n",
    "\n",
    "Il confronto è effettuato sulle feature pre-calcolate contenute nel file `.npz`, e la visualizzazione mostra anche i nomi dei file e i valori di distanza o similarità.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_similar(query_img_path, k, model, image_folder, metric):\n",
    "\n",
    "    # Estrai feature per immagine query\n",
    "    query_feature = extract_and_process_image(query_img_path, model).reshape(1, -1)\n",
    "\n",
    "    # Calcola similarità/distanza\n",
    "    if metric == \"euclidean\":\n",
    "        res = np.linalg.norm(feat_matrix_part1 - query_feature, axis=1)\n",
    "        bad_value = np.inf  # penalizza immagini identiche\n",
    "    elif metric == \"cosine\":\n",
    "        res = cosine_similarity(query_feature, feat_matrix_part1)[0]\n",
    "        bad_value = -np.inf  # penalizza immagini identiche\n",
    "    else:\n",
    "        raise ValueError(\"Metric must be 'euclidean' or 'cosine'\")\n",
    "\n",
    "    # Escludi la query se è nel dataset\n",
    "    query_filename = os.path.basename(query_img_path)\n",
    "    query_label = os.path.basename(os.path.dirname(query_img_path))\n",
    "    for i in range(len(flname_part1)):\n",
    "        if flname_part1[i] == query_filename and lbls_part1[i] == query_label:\n",
    "            res[i] = bad_value\n",
    "            break\n",
    "\n",
    "    # Ordina in base alla metrica\n",
    "    if metric == \"euclidean\":\n",
    "        top_k_indices = np.argsort(res)[:k]  # più vicino = meglio\n",
    "    else:  # cosine\n",
    "        top_k_indices = np.argsort(res)[::-1][:k]  # più simile = meglio\n",
    "\n",
    "    # Visualizzazione\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, idx in enumerate(top_k_indices):\n",
    "        img_path = os.path.join(\n",
    "            image_folder, lbls_part1[idx], flname_part1[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        plt.subplot(1, k, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        if metric == \"euclidean\":\n",
    "            plt.title(f\"{flname_part1[idx]}\\nDist: {res[idx]:.4f}\")\n",
    "        else:\n",
    "            plt.title(f\"{flname_part1[idx]}\\nSim: {res[idx]:.4f}\")\n",
    "\n",
    "    plt.suptitle(f\"Top-{k} immagini più simili ({metric})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371043f",
   "metadata": {},
   "source": [
    "### Esecuzione del Retrieval con Parametri Specifici\n",
    "\n",
    "In questa sezione impostiamo i parametri per eseguire il retrieval delle immagini simili:\n",
    "\n",
    "- **Immagine di Query**: `brain_tumor_0001.jpg`, appartenente alla categoria `brain_tumor`.\n",
    "- **k = 5**: numero di immagini simili da recuperare.\n",
    "- **Cartella Immagini**: `Part1`, la directory base contenente le immagini organizzate in sottocartelle per categoria.\n",
    "\n",
    "Viene poi effettuata la chiamata alla funzione `find_top_k_similar`, utilizzando la **distanza euclidea** oppure la **cosine similarity**.\n",
    "\n",
    "Questa sezione consente di confrontare visivamente i risultati ottenuti con le due diverse metriche, valutando quale produce immagini semanticamente più simili rispetto alla query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35317c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_path = \"Part1/brain_tumor/brain_tumor_0001.jpg\"\n",
    "k = 5\n",
    "base_folder = \"Part1\"\n",
    "\n",
    "find_top_k_similar(query_image_path, k, model, base_folder, metric=\"euclidean\") # Distanza Euclidea\n",
    "find_top_k_similar(query_image_path, k, model, base_folder, metric=\"cosine\") # Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b390b2",
   "metadata": {},
   "source": [
    "# Task 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ba9c3",
   "metadata": {},
   "source": [
    "Implementa un programma che, dati (a) un file immagine di query della parte 2, (b) uno spazio di feature selezionato dall'utente e (c) un numero intero positivo k (k<=2), identifica ed elenca le k etichette di corrispondenza più probabili, insieme ai loro punteggi, nello spazio di feature selezionato.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909de209",
   "metadata": {},
   "source": [
    "### Metodo\n",
    "\n",
    "1. **Estrazione delle Feature**\n",
    "\n",
    "2. **Rappresentazione del Dataset Etichettato**\n",
    "\n",
    "3. **Calcolo delle Distanze/Similarità**\n",
    "   - Per misurare quanto la query è vicina alle immagini di ciascuna classe, si calcolano metriche di distanza o similarità tra il vettore query e i vettori delle immagini nel dataset.\n",
    "   - Le metriche considerate, che performano meglio con ResNet, sono:\n",
    "     - **Distanza Euclidea** (norma L2)\n",
    "     - **Cosine Similarity** (misura l’angolo tra vettori)\n",
    "\n",
    "4. **Aggregazione dei Punteggi per Etichetta**\n",
    "\n",
    "   Due strategie di aggregazione sono state implementate:\n",
    "\n",
    "    - **Strategia 1: Media delle distanze/similarità**\n",
    "       - Per ogni classe, si calcolano tutte le distanze (o similarità) tra la query e le immagini di quella classe.\n",
    "       - Si prende la media di questi valori come punteggio rappresentativo della classe.\n",
    "\n",
    "    - **Strategia 2: Distanza dal prototipo (centroide)**\n",
    "       - Si calcola il vettore prototipo (centroide) di ciascuna classe come media dei vettori delle sue immagini.\n",
    "       - Si misura la distanza o similarità tra la query e ciascun prototipo.\n",
    "\n",
    "5. **Selezione delle Etichette Predette**\n",
    "   - Le classi vengono ordinate in base al punteggio calcolato (distanza crescente o similarità decrescente).\n",
    "   - Si selezionano le prime `k` classi come etichette più probabili."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f4cb5",
   "metadata": {},
   "source": [
    "### Output atteso\n",
    "\n",
    "La funzione esegue due strategie diverse e stampa, per ognuna:\n",
    "\n",
    "- Le **k etichette più simili** (cioè più vicine alla query nel feature space)\n",
    "- Un **punteggio** associato a ciascuna etichetta:\n",
    "  - **Distanza media** dalla query verso le immagini della classe\n",
    "  - **Distanza dal prototipo** (centroide) della classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola la distanza o similarità tra la feature della query e un insieme di feature target.\n",
    "# Supporta sia distanza euclidea che similarità coseno.\n",
    "def compute_metric(query_feat, target_feats, metric):\n",
    "    query_feat = query_feat.reshape(1, -1)\n",
    "    if metric == \"euclidean\":\n",
    "        return np.linalg.norm(target_feats - query_feat, axis=1)\n",
    "    elif metric == \"cosine\":\n",
    "        return cosine_similarity(query_feat, target_feats)[0]\n",
    "    else:\n",
    "        raise ValueError(\"Metric must be 'euclidean' or 'cosine'\")\n",
    "\n",
    "\n",
    "# Per ogni etichetta si calcola la media delle distanze tra la query e tutte le immagini della classe.\n",
    "# Le top-k etichette con la distanza media più bassa (o similarità più alta) vengono selezionate.\n",
    "def predict_top_k_labels_distance_mean(query_img_path, k, model, metric):\n",
    "    query_feat = extract_and_process_image(query_img_path, model)\n",
    "    unique_labels = np.unique(lbls_part1)\n",
    "    avg_scores = []\n",
    "    for label in unique_labels:\n",
    "        class_feats = feat_matrix_part1[lbls_part1 == label]\n",
    "        scores = compute_metric(query_feat, class_feats, metric)\n",
    "        avg_scores.append(scores.mean())\n",
    "    if metric == \"euclidean\":\n",
    "        sorted_indices = np.argsort(avg_scores)\n",
    "    else:  # cosine\n",
    "        sorted_indices = np.argsort(avg_scores)[::-1]\n",
    "    print(f\"--- Top-{k} etichette - Distanza media ({metric}) ---\")\n",
    "    for i in range(min(k, len(unique_labels))):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"{unique_labels[idx]} \\t Score medio: {avg_scores[idx]:.4f}\")\n",
    "\n",
    "\n",
    "# Per ogni etichetta si calcola il prototipo (media dei vettori feature),\n",
    "# poi si misura la distanza o similarità tra la query e ogni prototipo.\n",
    "def predict_top_k_labels_prototype(query_img_path, k, model, metric):\n",
    "    query_feat = extract_and_process_image(query_img_path, model)\n",
    "    unique_labels = np.unique(lbls_part1)\n",
    "    prototypes = []\n",
    "    for label in unique_labels:\n",
    "        class_feats = feat_matrix_part1[lbls_part1 == label]\n",
    "        prototypes.append(class_feats.mean(axis=0))\n",
    "    prototypes = np.vstack(prototypes)\n",
    "    scores = compute_metric(query_feat, prototypes, metric)\n",
    "    if metric == \"euclidean\":\n",
    "        sorted_indices = np.argsort(scores)\n",
    "    else:\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "    print(f\"--- Top-{k} etichette - Prototipo classe ({metric}) ---\")\n",
    "    for i in range(min(k, len(unique_labels))):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"{unique_labels[idx]} \\t Score: {scores[idx]:.4f}\")\n",
    "\n",
    "\n",
    "# Dato un'immagine di query, un valore k e una metrica, stampa le top-k etichette predette\n",
    "# usando entrambe le strategie: distanza media o cosine similarty.\n",
    "def task4_predict_labels(query_img_path, k, model, metric):\n",
    "    assert k <= 2, \"k deve essere <= 2\"\n",
    "    print(f\"Predizione top-{k} per immagine '{query_img_path}' usando metrica '{metric}'\")\n",
    "    predict_top_k_labels_distance_mean(query_img_path, k, model, metric)\n",
    "    print()\n",
    "    predict_top_k_labels_prototype(query_img_path, k, model, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2ea7e",
   "metadata": {},
   "source": [
    "Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = \"Part2/brain_glioma/brain_glioma_1112.jpg\"\n",
    "\n",
    "task4_predict_labels(query_img, k=2, model=model, metric=\"euclidean\")\n",
    "\n",
    "task4_predict_labels(query_img, k=2, model=model, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d85e3",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87267ee8",
   "metadata": {},
   "source": [
    "Implementa un programma che:<br>\n",
    "(a) dato uno dei modelli di feature,<br> \n",
    "(b) un valore k specificato dall'utente, <br>\n",
    "(c) una delle tre tecniche di riduzione della dimensionalità (SVD, LDA, k-means) scelte dall'utente,<br> \n",
    "riporta le prime k semantiche latenti estratte nello spazio di feature selezionato.    \n",
    "\n",
    "- Memorizza le semantiche latenti in un file di output adeguatamente nominato.    \n",
    "\n",
    "- Elenca le coppie imageID-peso, ordinate in ordine decrescente di pesi.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c46a56",
   "metadata": {},
   "source": [
    "\n",
    "### Metodologia\n",
    "\n",
    "1. **Input**\n",
    "   - Viene fornito un file `.npz` che contiene le feature estratte dalle immagini (`feature_model_path`).\n",
    "   - Si sceglie una delle seguenti tecniche:\n",
    "     - `\"svd\"`: Singular Value Decomposition\n",
    "     - `\"lda\"`: Linear Discriminant Analysis\n",
    "     - `\"kmeans\"`: Clustering KMeans\n",
    "   - Si specifica un numero `k` di concetti latenti da estrarre.\n",
    "\n",
    "2. **Trasformazione**\n",
    "   - Le feature originali vengono proiettate in uno spazio latente a `k` dimensioni.\n",
    "   - Vengono calcolati i **pesi** (coefficenti di proiezione o distanze) associati a ciascuna immagine rispetto a ogni componente o cluster.\n",
    "\n",
    "3. **Ordinamento e Interpretazione**\n",
    "   - Per ogni concetto latente, le immagini vengono ordinate in base al peso assoluto (quanto \"rappresentano\" quel concetto).\n",
    "   - Questo permette di individuare le immagini più emblematiche per ogni componente/cluster.\n",
    "\n",
    "4. **Visualizzazione**\n",
    "   - Se possibile, i dati trasformati vengono visualizzati in 2D (es. primi 2 componenti) con colorazione per classe.\n",
    "\n",
    "5. **Salvataggio dei Risultati**\n",
    "   - I risultati vengono salvati in un file di testo:\n",
    "     ```\n",
    "     latent_semantics_<tecnica>_<nome_feature_model>_k<k>.txt\n",
    "     ```\n",
    "   - Per ogni concetto latente viene elencata una lista ordinata di immagini con:\n",
    "     - Nome file\n",
    "     - Peso\n",
    "     - Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90464e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza la proiezione 2D dello spazio latente.\n",
    "def plot_latent_space_2d(X_transformed, labels, technique, k):\n",
    "    if X_transformed.shape[1] < 2:\n",
    "        print(\"[INFO] Meno di 2 componenti: impossibile visualizzare in 2D.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_transformed[:, 0], y=X_transformed[:, 1], hue=labels, palette=\"Set2\", s=80)\n",
    "    plt.title(f\"{technique.upper()} - Proiezione sulle prime 2 componenti latenti (k={k})\")\n",
    "    plt.xlabel(\"Componente 1\")\n",
    "    plt.ylabel(\"Componente 2\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Visualizza i cluster KMeans in 2D usando SVD per proiezione.\n",
    "def plot_kmeans_clusters_2d(feature_matrix, labels, n_clusters):    \n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd.fit_transform(feature_matrix)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=cluster_labels, palette='tab10', s=80, style=labels)\n",
    "    plt.title(f\"KMeans Clustering (k={n_clusters}) con proiezione SVD 2D\")\n",
    "    plt.xlabel(\"Componente Latente 1 (da SVD)\")\n",
    "    plt.ylabel(\"Componente Latente 2 (da SVD)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d81d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La funzione applica una tecnica di riduzione dimensionale (SVD, LDA, KMeans)\n",
    "# sullo spazio delle feature estratte da ResNet e salva su file le feature latenti ottenute.\n",
    "def task5_latent_semantics_resnet(feature_model_path, technique, k):\n",
    "    technique = technique.lower()\n",
    "\n",
    "    if technique == \"svd\":\n",
    "        model = TruncatedSVD(n_components=k, random_state=42)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1)\n",
    "        components = model.components_\n",
    "        method = \"svd\"\n",
    "\n",
    "    elif technique == \"lda\":\n",
    "        unique_labels = np.unique(lbls_part1)\n",
    "        max_k = len(unique_labels) - 1\n",
    "        if k > max_k:\n",
    "            print(f\"Warning: LDA supporta al massimo {max_k} componenti con {len(unique_labels)} classi.\")\n",
    "            k = max_k\n",
    "        model = LDA(n_components=k)\n",
    "        X_transformed = model.fit_transform(feat_matrix_part1, lbls_part1)\n",
    "        components = model.scalings_.T[:k]\n",
    "        method = \"lda\"\n",
    "\n",
    "    elif technique == \"kmeans\":\n",
    "        model = KMeans(n_clusters=k, random_state=42)\n",
    "        model.fit(feat_matrix_part1)\n",
    "        components = model.cluster_centers_\n",
    "        X_transformed = model.transform(feat_matrix_part1)\n",
    "        method = \"kmeans\"\n",
    "    else:\n",
    "        print(\"Error: Tecnica non supportata. Usa: 'svd', 'lda', 'kmeans'\")\n",
    "        return\n",
    "\n",
    "    # === Visualizzazione ===\n",
    "    if technique in [\"svd\", \"lda\"]:\n",
    "        plot_latent_space_2d(X_transformed, lbls_part1, technique, k)\n",
    "    elif technique == \"kmeans\":\n",
    "        plot_kmeans_clusters_2d(feat_matrix_part1, lbls_part1, k)\n",
    "    \n",
    "    # === Output file ===\n",
    "    base_name = os.path.splitext(os.path.basename(feature_model_path))[0]\n",
    "    out_file = f\"latent_semantics_{method}_{base_name}_k{k}.txt\"\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for i in range(k):\n",
    "            f.write(f\"\\n--- Latent Semantic {i+1} ---\\n\")\n",
    "            if technique in [\"svd\", \"lda\"]:\n",
    "                weights = feat_matrix_part1 @ components[i].T\n",
    "            else:\n",
    "                weights = -X_transformed[:, i]  # distanza inversa\n",
    "\n",
    "            sorted_idx = np.argsort(-np.abs(weights))\n",
    "            for idx in sorted_idx:\n",
    "                f.write(f\"{flname_part1[idx]} | Peso: {weights[idx]:.4f} | Classe: {lbls_part1[idx]}\\n\")\n",
    "\n",
    "    print(f\"[SALVATO] Latent semantics salvati in: {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c355b",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c479a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supponendo tu abbia salvato il file .npz come \"resnet_layer3.npz\"\n",
    "task5_latent_semantics_resnet(\"resnet_features_part1.npz\", technique=\"svd\", k=5)\n",
    "task5_latent_semantics_resnet(\"resnet_features_part1.npz\", technique=\"lda\", k=2)\n",
    "task5_latent_semantics_resnet(\"resnet_features_part1.npz\", technique=\"kmeans\", k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c273e38",
   "metadata": {},
   "source": [
    "### Task 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c58b69",
   "metadata": {},
   "source": [
    "### Task 6a\n",
    "Implementa un programma che calcola e stampa la \"inherent dimensionality\" associata alle immagini della parte 1.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea92f8b2",
   "metadata": {},
   "source": [
    "### Metodo utilizzato\n",
    "\n",
    "Il calcolo si basa su:\n",
    "- L’Analisi delle Componenti Principali (PCA)\n",
    "- Si esegue una decomposizione con TruncatedSVD o PCA sul dataset feat_matrix_part1\n",
    "- Si osserva il cumulative explained variance ratio\n",
    "\n",
    "### Obiettivo: \n",
    " - [SVD] Varianza spiegata cumulativa (95%): raggiunta con 87 componenti\n",
    " - [SVD] Varianza spiegata cumulativa (99%): raggiunta con 156 componenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f24274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_intrinsic_dimensionality(feature_matrix, threshold, plot=True):\n",
    "    max_components = min(feature_matrix.shape)\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(feature_matrix)\n",
    "\n",
    "    explained = pca.explained_variance_ratio_   \n",
    "    cumulative = np.cumsum(explained)\n",
    "\n",
    "    if threshold >= 1.0:\n",
    "        intrinsic_dim = len(cumulative)\n",
    "    else:\n",
    "        intrinsic_dim = np.argmax(cumulative >= threshold) + 1\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(cumulative, marker='o', label=\"Varianza cumulativa\")\n",
    "        plt.axhline(y=threshold, color='r', linestyle='--', label=f\"Soglia {threshold*100:.0f}%\")\n",
    "        plt.axvline(x=intrinsic_dim, color='g', linestyle='--', label=f\"k suggerito: {intrinsic_dim}\")\n",
    "        plt.xlabel(\"Numero componenti\")\n",
    "        plt.ylabel(\"Varianza cumulativa\")\n",
    "        plt.title(\"Scelta ottimale di k (PCA)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"[INFO] k ottimale suggerito (soglia {threshold*100:.0f}%): {intrinsic_dim}\")\n",
    "    return intrinsic_dim, cumulative\n",
    "\n",
    "def suggest_k(feature_matrix, threshold_list=[0.90, 0.95, 0.99]):\n",
    "    print(f\"[INFO] Feature matrix shape: {feature_matrix.shape}\")\n",
    "    k_values = {}\n",
    "    for t in threshold_list:\n",
    "        k, _ = estimate_intrinsic_dimensionality(feature_matrix, threshold=t, plot=False)\n",
    "        k_values[t] = k\n",
    "        print(f\"Soglia {int(t*100)}% : k = {k}\")\n",
    "    return k_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f59c7b",
   "metadata": {},
   "source": [
    "### Task 6b: \n",
    "Implementa un programma che calcola e stampa la \"dimensionalità intrinseca\" (numero di dim indipendenti minime necassari per rappresentare set) associata a ciascuna etichetta univoca delle immagini della parte 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebcc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_dimensionality_per_label(feature_matrix, labels, threshold):\n",
    "    label_dim_map = {}\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    print(f\"[INFO] Etichette uniche trovate: {len(unique_labels)}\")\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        label_features = feature_matrix[indices]\n",
    "\n",
    "        if len(indices) < 2:\n",
    "            print(f\"[AVVISO] Label '{label}' ha meno di 2 campioni — ignorata.\")\n",
    "            continue\n",
    "\n",
    "        k, _ = estimate_intrinsic_dimensionality(label_features, threshold=threshold, plot=False)\n",
    "        label_dim_map[label] = k\n",
    "        print(f\" Label '{label}' : k = {k}\")\n",
    "\n",
    "    return label_dim_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf1c72",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStima automatica di k in base alla varianza spiegata:\\n\")\n",
    "k_suggeriti = suggest_k(feat_matrix_part1)\n",
    "\n",
    "print(\"\\nTask 6a - Dimensionalità instrinsca dell'insieme di img Part1\")\n",
    "estimate_intrinsic_dimensionality(feat_matrix_part1, threshold=1.00, plot=True)\n",
    "\n",
    "print(\"\\n Task 6b - Dimensionalità per etichetta:\\n\")\n",
    "label_dimensionalities = estimate_dimensionality_per_label(feat_matrix_part1, lbls_part1, threshold=1.00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c87115",
   "metadata": {},
   "source": [
    "# Task 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cffa2",
   "metadata": {},
   "source": [
    "Implementa un programma che,per ciascuna etichetta univoca l, calcola le corrispondenti k semantiche latenti (a tua scelta) associate alle immagini della parte 1, e per le immagini della parte 2, prevede le etichette più probabili utilizzando distanze/similarità calcolate sotto le semantiche latenti specifiche dell'etichetta.\n",
    "Il sistema dovrebbe anche fornire valori di precision, recall, and F1-score per etichetta, nonché un valore di accuratezza complessiva.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af5b23",
   "metadata": {},
   "source": [
    "### Metodo\n",
    "\n",
    "1. **Costruzione degli spazi latenti per etichetta**:\n",
    "   - Per ogni classe, estraiamo tutte le immagini della parte 1 che la rappresentano.\n",
    "   - Applichiamo **StandardScaler** per normalizzare le feature.\n",
    "   - Eseguiamo una **SVD** su ogni sottoinsieme per ottenere `k` concetti latenti.\n",
    "   - Calcoliamo la **media (centroide)** dei vettori latenti ottenuti.\n",
    "\n",
    "2. **Predizione su immagini della parte 2**:\n",
    "   - Ogni immagine viene trasformata nello spazio latente di ogni classe.\n",
    "   - Calcoliamo la **distanza euclidea** tra la rappresentazione latente dell'immagine e il centroide della classe.\n",
    "   - L’etichetta predetta è quella con distanza minima.\n",
    "\n",
    "3. **Valutazione delle predizioni**:\n",
    "   - Calcoliamo metriche di classificazione: **precision, recall, F1-score per classe**.\n",
    "   - Calcoliamo anche l’**accuracy complessiva**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent_semantics_per_class(X, y, k=10):\n",
    "    class_models = {}\n",
    "    class_means = {}\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        X_class = X[y == label]  # Prende solo le istanze della classe corrente\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_class)  # Normalizza i dati della classe\n",
    "\n",
    "        svd = TruncatedSVD(n_components=k)\n",
    "        latent = svd.fit_transform(X_scaled)  # Riduzione dimensionale con SVD\n",
    "    \n",
    "        # Salva modello SVD e scaler per la classe\n",
    "        class_models[label] = {\n",
    "            'svd': svd,\n",
    "            'scaler': scaler,\n",
    "            'latent_vectors': latent\n",
    "        }\n",
    "        # Calcola la media dei vettori latenti della classe\n",
    "        class_means[label] = np.mean(latent, axis=0)\n",
    "    return class_models, class_means\n",
    "\n",
    "def predict_label(X_test, class_models, class_means):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        best_label = None\n",
    "        min_dist = float('inf')\n",
    "        for label, model in class_models.items():\n",
    "            x_scaled = model['scaler'].transform(x.reshape(1, -1))  # Normalizza x\n",
    "            x_latent = model['svd'].transform(x_scaled)  # Trasforma in spazio latente\n",
    "            dist = np.linalg.norm(x_latent - class_means[label])  # Distanza dal centroide\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_label = label\n",
    "        y_pred.append(best_label)\n",
    "    return y_pred\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    labels = np.unique(y_true)\n",
    "    print(\"Per-class metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(\n",
    "            f\"Class {label}: P={precision[i]:.2f}, R={recall[i]:.2f}, F1={f1[i]:.2f}\")\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "def evaluate_predictions(true_labels, predicted_labels):\n",
    "    print(\"[VALUTAZIONE] Report di classificazione:\")\n",
    "    print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dec790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento sui dati di Part1\n",
    "class_models, class_means = compute_latent_semantics_per_class(\n",
    "    feat_matrix_part1, lbls_part1, k=10)\n",
    "\n",
    "# Predizione su Part2\n",
    "predicted_labels = predict_label(feat_matrix_part2, class_models, class_means)\n",
    "\n",
    "# Valutazione\n",
    "evaluate(lbls_part2, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11300f",
   "metadata": {},
   "source": [
    "## Task 8\n",
    "Implementa un programma che, per ciascuna etichetta univoca l, calcola i corrispondenti c cluster più significativi associati alle immagini della parte 1 (utilizzando l'algoritmo DBScan); <br> i cluster risultanti devono essere visualizzati sia come nuvole di punti colorate in modo diverso in uno spazio MDS a 2 dimensioni, sia come gruppi di miniature di immagini.\n",
    "\n",
    "#### Obiettivo del Task\n",
    "In questo task esploriamo la **struttura interna dei dati di ciascuna classe**, cercando di identificare eventuali **cluster naturali** (gruppi di immagini statisticamente simili) all’interno delle feature.\n",
    "\n",
    "Per farlo combiniamo due tecniche fondamentali:\n",
    "- Dimensionality Reduction (PCA, UMAP).\n",
    "- **DBSCAN:** per identificare cluster basati sulla densità dei punti nello spazio delle feature.\n",
    "\n",
    "---\n",
    "#### **Metodo utilizzato**\n",
    "\n",
    "##### 1️ - **Riduzione dimensionale**\n",
    "- Prima di applicare il clustering, riduciamo lo spazio delle feature da 900 a **50 componenti**, mantenendo la maggior parte della varianza.\n",
    "- La riduzione aiuta DBSCAN a lavorare meglio, eliminando il rumore di alta dimensionalità.\n",
    "<br>Metodi:<br>\n",
    "    - **PCA (Principal Component Analysis)**: proietta i dati lungo le direzioni di massima varianza, preservando la struttura lineare globale.\n",
    "    - **UMAP (Uniform Manifold Approximation and Projection)**: preserva sia la struttura locale che globale, più veloce e scalabile rispetto a t-SNE.\n",
    "\n",
    "\n",
    "#### 2️ **Clustering con DBSCAN**\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise):\n",
    "- Identifica i cluster come gruppi di punti vicini tra loro.\n",
    "- I punti che non appartengono a nessun cluster vengono etichettati come **rumore (-1)**.\n",
    "\n",
    "Parametri principali utilizzati:\n",
    " - **eps**             Raggio di vicinanza per considerare due punti vicini     2.0\n",
    " - **min_samples**     Minimo numero di punti per formare un cluster            3\n",
    "\n",
    "#### 3️ **Selezione dei cluster più significativi**\n",
    "Dopo aver identificato tutti i cluster, selezioniamo gli **'n' cluster più popolosi** per ciascuna classe, escludendo il rumore.\n",
    "\n",
    "#### 4️ **Visualizzazione dei risultati**\n",
    "Abbiamo rappresentato i risultati in due modi:\n",
    "- **MDS 2D (Multidimensional Scaling)** \n",
    "- **Griglie di immagini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applichiamo tecniche di dimensionality reduction\n",
    "def reduce_features(features, method, n_components, random_state=42):\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=n_components)\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Metodo di riduzione '{method}' non supportato.\")\n",
    "\n",
    "    return reducer.fit_transform(features)\n",
    "\n",
    "#Applichiamo una tecnica di riduzione mediante reduce_feature alle features di partenza ottenendo la lista di Feature Latenti\n",
    "#Applichiamo su insieme di feature latenti StandardScaler per cercare di ottenere migliori cluster tramite DBSCAN\n",
    "\n",
    "def apply_dbscan_with_pca(features, eps, min_samples, n_components, method):\n",
    "    print(f\"Applicazione di {method} -> Riduzione a {n_components} componenti\")\n",
    "    reduced_features = reduce_features(features, method, n_components=n_components)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    reduced_scaled = scaler.fit_transform(reduced_features)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(reduced_scaled)\n",
    "    return labels\n",
    "\n",
    "#Calcola i 'c' cluster di maggior cardinalità\n",
    "def top_c_clusters(cluster_labels, c):\n",
    "    label_counts = Counter(cluster_labels)\n",
    "    label_counts.pop(-1, None) # rimozione cluster catalogato come rumore (-1)\n",
    "    if not label_counts:\n",
    "        print(\"[WARN] DBSCAN non ha trovato alcun cluster valido (solo rumore).\")\n",
    "        return []\n",
    "    \n",
    "    # Estraiamo i 'c' cluster più frequenti\n",
    "    most_common = label_counts.most_common(c)\n",
    "    top = [int(lbl) for lbl, _ in most_common]\n",
    "    \n",
    "    if len(top) < c:\n",
    "        print(f\"[WARN] DBSCAN ha trovato solo {len(top)} cluster (meno di {c}).\")\n",
    "    return top\n",
    "\n",
    "\n",
    "#Applichiamo al risultato di DBSCAN l'algoritmo di MDS\n",
    "def plot_mds_clusters(features, cluster_labels, top_clusters, metric):\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    #Generazione di una nuova matrice basata sul parametro metric (es. 'cosine')\n",
    "    D = pairwise_distances(features_scaled, metric=metric)\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    Y = mds.fit_transform(D)\n",
    "\n",
    "    cmap= matplotlib.colormaps['tab10']\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(len(Y)):\n",
    "        lbl = cluster_labels[i]\n",
    "        if lbl in top_clusters:\n",
    "            color_idx = top_clusters.index(lbl)\n",
    "            plt.scatter(Y[i,0], Y[i,1], color=cmap(color_idx), s=30, edgecolor='k', linewidth=0.2)\n",
    "        else:\n",
    "            # punti rumore o cluster “non top”\n",
    "            plt.scatter(Y[i,0], Y[i,1], color='lightgray', s=8)\n",
    "    \n",
    "    plt.title(f\"MDS 2D - Top {len(top_clusters)} cluster\")\n",
    "    plt.xlabel(\"MDS 1\")\n",
    "    plt.ylabel(\"MDS 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Genera blocco di immagini trovate per cluster\n",
    "def show_cluster_thumbnails(images, cluster_labels, top_clusters, thumb_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    images: lista (o array) di percorsi file (lunghezza N), \n",
    "            ossia a images[i] corrisponde features[i].\n",
    "    cluster_labels: array (N,) di cluster per ogni immagine.\n",
    "    top_clusters: lista dei c cluster (int) che vogliamo visualizzare.\n",
    "    thumb_size: dimensione (w,h) di ogni miniatura.\n",
    "    Per ogni cluster ∈ top_clusters stampa a video (o fa plt.show) \n",
    "    una griglia di miniature (fino a ~16-25 alla volta).\n",
    "    \"\"\"\n",
    "    for cluster_id in top_clusters:\n",
    "        # Indici di tutte le immagini che appartengono a questo cluster\n",
    "        idxs = [i for i, cl in enumerate(cluster_labels) if cl == cluster_id]\n",
    "        print(f\"[INFO] Cluster {cluster_id}: {len(idxs)} immagini trovate\")\n",
    "\n",
    "        # Se vogliamo limitare a N miniatura per cluster (tipo 16):\n",
    "        max_display = min(len(idxs), 16)\n",
    "        n = int(np.ceil(np.sqrt(max_display)))  # facciamo una griglia n×n\n",
    "        plt.figure(figsize=(n, n))\n",
    "\n",
    "        for j, i_img in enumerate(idxs[:max_display]):\n",
    "            img = Image.open(images[i_img]).convert('RGB')\n",
    "            img_thumb = img.resize(thumb_size, Image.LANCZOS)\n",
    "            \n",
    "            ax = plt.subplot(n, n, j+1)\n",
    "            plt.imshow(img_thumb)\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Cluster {cluster_id} – {len(idxs)} immagini (mostrate: {max_display})\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "def db_scan_detection(eps, min_samples, n_components, c, method):\n",
    "\n",
    "    # Costruisce l’elenco dei full path per tutte le immagini\n",
    "    base_folder = \"Part1\"  # o path assoluto \"/Users/.../Parte1\"\n",
    "    images_full = [os.path.join(base_folder, lbl, fname) for fname, lbl in zip(flname_part1, lbls_part1)]\n",
    "\n",
    "    # Scorre ogni label di Parte1 ed applico DBSCAN+PCA\n",
    "    unique_labels = np.unique(lbls_part1)  # es. [\"Glioma\",\"Meningioma\",\"Pituitary\"]\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"[INFO] Elaboro label: {lbl}\")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        #Estrae le righe di feat_matrix_part1 / flname_part1 corrispondenti\n",
    "        mask_lbl = (lbls_part1 == lbl)\n",
    "        features_label = feat_matrix_part1[mask_lbl]   # shape = (n_i, d)\n",
    "        images_label = np.array(images_full)[mask_lbl]\n",
    "\n",
    "        #Chiama la tua funzione PCA + DBSCAN\n",
    "        cluster_labels = apply_dbscan_with_pca(\n",
    "            features_label,\n",
    "            eps=eps,\n",
    "            min_samples=min_samples,\n",
    "            n_components=n_components,\n",
    "            method = method\n",
    "        )\n",
    "        print(f\"[INFO] Cluster-labels trovati: {np.unique(cluster_labels)}\")\n",
    "\n",
    "        # Trova i c cluster più grandi\n",
    "        top_clusters = top_c_clusters(cluster_labels, c)\n",
    "        print(f\"[INFO] Top {c} cluster (per dimensione): {top_clusters}\")\n",
    "\n",
    "        # MDS‐2D + scatter plot del clustering\n",
    "        print(f\"[INFO] Disegno MDS 2D per i cluster di '{lbl}' …\")\n",
    "\n",
    "        plot_mds_clusters(\n",
    "            features_label,\n",
    "            cluster_labels,\n",
    "            top_clusters,\n",
    "            metric='cosine'\n",
    "        )\n",
    "\n",
    "        # Creo le miniature di ogni cluster “significativo”\n",
    "        print(f\"[INFO] Genero miniature per ciascun cluster di '{lbl}' …\")\n",
    "        show_cluster_thumbnails(\n",
    "            images_label,      # array di stringhe di percorsi\n",
    "            cluster_labels,    # array di int di lunghezza n_i\n",
    "            top_clusters,      # la lista dei c indici di cluster\n",
    "            thumb_size=(64, 64)\n",
    "        )\n",
    "\n",
    "    print(\"\\n[FINITO] Task 8 completato per tutte le label di Parte1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 5.0            # valore DBSCAN di esempio\n",
    "min_samples = 3      # valore DBSCAN di esempio\n",
    "n_components = 50    # quante dimensioni tenere con PCA PRIMA di DBSCAN\n",
    "c = 3                # quanti cluster “significativi” voglio prendere per ciascuna label\n",
    "method = 'umap'      # umap or pca\n",
    "db_scan_detection(eps, min_samples, n_components, c, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51205fe",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "Implementa un programma che, dati le immagini della parte 1:\n",
    " - crea un classificatore m-NN (per una m specificata dall'utente),\n",
    " - crea un classificatore ad albero decisionale,<br>\n",
    "Per questo task, puoi utilizzare lo spazio delle feature di tua scelta.<br>\n",
    "Per le immagini della parte 2, prevede le etichette più probabili utilizzando il classificatore selezionato dall'utente.<br>\n",
    "Il sistema dovrebbe anche fornire valori di precisione, richiamo e punteggio F1 per etichetta, nonché un valore di accuratezza complessiva.    \n",
    "\n",
    "#### **Obiettivo del Task**\n",
    "In questo task testiamo due approcci di **classificazione supervisionata**, usando come feature i **Color Moments** estratti in precedenza.  \n",
    "L’obiettivo è **prevedere la classe** di ciascuna immagine del dataset **Parte 2**, dopo aver addestrato i classificatori sulla **Parte 1**.\n",
    "\n",
    "\n",
    "#### **Classificatori utilizzati**\n",
    "\n",
    " - k-NN (k Nearest Neighbors)\n",
    "<br>Parametri:<br>\n",
    "     - **k = 5**, valore bilanciato che riduce il rischio di overfitting.\n",
    "\n",
    "\n",
    "- Decision Tree\n",
    "<br>Parametri:<br>\n",
    "     - Parametri di default → l’albero viene costruito senza limitazioni particolari sulla profondità.\n",
    "\n",
    "#### **Metodo**\n",
    "\n",
    "1. Training:\n",
    "   - I classificatori vengono addestrati sulle feature del dataset **Parte 1**, per ciascuna immagine e classe nota.\n",
    "2. Testing:\n",
    "   - I modelli vengono testati sulle feature delle immagini della **Parte 2**, simulando un vero scenario di classificazione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imposta il valore di m per l'm-NN\n",
    "m = 5  # Modifica questo valore in base alle tue necessità\n",
    "\n",
    "# Addestramento m-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=m)\n",
    "knn_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Addestramento Decision Tree\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(feat_matrix_part1, lbls_part1)\n",
    "\n",
    "# Predizioni su Part2\n",
    "pred_knn = knn_model.predict(feat_matrix_part2)\n",
    "pred_dt = dt_model.predict(feat_matrix_part2)\n",
    "\n",
    "# Valutazione m-NN\n",
    "print(\"Risultati m-NN:\")\n",
    "print(classification_report(lbls_part2, pred_knn))\n",
    "print(\"Accuratezza complessiva m-NN:\", accuracy_score(lbls_part2, pred_knn))\n",
    "\n",
    "# Valutazione Decision Tree\n",
    "print(\"Risultati Decision Tree:\")\n",
    "print(classification_report(lbls_part2, pred_dt))\n",
    "print(\"Accuratezza complessiva Decision Tree:\", accuracy_score(lbls_part2, pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d34d7",
   "metadata": {},
   "source": [
    "# Task 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30cd5c9",
   "metadata": {},
   "source": [
    "### 10a:\n",
    "Implementa uno strumento di Locality Sensitive Hashing (LSH) (per la distanza euclidea) che prende come input (a) il numero di livelli, L, (b) il numero di hash per livello, h, e (c) un insieme di vettori come input e crea una struttura di indice in memoria contenente l'insieme di vettori dato. \n",
    "\n",
    "Vedi:\n",
    "\"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions\" (di Alexandr Andoni e Piotr Indyk). Communications of the ACM, vol. 51, no. 1, 2008, pp. 117-122.    \n",
    "\n",
    "### 10b:\n",
    "Implementa un algoritmo di ricerca di immagini simili utilizzando questa struttura di indice che memorizza le immagini della parte 1 e un modello visivo di tua scelta (il modello visivo combinato deve avere almeno 256 dimensioni): per una data immagine di query e un numero intero t, \n",
    " \n",
    "*visualizza le t immagini più simili,\n",
    "*fornisce il numero di immagini univoche e il numero complessivo di immagini considerate durante il processo.    \n",
    "\n",
    "\n",
    "#### **Obiettivo del Task**\n",
    "\n",
    "Fino ad ora abbiamo calcolato la similarità tra immagini confrontando la query con tutte le immagini del dataset.  \n",
    "Questo approccio è efficace, ma **computazionalmente molto costoso**, soprattutto per dataset di grandi dimensioni.\n",
    "\n",
    "In questo task introduciamo il **Locality Sensitive Hashing (LSH)**, una tecnica che consente di **ridurre il numero di confronti**, accelerando la ricerca delle immagini simili.\n",
    "\n",
    "#### **Metodo adottato**\n",
    "\n",
    " - 1 Creazione dell’indice LSH\n",
    "<br>Abbiamo implementato un indice chiamato **LSH_EuclideanQuantized**, progettato per la **distanza Euclidea**.<br>  \n",
    "    <br>Ogni vettore viene:<br>\n",
    "     - **centrato**, sottraendo la media globale,\n",
    "     - **normalizzato** a norma unitaria (**L2 norm**).\n",
    "\n",
    "- 2️ Ricerca della query\n",
    "    <br>Per cercare immagini simili a una query:<br>\n",
    "     - Si estrae il vettore dei Color Moments,\n",
    "     - Si centra e normalizza con gli stessi parametri del training,\n",
    "     - Si cercano i bucket hash più rilevanti (in tutti i livelli hash),\n",
    "     - Si calcolano le **distanze Euclidee reali** solo con i candidati recuperati dai bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d64c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe LSH con quantizzazione\n",
    "\n",
    "class LSH_EuclideanQuantized:\n",
    "    \"\"\"\n",
    "    LSH per distanza Euclidea (p-stable) con bucket width r.\n",
    "    Ogni hash h_j(v) = floor((a_j · v + b_j) / r).\n",
    "\n",
    "    Parametri:\n",
    "      - num_layers   = L = numero di tavole hash\n",
    "      - num_hashes   = h = numero di functions concatenati in ciascuna tavola\n",
    "      - dim          = D = dimensione dei vettori di input\n",
    "      - r            = bucket width (parte intera di quantizzazione)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_hashes: int, dim: int, r: float):\n",
    "        self.L = num_layers\n",
    "        self.h = num_hashes\n",
    "        self.d = dim\n",
    "        self.r = r\n",
    "\n",
    "        # Prepara L tavole hash: ciascuna è un dict (chiave tuple di h interi -> lista di indici)\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(self.L)]\n",
    "\n",
    "        # Per ogni layer l=0..L-1, e per ogni j=0..h-1, genero:\n",
    "        #   - a_lj  vettore gaussiano di dimensione D\n",
    "        #   - b_lj  offset (uniforme in [0, r) )\n",
    "        self.a_vectors = [\n",
    "            [np.random.randn(self.d) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "        self.b_offsets = [\n",
    "            [np.random.uniform(0, self.r) for _ in range(self.h)]\n",
    "            for _ in range(self.L)\n",
    "        ]\n",
    "\n",
    "        # Memorizzerò i vettori originali di Part1 in questo array, shape=(N, D)\n",
    "        self.data_vectors = None\n",
    "\n",
    "    def _compute_hash_tuple(self, vec: np.ndarray, layer_idx: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Calcola l'hash (h interi) per il layer layer_idx su un vettore vec:\n",
    "          h_j = floor((a_vectors[layer_idx][j] · vec + b_offsets[layer_idx][j]) / r)\n",
    "        Ritorna una tupla di h interi.\n",
    "        \"\"\"\n",
    "        keys = []\n",
    "        a_vs = self.a_vectors[layer_idx]\n",
    "        b_os = self.b_offsets[layer_idx]\n",
    "        for j in range(self.h):\n",
    "            a_j = a_vs[j]         # vettore dimensione D\n",
    "            b_j = b_os[j]         # float in [0, r)\n",
    "            proj = float(np.dot(a_j, vec) + b_j)\n",
    "            h_val = int(np.floor(proj / self.r))\n",
    "            keys.append(h_val)\n",
    "        return tuple(keys)\n",
    "\n",
    "    def index(self, vectors: np.ndarray):\n",
    "        \"\"\"\n",
    "        Costruisci l'indice LSH su un insieme di vettori di Part1:\n",
    "          vectors: numpy array shape = (N, D)\n",
    "        Al termine di questa chiamata:\n",
    "          - self.data_vectors = vectors\n",
    "          - self.hash_tables[l][hash_tuple] conterrà la lista di indici i per cui\n",
    "            hash_tuple = _compute_hash_tuple(vectors[i], l).\n",
    "        \"\"\"\n",
    "        self.data_vectors = vectors\n",
    "        N, D = vectors.shape\n",
    "        assert D == self.d, f\"Errore: dimensione vettore ({D}) ≠ atteso ({self.d}).\"\n",
    "\n",
    "        # Inserisco ogni vettore in ciascuna tavola hash\n",
    "        for idx in range(N):\n",
    "            v = vectors[idx]\n",
    "            for l in range(self.L):\n",
    "                key = self._compute_hash_tuple(v, l)\n",
    "                self.hash_tables[l][key].append(idx)\n",
    "\n",
    "    def query(self, q_vec: np.ndarray, top_t: int = 5):\n",
    "        \"\"\"\n",
    "        Esegui una query LSH per cercare i top_t vettori più vicini a q_vec.\n",
    "        Restituisce:\n",
    "          - top_results: lista di tuple (indice, distanza) ord. per dist. crescente\n",
    "          - unique_count: numero di indici distinti considerati (cardinalità dei candidati)\n",
    "          - total_checked: somma della lunghezza di tutti i bucket esaminati\n",
    "        \"\"\"\n",
    "        assert q_vec.shape[0] == self.d, \"Errore: dimensione query ≠ D.\"\n",
    "        candidati = set()\n",
    "        total_checked = 0\n",
    "\n",
    "        # Per ciascun layer, ottengo la chiave polidimensionale e i suoi bucket\n",
    "        for l in range(self.L):\n",
    "            h_tuple = self._compute_hash_tuple(q_vec, l)\n",
    "            bucket = self.hash_tables[l].get(h_tuple, [])\n",
    "            total_checked += len(bucket)\n",
    "            candidati.update(bucket)\n",
    "\n",
    "        # Ora calcolo la distanza euclidea esatta tra q_vec e ciascun candidato\n",
    "        risultati = []\n",
    "        for idx in candidati:\n",
    "            v_i = self.data_vectors[idx]\n",
    "            dist = np.linalg.norm(v_i - q_vec)\n",
    "            risultati.append((idx, dist))\n",
    "\n",
    "        # Ordino e prendo i primi top_t\n",
    "        risultati.sort(key=lambda x: x[1])\n",
    "        top_results = risultati[:top_t]\n",
    "        return top_results, len(candidati), total_checked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47446e00",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd54c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# costruzione LSH_EuclideanQuantized su Part1\n",
    "\n",
    "# 1) (Opzionale ma consigliato) centra e normalizza i vettori di Part1\n",
    "#    Questo passaggio riduce l'effetto di scale diverse e spesso migliora la qualità LSH\n",
    "mean_vec = np.mean(feat_matrix_part1, axis=0)\n",
    "feat_centered = feat_matrix_part1 - mean_vec\n",
    "feat_normed = normalize(feat_centered, norm='l2', axis=1)\n",
    "\n",
    "# 2) Parametri LSH\n",
    "D = feat_normed.shape[1]      # di solito 900\n",
    "L = 10                         # numero di tavole hash (scegli in base a esperimenti)\n",
    "h = 7                      # numero di funzioni concatenati in ciascuna tavola\n",
    "r = 1                       # parametro di larghezza (esempio: 0.5); puoi sperimentare\n",
    "\n",
    "# 3) Creo l'oggetto e indicizzo\n",
    "lsh_quant = LSH_EuclideanQuantized(num_layers=L, num_hashes=h, dim=D, r=r)\n",
    "lsh_quant.index(feat_normed)\n",
    "\n",
    "print(f\"[INFO] LSH quantizzato costruito: D={D}, L={L}, h={h}, r={r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea150e2",
   "metadata": {},
   "source": [
    "> 10b:\n",
    "\n",
    " Implementa un algoritmo di ricerca di immagini simili utilizzando questa struttura di indice che memorizza le immagini della parte 1 e un modello visivo di tua scelta (il modello visivo combinato deve avere almeno 256 dimensioni): per una data immagine di query e un numero intero t, \n",
    " \n",
    "*visualizza le t immagini più simili,\n",
    "*fornisce il numero di immagini univoche e il numero complessivo di immagini considerate durante il processo.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19aa9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione per cercare top_k con LSH quantizzato\n",
    "\n",
    "def find_k_similar_lsh_quant(base_folder: str, img_path: str, k: int):\n",
    "    \"\"\"\n",
    "    Cerca le k immagini di Part1 più simili a img_path (di Part2) usando LSH_EuclideanQuantized.\n",
    "    Stampa:\n",
    "      - i primi k risultati (file name, label, distanza)\n",
    "      - il numero di immagini uniche considerate\n",
    "      - il numero totale di controlli (somma delle lunghezze dei bucket)\n",
    "    E poi visualizza (query + k risultati) con matplotlib.\n",
    "    \"\"\"\n",
    "    # 1) Estrazione feature raw (900-dim) con la funzione esistente\n",
    "    raw_q = np.array(extract_and_process_image(img_path,model), dtype=np.float32)\n",
    "\n",
    "    # 2) Center + normalize (stesso mean_vec e L2 norm usati su Part1)\n",
    "    q_centered = raw_q - mean_vec\n",
    "    q_normed = q_centered / np.linalg.norm(q_centered)\n",
    "\n",
    "    # 3) Chiamata a LSH\n",
    "    top_results, unique_count, total_checked = lsh_quant.query(q_normed, top_t=k)\n",
    "\n",
    "    # 4) Stampa output testuale\n",
    "    print(f\"\\n[LSH-Quant] Top {k} simili a: {img_path}\")\n",
    "    for rank, (idx, dist) in enumerate(top_results, start=1):\n",
    "        label = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        print(f\"  {rank}. {fname} | Classe: {label} | Distanza Euclidea: {dist:.2f}\")\n",
    "    print(f\"[LSH-Quant] Immagini uniche considerate: {unique_count}\")\n",
    "    print(f\"[LSH-Quant] Immagini totali controllate: {total_checked}\")\n",
    "\n",
    "    # 5) Visualizzazione (query + primi k risultati)\n",
    "    fig, axs = plt.subplots(1, k+1, figsize=(4*(k+1), 4))\n",
    "    img_q = cv2.imread(img_path)\n",
    "    img_q = cv2.cvtColor(img_q, cv2.COLOR_BGR2RGB)\n",
    "    axs[0].imshow(img_q)\n",
    "    axs[0].set_title(\"Query (LSH-Quant)\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    for i, (idx, dist) in enumerate(top_results, start=1):\n",
    "        lab = lbls_part1[idx]\n",
    "        fname = flname_part1[idx]\n",
    "        full_path = os.path.join(base_folder, lab, fname)\n",
    "        img_match = cv2.imread(full_path)\n",
    "        img_match = cv2.cvtColor(img_match, cv2.COLOR_BGR2RGB)\n",
    "        axs[i].imshow(img_match)\n",
    "        axs[i].set_title(f\"Rank {i}\\nd={dist:.2f}\")\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65090a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempio di utilizzo su un'immagine di Part2 ---\n",
    "query_path = \"Part2/brain_glioma/brain_glioma_1409.jpg\"\n",
    "\n",
    "k = 5                         # numero di immagini simili da visualizzare\n",
    "\n",
    "# Eseguo la ricerca LSH\n",
    "find_k_similar_lsh_quant(\"Part1\", query_path, k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
